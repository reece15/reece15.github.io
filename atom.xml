<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>除了代码也想写点其他的</title>
  <icon>https://www.gravatar.com/avatar/06aec78c003bcdbb5452f0d1a98f2b77</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.javabin.cn/"/>
  <updated>2024-08-04T09:20:16.521Z</updated>
  <id>http://www.javabin.cn/</id>
  
  <author>
    <name>reece</name>
    <email>reece@javabin.cn</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【大模型-08】使用graphrag构建知识图谱并导入neo4j</title>
    <link href="http://www.javabin.cn/2024/2024_llm_08_graphrag_neo4j.html"/>
    <id>http://www.javabin.cn/2024/2024_llm_08_graphrag_neo4j.html</id>
    <published>2024-08-04T06:24:00.000Z</published>
    <updated>2024-08-04T09:20:16.521Z</updated>
    
    <content type="html"><![CDATA[<p>本篇使用graphrag构建知识图谱，并将知识图谱数据导入neo4j。步骤为先将小说《凡人修仙传》（这里只做示例，其他txt文档也可以）通过graphrag进行索引，再将人物，法宝，地点，技能等重要实体从graphrag数据中解析出，最后导入neo4j中查看。</p><h2 id="本篇内容"><a href="#本篇内容" class="headerlink" title="本篇内容:"></a>本篇内容:</h2><ul><li>什么是neo4j</li><li>部署neo4j</li><li>异常问题处理<ul><li>json报错</li><li>提取的数据异常</li></ul></li><li>parquet文件转换为csv</li><li>csv导入neo4j</li><li>使用neo4j查看知识图谱<a id="more"></a></li></ul><h2 id="什么是neo4j"><a href="#什么是neo4j" class="headerlink" title="什么是neo4j"></a>什么是neo4j</h2><p>Neo4j是一个流行的图形数据库管理系统，它使用图论来存储和查询数据。与传统的关系型数据库不同，Neo4j使用节点和边来表示数据，其中节点代表实体，边代表实体之间的关系。这种数据模型非常适合处理复杂的关系网络，如社交网络、推荐系统、供应链管理等。</p><p>Neo4j的主要特点包括：</p><ul><li>图数据库：基于图的数据模型，非常适合存储和查询复杂的关系数据。</li><li>灵活的数据模型：可以动态添加或修改节点和边，适应不断变化的数据需求。</li><li>高性能查询：使用Cypher查询语言，提供了高性能的图查询能力。</li><li>实时性：支持实时数据更新和查询，适用于需要快速响应的应用场景。</li><li>社区和商业支持：拥有活跃的开发者社区和商业支持，易于学习和使用。</li></ul><p>Neo4j广泛应用于各种领域，包括金融、电信、生命科学、社交网络等，帮助企业和开发者高效地解决复杂的数据关系问题。</p><h2 id="部署neo4j"><a href="#部署neo4j" class="headerlink" title="部署neo4j"></a>部署neo4j</h2><p>为了简单，我们直接使用docker部署neo4j</p><ul><li>创建docker-compose文件，将以下内容保存到docker-compose.yml</li></ul><pre><code class="yaml">    version: &#39;3.8&#39;  # 根据你的Docker Compose版本选择合适的版本号    services:      neo4j:        image: neo4j  # 这里假设使用官方的Neo4j镜像        container_name: neo4j        ports:          - &quot;7474:7474&quot;  # 浏览器访问端口          - &quot;7687:7687&quot;  # 接口使用的端口        environment:          - NEO4J_AUTH=neo4j/reece123456  # 设置Neo4j的认证信息        volumes:          - ./neo4j/data:/data  # 数据卷          - ./neo4j/logs:/logs    # 日志卷          - ./neo4j/import:/var/lib/neo4j/import  # 导入卷        deploy:          mode: replicated          replicas: 1        networks:          - neo4j-network    networks:      neo4j-network:        driver: bridge</code></pre><ul><li>创建neo4j配置文件：在neo4j/conf目录下创建neo4j.conf文件并输入以下配置：</li></ul><pre><code class="shell">server.memory.pagecache.size=512Mserver.default_listen_address=0.0.0.0dbms.import.csv.buffer_size=200000000dbms.directories.import=/var/lib/neo4j/importserver.directories.logs=/logs</code></pre><ul><li>启动容器: 执行<code>docker compose -f .\docker-compose.yml up -d</code> 启动neo4j容器</li><li>访问地址：浏览器打开 <a href="http://127.0.0.1:7474" target="_blank" rel="noopener">http://127.0.0.1:7474</a> 后输入配置的用户名密码<code>neo4j/reece123456</code>后即可访问neo4j</li></ul><h2 id="异常问题处理"><a href="#异常问题处理" class="headerlink" title="异常问题处理"></a>异常问题处理</h2><h3 id="json报错"><a href="#json报错" class="headerlink" title="json报错"></a>json报错</h3><p>由于模型能力问题，偶尔会返回不正常的json格式，导致解析失败。可在<code>.\data\output\20240804-130723\reports\indexing-engine.log</code>日志内看到<code>RuntimeError: Failed to generate valid JSON output</code>等异常。以下是解决方法：</p><ul><li>不支持json格式返回的模型，需要设置<code>model_supports_json:false</code></li><li>对于实体提取的比较乱的情况，可将<code>chunks.size</code>改为500, 使每次处理的数据不至于太多，测试时发现当为qwen2:7b输入的提示词较长时，处理越来越差。不过实体提取时间会变得非常长，一本小说处理完毕大概需要3天时间。</li><li>需要修改json解析函数，在解析json前处理掉不正常的输出.修改<code>.venv\Lib\site-packages\graphrag\llm\openai\utils.py</code></li></ul><pre><code class="python">    def try_parse_json_object(input: str) -&gt; dict:        &quot;&quot;&quot;Generate JSON-string output using best-attempt prompting &amp; parsing techniques.&quot;&quot;&quot;        try:            clean_json = clean_up_json(input)            result = json.loads(clean_json)        except json.JSONDecodeError:            log.exception(&quot;error loading json, json=%s&quot;, input)            raise        else:            if not isinstance(result, dict):                raise TypeError            return result    def clean_up_json(json_str: str) -&gt; str:        &quot;&quot;&quot;Clean up json string.&quot;&quot;&quot;        json_str = (            json_str.replace(&quot;\\n&quot;, &quot;&quot;)            .replace(&quot;\n&quot;, &quot;&quot;)            .replace(&quot;\r&quot;, &quot;&quot;)            .replace(&#39;&quot;[{&#39;, &quot;[{&quot;)            .replace(&#39;}]&quot;&#39;, &quot;}]&quot;)            .replace(&quot;\\&quot;, &quot;&quot;)            .replace(&quot;{{", "{")            .replace("}}&quot;, &quot;}&quot;)            .strip()        )        # Remove JSON Markdown Frame        if json_str.startswith(&quot;```json&quot;):            json_str = json_str[len(&quot;```json&quot;):]        if json_str.endswith(&quot;```&quot;):            json_str = json_str[: len(json_str) - len(&quot;```&quot;)]        return json_str</code></pre><h3 id="提取的数据异常"><a href="#提取的数据异常" class="headerlink" title="提取的数据异常"></a>提取的数据异常</h3><p>由于模型能力问题和系统提示词太长，导致偶尔提取出关系异常，可修改提示词或者对异常数据进行过滤。</p><h3 id="parquet文件转换为csv"><a href="#parquet文件转换为csv" class="headerlink" title="parquet文件转换为csv"></a>parquet文件转换为csv</h3><p>在此之前需要按上篇blog内容将准备的小说内容使用graphrag进行索引，获取到graphrag生成的parquet文件。</p><p>parquet文件可使用pandas转换为csv文件，然后使用neo4j的<code>LOAD CSV</code>方法来将数据导入neo4j。parquet转换csv的脚本如下:</p><pre><code class="python">    import os    import pandas as pd    import csv    def parquet2csv(input_dir, output_dir):        for filename in os.listdir(input_dir):            if not filename.lower().endswith(&#39;.parquet&#39;):                continue            path = os.path.join(input_dir, filename)            print(f&#39;start convert {path}...&#39;)            data_frame = pd.read_parquet(path)            csv_path = os.path.join(output_dir, path.replace(&#39;.parquet&#39;, &#39;.csv&#39;))            data_frame.to_csv(csv_path, index=False, escapechar=&#39;\\&#39;, quoting=csv.QUOTE_ALL)    if __name__ == &#39;__main__&#39;:        parquet2csv(            r&#39;F:\code\py\graph_rag_test\data\output\20240803-070956\artifacts&#39;,            r&#39;F:\code\py\graph_rag_test\neo4j\import&#39;        )</code></pre><h3 id="csv导入neo4j"><a href="#csv导入neo4j" class="headerlink" title="csv导入neo4j"></a>csv导入neo4j</h3><p>这次我们只需要查看小说里的人物,法宝,地点等重要事情之间的关系，只导入entities和relationships。</p><p>对于大模型提取出的多的不相干实体，先进行下过滤。下面的脚本将删除非中文名称的实体和类型不在<code>ENTITY_TYPES</code>内的实体，并将结果重新导出到<code>.fixed.csv</code>文件。</p><pre><code class="python">    import os    import re    import pandas as pd    import csv    P_BAD_NAME = re.compile(        r&#39;^[^\u4e00-\u9fa5]+$&#39;    )    ENTITY_TYPES = set([&#39;PERSON&#39;, &#39;EVENT&#39;, &#39;ORGANIZATION&#39;, &#39;GEO&#39;, &#39;CONCEPT&#39;, &#39;OBJECT&#39;, &#39;SKILL&#39;, &#39;TECHNOLOGY&#39;, &#39;AUTHOR&#39;, &#39;BOOK&#39;, &#39;PRODUCT&#39;, &#39;ITEM&#39;, &#39;ARTIFACT&#39;])    def validate(relation_file, entity_file):        bad_names = {}        entities = []        with open(entity_file, &#39;rt&#39;, encoding=&#39;utf-8&#39;) as file:            reader = csv.DictReader(file)            entity_header = reader.fieldnames            for row in reader:                if P_BAD_NAME.search(row[&#39;name&#39;]) or row[&#39;type&#39;] not in ENTITY_TYPES:                    print(row[&#39;name&#39;])                    bad_names[row[&#39;name&#39;]] = row                else:                    entities.append(row)        with open(entity_file.replace(&#39;.csv&#39;, &#39;.fixed.csv&#39;), &#39;wt&#39;, encoding=&#39;utf-8&#39;) as file:            writer = csv.DictWriter(file, fieldnames=entity_header)            writer.writeheader()            writer.writerows(entities)        relations = []        with open(relation_file, &#39;rt&#39;, encoding=&#39;utf-8&#39;) as file:            reader = csv.DictReader(file)            relation_header = reader.fieldnames            for row in reader:                if row[&#39;source&#39;] in bad_names or row[&#39;target&#39;] in bad_names:                    continue                relations.append(row)        with open(relation_file.replace(&#39;.csv&#39;, &#39;.fixed.csv&#39;), &#39;wt&#39;, encoding=&#39;utf-8&#39;) as file:            writer = csv.DictWriter(file, fieldnames=relation_header)            writer.writeheader()            writer.writerows(relations)    if __name__ == &#39;__main__&#39;:        validate(            r&#39;F:\code\py\graph_rag_test\neo4j\import\create_final_relationships.csv&#39;,            r&#39;F:\code\py\graph_rag_test\neo4j\import\create_final_entities.csv&#39;        )</code></pre><p>然后将csv数据导入neo4j。打开neo4j web管理界面，执行以下代码即可导入数据。</p><pre><code class="shell">LOAD CSV WITH HEADERS FROM &#39;file:///create_final_entities.fixed.csv&#39; AS rowCREATE (e:Entity {    id: row.id,    name: row.name,    type: row.type,    description: row.description});LOAD CSV WITH HEADERS FROM &#39;file:///create_final_relationships.fixed.csv&#39; AS rowCREATE (r:Relationship {    source: row.source,    target: row.target,    weight: toFloat(row.weight),    description: row.description,    id: row.id,    source_degree: toInteger(row.source_degree),    target_degree: toInteger(row.target_degree),    rank: toInteger(row.rank)});CREATE INDEX FOR (e:Entity) ON (e.id);CREATE INDEX FOR (r:Relationship) ON (r.id);MATCH (r:Relationship)MATCH (source:Entity {name: r.source})MATCH (target:Entity {name: r.target})CREATE (source)-[:RELATES_TO]-&gt;(target);</code></pre><h3 id="使用neo4j查看知识图谱"><a href="#使用neo4j查看知识图谱" class="headerlink" title="使用neo4j查看知识图谱"></a>使用neo4j查看知识图谱</h3><p>打开neo4j web管理页面后执行以下语句即可查询并可视化知识图谱。</p><p>查询所有关系，展示前10000个</p><pre><code class="shell">MATCH (e1:Entity)-[r:RELATES_TO]-&gt;(e2:Entity)RETURN e1, r, e2LIMIT 10000;</code></pre><p><img src="/photo_2024/llm_08_neo4j_graph_all.svg" alt=""></p><p>查询人物之间的关系</p><pre><code class="shell">MATCH (e1:Entity)-[r:RELATES_TO]-&gt;(e2:Entity)WHERE e1.type = &#39;PERSON&#39; AND e2.type = &#39;PERSON&#39;RETURN e1, r, e2LIMIT 10000;</code></pre><p><img src="/photo_2024/llm_08_neo4j_graph_person.svg" alt=""></p><p>查询 南宫婉 的关系：</p><pre><code class="shell">MATCH (e1:Entity{name:&quot;南宫婉&quot;})-[r:RELATES_TO]-(e2:Entity)RETURN e1, r, e2LIMIT 10000;</code></pre><p><img src="/photo_2024/llm_08_neo4j_nangongwan.png" alt=""></p><p>如果neo4j只能看到前300个节点，可在界面左下角的设置中将<code>Initial Node Display， Max neighbours from vis interaction，Result view max rows，Max record fields</code>调整大一点。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><blockquote><p>neo4j: <a href="https://neo4j.com/docs/" target="_blank" rel="noopener">https://neo4j.com/docs/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇使用graphrag构建知识图谱，并将知识图谱数据导入neo4j。步骤为先将小说《凡人修仙传》（这里只做示例，其他txt文档也可以）通过graphrag进行索引，再将人物，法宝，地点，技能等重要实体从graphrag数据中解析出，最后导入neo4j中查看。&lt;/p&gt;
&lt;h2 id=&quot;本篇内容&quot;&gt;&lt;a href=&quot;#本篇内容&quot; class=&quot;headerlink&quot; title=&quot;本篇内容:&quot;&gt;&lt;/a&gt;本篇内容:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;什么是neo4j&lt;/li&gt;
&lt;li&gt;部署neo4j&lt;/li&gt;
&lt;li&gt;异常问题处理&lt;ul&gt;
&lt;li&gt;json报错&lt;/li&gt;
&lt;li&gt;提取的数据异常&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;parquet文件转换为csv&lt;/li&gt;
&lt;li&gt;csv导入neo4j&lt;/li&gt;
&lt;li&gt;使用neo4j查看知识图谱
    
    </summary>
    
    
      <category term="学习" scheme="http://www.javabin.cn/tags/study/"/>
    
      <category term="大模型" scheme="http://www.javabin.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="程序" scheme="http://www.javabin.cn/tags/%E7%A8%8B%E5%BA%8F/"/>
    
      <category term="graphrag" scheme="http://www.javabin.cn/tags/graphrag/"/>
    
      <category term="neo4j" scheme="http://www.javabin.cn/tags/neo4j/"/>
    
      <category term="知识图谱" scheme="http://www.javabin.cn/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>【大模型-07】本地使用graphrag进行知识检索</title>
    <link href="http://www.javabin.cn/2024/2024_llm_07_graphrag.html"/>
    <id>http://www.javabin.cn/2024/2024_llm_07_graphrag.html</id>
    <published>2024-07-31T14:31:04.000Z</published>
    <updated>2024-08-02T11:18:25.130Z</updated>
    
    <content type="html"><![CDATA[<p>本篇将本地配置微软的graphrag软件包，并使用graphrag对小说进行索引后进行数据的检索。</p><h2 id="本篇内容"><a href="#本篇内容" class="headerlink" title="本篇内容:"></a>本篇内容:</h2><ul><li>什么是graphrag</li><li>配置graphrag<ul><li>部署本地embedding服务bge</li><li>部署本地大模型服务qwen2:7b</li><li>配置测试graphrag</li></ul></li><li>数据存储格式</li><li>使用neo4j可视化知识图谱<a id="more"></a></li></ul><h2 id="什么是graphrag"><a href="#什么是graphrag" class="headerlink" title="什么是graphrag"></a>什么是graphrag</h2><p>graphrag是一种基于图的rag技术，是传统rag的优化版本。传统的rag使用embedding模型为文档块生成embedding向量后存入向量数据库，在查询时为查询语句也生成embedding向量，通过查找出数据库内与查询向量最相接近的文档块向量来检索出语义最相关的文档块。而graphrag会先抽取出文档块内的人物，地点，事件等组成实体间的关系网络，在查询时通过实体关系查询出和上下文相关的子图，最后通过大模型整合为连贯的最终输出。</p><p>graphrag检索出的信息不再是孤立的，而是相互之间有关联的，所以相关文档块的上下文更加丰富，没有传统RAG东拼西凑的感觉。</p><p>graphrag支持两种查询方式：global和local。</p><ul><li>Local查询方式： 主要针对需要理解文档中特定实体的问题，通过结合AI提取的知识图谱和原始文档的文本块生成答案。这种方法适用于需要对文档中提到的特定实体有所理解的问题，例如询问某种草药的治疗特性 。</li><li>Global查询方式：在所有AI生成的社区报告中进行搜索，以map-reduce的方式生成答案。这是一种资源密集型方法，但通常能够很好地回答那些需要对数据集整体有全面理解的问题，例如询问在某个笔记本中提到的草药最重要的价值 </li></ul><p>GraphRAG的索引和查询包括了几个关键步骤：</p><ul><li>将源文档分割成文本块</li><li>从文本块中提取实体和关系实例</li><li>为每个图元素生成摘要</li><li>使用社区检测算法将图划分为社区</li><li>为每个社区生成摘要</li><li>利用社区摘要生成局部答案</li><li>汇总这些局部答案以生成全局答案 </li></ul><h2 id="配置graphrag"><a href="#配置graphrag" class="headerlink" title="配置graphrag"></a>配置graphrag</h2><p>本地部署graphrag需要配置embedding服务和大模型服务，由于graphrag生成索引和进行查询时会消耗大量token, 为了更经济实用，我们将本地部署大模型和embedding模型。</p><h3 id="部署embedding服务"><a href="#部署embedding服务" class="headerlink" title="部署embedding服务"></a>部署embedding服务</h3><h4 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h4><pre><code class="shell">pip3 install modelscope fastapi FlagEmbedding pydantic uvicorn</code></pre><h4 id="编写embedding服务端代码"><a href="#编写embedding服务端代码" class="headerlink" title="编写embedding服务端代码"></a>编写embedding服务端代码</h4><p>将以下代码保存到bge.py</p><pre><code class="python">    import os    from typing import List, Union    from fastapi import FastAPI    from modelscope import snapshot_download    from FlagEmbedding import FlagModel    from pydantic import BaseModel    model_dir = snapshot_download(&quot;AI-ModelScope/bge-large-zh-v1.5&quot;, revision=&#39;master&#39;, local_dir=&#39;./bge&#39;)    model = FlagModel(model_dir, use_fp16=True)    app = FastAPI()    BATCH_SIZE = int(os.environ.get(&#39;EMBEDDING_BATCH_SIZE&#39;, 256))    class EmbeddingData(BaseModel):        input: Union[List, str]    @app.post(&quot;/embeddings&quot;)    async def get_embedding(data: EmbeddingData):        embeddings = model.encode(data.input, batch_size=BATCH_SIZE)        if isinstance(data.input, str):            embeddings = [embeddings]        return {            &#39;data&#39;: [{&#39;embedding&#39;: item.tolist(), &#39;index&#39;: index, &#39;object&#39;: &#39;embedding&#39;} for index, item in enumerate(embeddings)],            &quot;object&quot;: &quot;list&quot;,            &quot;model&quot;: &quot;bge&quot;,            &quot;usage&quot;: {                &quot;prompt_tokens&quot;: 0,                &quot;total_tokens&quot;: 0            }        }</code></pre><h4 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h4><p>在bge.py同目录执行：</p><pre><code class="shell">uvicorn bge:app --host 0.0.0.0 --port 3001</code></pre><p>此时embedding模型服务将运行在本机3001端口</p><h3 id="部署大模型服务"><a href="#部署大模型服务" class="headerlink" title="部署大模型服务"></a>部署大模型服务</h3><p>可參考blog <code>【大模型-01】本地部署qwen2模型+集成聊天界面ChatGPT-Next-Web（无显卡也可运行）</code>来使用ollama本地运行qwen2:7b。</p><h3 id="配置graphrag-1"><a href="#配置graphrag-1" class="headerlink" title="配置graphrag"></a>配置graphrag</h3><h4 id="安装依赖-1"><a href="#安装依赖-1" class="headerlink" title="安装依赖"></a>安装依赖</h4><pre><code class="shell">pip3 install graphrag</code></pre><h4 id="初始化目录"><a href="#初始化目录" class="headerlink" title="初始化目录"></a>初始化目录</h4><pre><code class="shell">python -m graphrag.index --init --root ./data# 创建输入目录mkdir -p ./data/input</code></pre><p>将准备好的小说，复制到./data/input目录内，注意文件格式为txt,编码为utf-8。graphrag配置文件中配置了默认会从input目录读取文件进行索引。</p><h4 id="配置大模型接口"><a href="#配置大模型接口" class="headerlink" title="配置大模型接口"></a>配置大模型接口</h4><p>打开./data/settings.yaml修改以下配置: </p><ul><li>llm.model: 改为 <code>qwen2:7b</code></li><li>llm.api_base: 改为 <code>http://127.0.0.1:11434/v1</code></li></ul><pre><code class="yaml">llm:  model: qwen2:7b  api_base: http://127.0.0.1:11434/v1  model_supports_json: false  max_tokens: 32768</code></pre><h4 id="配置embedding接口"><a href="#配置embedding接口" class="headerlink" title="配置embedding接口"></a>配置embedding接口</h4><ul><li>embeddings.llm.model: 改为 <code>bge</code></li><li>embeddings.llm.api_base: 改为 <code>http://127.0.0.1:3001</code></li></ul><pre><code class="yaml">    embeddings:      llm:        model: bge        api_base: http://127.0.0.1:3001</code></pre><h4 id="生成索引"><a href="#生成索引" class="headerlink" title="生成索引"></a>生成索引</h4><p>配置好后我们就可以执行命令来生成索引数据，这个过程会花费很长时间，初次测试建议先用小文件进行测试。</p><pre><code class="shell">python -m graphrag.index --root ./data</code></pre><h4 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h4><p>建立索引完成后，就可以通过local和global查询方式来检索我们的问题：</p><pre><code class="shell">python -m graphrag.query --root .\data\ --method local &quot;韩立是谁？&quot;python -m graphrag.query --root .\data\ --method global &quot;这篇小说讲的什么&quot;</code></pre><h2 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h2><p>graphrag的知识图谱相关数据都存储在目录<code>data/output/20240731-202342/artifacts/</code>目录下的<code>.parquet</code>文件中，pycharm中可以下载安装<code>avro/parquet viewr</code>进行查看。以下是部分文件的说明和结构：</p><h3 id="create-final-nodes-parquet"><a href="#create-final-nodes-parquet" class="headerlink" title="create_final_nodes.parquet"></a>create_final_nodes.parquet</h3><p>知识图谱中的各种节点信息,包括节点的说明，名称，类型等。<br><img src="/photo_2024/llm_07_graphrag_node.png" alt=""></p><h3 id="create-final-entities-parquet"><a href="#create-final-entities-parquet" class="headerlink" title="create_final_entities.parquet"></a>create_final_entities.parquet</h3><p>知识图谱中的实体信息<br><img src="/photo_2024/llm_07_graphrag_entity.png" alt=""></p><h3 id="create-final-relationships-parquet"><a href="#create-final-relationships-parquet" class="headerlink" title="create_final_relationships.parquet"></a>create_final_relationships.parquet</h3><p>知识图谱中的关系信息<br><img src="/photo_2024/llm_07_graphrag_relation.png" alt=""></p><h3 id="create-final-community-reports-parquet"><a href="#create-final-community-reports-parquet" class="headerlink" title="create_final_community_reports.parquet"></a>create_final_community_reports.parquet</h3><p>生成的各个社区的标题和描述</p><h2 id="使用neo4j可视化知识图谱"><a href="#使用neo4j可视化知识图谱" class="headerlink" title="使用neo4j可视化知识图谱"></a>使用neo4j可视化知识图谱</h2><p>我们将.parquet文件转换为csv后可导入neo4j中查看实体间的关系。如何转换数据和导入neo4j，我们将在下一篇blog中说明。</p><p><img src="/photo_2024/llm_07_graphrag_neo4j.png" alt=""></p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><blockquote><p>graphrag github项目: <a href="https://github.com/microsoft/graphrag" target="_blank" rel="noopener">https://github.com/microsoft/graphrag</a></p><p>graphrag文档：<a href="https://microsoft.github.io/graphrag/" target="_blank" rel="noopener">https://microsoft.github.io/graphrag/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇将本地配置微软的graphrag软件包，并使用graphrag对小说进行索引后进行数据的检索。&lt;/p&gt;
&lt;h2 id=&quot;本篇内容&quot;&gt;&lt;a href=&quot;#本篇内容&quot; class=&quot;headerlink&quot; title=&quot;本篇内容:&quot;&gt;&lt;/a&gt;本篇内容:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;什么是graphrag&lt;/li&gt;
&lt;li&gt;配置graphrag&lt;ul&gt;
&lt;li&gt;部署本地embedding服务bge&lt;/li&gt;
&lt;li&gt;部署本地大模型服务qwen2:7b&lt;/li&gt;
&lt;li&gt;配置测试graphrag&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数据存储格式&lt;/li&gt;
&lt;li&gt;使用neo4j可视化知识图谱
    
    </summary>
    
    
      <category term="学习" scheme="http://www.javabin.cn/tags/study/"/>
    
      <category term="大模型" scheme="http://www.javabin.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="程序" scheme="http://www.javabin.cn/tags/%E7%A8%8B%E5%BA%8F/"/>
    
      <category term="RAG" scheme="http://www.javabin.cn/tags/RAG/"/>
    
      <category term="graphrag" scheme="http://www.javabin.cn/tags/graphrag/"/>
    
  </entry>
  
  <entry>
    <title>【大模型-06】大模型减少显存占用的方式：量化</title>
    <link href="http://www.javabin.cn/2024/2024_llm_06_quantization.html"/>
    <id>http://www.javabin.cn/2024/2024_llm_06_quantization.html</id>
    <published>2024-07-21T02:51:44.000Z</published>
    <updated>2024-07-29T15:59:22.118Z</updated>
    
    <content type="html"><![CDATA[<p>A100 RTX4090太贵买不起，但是还是想玩大模型怎么办？控制台又显示CUDA OUT OF MEMORY怎么办？显存不够，量化来凑！本篇讲解何为量化，量化的原理和使用三种量化方式（AWQ, GPTQ, Bitsandbytes）对模型进行量化。</p><h2 id="本篇内容"><a href="#本篇内容" class="headerlink" title="本篇内容:"></a>本篇内容:</h2><ul><li>量化是什么</li><li>量化方式简介<ul><li>GPTQ</li><li>AWQ</li><li>Bitsandbytes</li></ul></li><li>对qwen2进行量化并导出:<ul><li>GPTQ</li><li>AWQ</li><li>Bitsandbytes</li></ul></li><li>部署量化后的模型<ul><li>执行预测</li><li>vllm部署</li></ul></li></ul><a id="more"></a><h2 id="量化是什么"><a href="#量化是什么" class="headerlink" title="量化是什么"></a>量化是什么</h2><p>在大模型中，量化是将高比特的权重转换到低比特权重的过程。在实际训练和推理过程中，最常用的权重精度为32bit浮点数或16bit浮点数。但是更高的精度意味着模型的体积更大，运行占用的GPU显存更大。这种对硬件资源的消耗，对于我们一般设备进行体验大模型来说，有着很大的挑战。 而量化就是将16/32位浮点数的权重映射到4位int/8位int/8位浮点等低比特权重，且尽可能少的牺牲模型原有能力的方法。</p><p>通俗的类比，可以想象你有一个只能展示8bit黑白图像的显示器，但是你有一张彩色的图片要进行显示。你只能将rgb像素转换为灰度像素，即对于每个rgb像素，将原有的R(8bit),G(8bit),B(8bit)共24bit,转换为灰度表示(8bit)。这时候，由于一个像素只有8bit，图像只有256级灰度表示，虽然丢失了一定的色彩变化，但是还是能完美保留图像的细节。(如图左半边所示)<br><img src="/photo_2024/llm_06_quantization.png" alt=""><br>对于显示图片来说，如果我们的设备是一块单片机或者低功耗设备，那可能还需要将图片转换到更低的bit，如2bit上，以下图片即是通过Atkinson抖动算法将原有每像素24bit图像转换为每像素2bit。可以看到转换到2bit时，图像细节已经有了一定的丢失。<br><img src="/photo_2024/llm_06_quantization_2bit.png" alt=""></p><p>也就是说，量化虽然可以减少显存的占用和模型大小，但是越低的比特数会导致越严重的原有模型能力的损失。</p><p>在实际大模型量化中，一种简单的量化方法举例：如果我们将16位浮点数量化到int8,我们知道int8的表示范围为0-255。我们先计算出大模型所有权重中的最大值max_x和最小值min_x，然后将大模型的最大值max_x映射到255，将最小值min_x映射到0，然后将其他值以缩放比例公式<code>x_q=(x/max_x * 255)</code>全部转换到0-255间的对应数值上。 这样就可以将原有权重量化到int8上。<br><img src="/photo_2024/llm_06_quantization_value.png" alt=""></p><p>当然，以上只是最简单的映射方式中简单实现，在实际情况中，数值并不是均匀分布的，可能还需要增加偏置来充分利用int8值域空间，或者由于int8过于离散，只能表示有限整数，会将权重的小幅度数据变化全部量化为同一个数据，等。</p><h2 id="量化方式简介"><a href="#量化方式简介" class="headerlink" title="量化方式简介"></a>量化方式简介</h2><p>本篇blog使用已经实现各种量化算法的软件库来进行大模型量化，不进行底层量化算法的研究和实现。目前的主流量化方法有 GPTQ, AWQ, Bitsandbytes。</p><h3 id="GPTQ"><a href="#GPTQ" class="headerlink" title="GPTQ"></a>GPTQ</h3><p>GPTQ(Post-Training Quantization for GPT Models)是一种4位量化的训练后量化(PTQ)方法。GPTQ 可以在大约四个 GPU 小时内量化具有 1750 亿个参数的 GPT 模型，将位宽降低到每个权重 3 或 4 位，与未量化的基线相比，精度下降可以忽略不计。auto-gptq是实现了gptq算法的软件包，本篇blog将采用auto-gptq对大模型进行量化。</p><h3 id="AWQ"><a href="#AWQ" class="headerlink" title="AWQ"></a>AWQ</h3><p>AWQ (Activation-aware Weight Quantization)即激活感知权重量化算法，一种硬件友好的 LLM 低位仅权重量化方法，解决了训练后量化 (PTQ) 在低比特设置下准确率会大幅下降以及GPTQ通过二阶信息进行误差补偿导致过度拟合校准数据的问题。AutoAWQ是实现了AWQ算法的软件包，它是易于使用的4位量化模型软件包。与原有未量化模型比，AutoAWQ可将模型速度提高3倍，并将内存需求降低3倍。AutoAWQ 是在麻省理工学院的原始工作基础上创建和改进的。本篇我们使用AutoAWQ来对大模型进行量化。</p><h3 id="Bitsandbytes"><a href="#Bitsandbytes" class="headerlink" title="Bitsandbytes"></a>Bitsandbytes</h3><p>Bitsandbytes是一种4/8位优化的实现方式，它通过高效的量化算法将模型参数和中间计算结果转换为4/8位表示，以达到压缩和加速的目的。 </p><h2 id="对qwen2进行量化并导出"><a href="#对qwen2进行量化并导出" class="headerlink" title="对qwen2进行量化并导出:"></a>对qwen2进行量化并导出:</h2><h3 id="GPTQ-1"><a href="#GPTQ-1" class="headerlink" title="GPTQ"></a>GPTQ</h3><ul><li>安装依赖</li></ul><pre><code class="shell">pip install transformers optimum accelerate auto-gptq</code></pre><ul><li>准备校准数据。这里我们使用我们微调时的数据即可。</li><li>编写量化脚本。我们参考qwen官方文档和gptq官方文档说明即可。</li></ul><pre><code class="python">    import json    import torch    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig    from transformers import AutoTokenizer    def gen_data(train_data_paths):        for path in train_data_paths:            with open(path, &#39;rt&#39;, encoding=&#39;utf-8&#39;) as file:                for item in json.loads(file.read()):                    yield [                        {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: item[&#39;instruction&#39;]},                        {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: item[&#39;input&#39;]},                        {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: item[&#39;output&#39;]}                    ]    def run_gptq(model_path, quant_path, data_paths):        print(&#39;量化配置...&#39;)        quantize_config = BaseQuantizeConfig(            bits=4,  # 4 or 8            group_size=128,            damp_percent=0.01,            desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad            static_groups=False,            sym=True,            true_sequential=True,            model_name_or_path=None,            model_file_base_name=&quot;model&quot;        )        max_len = 8192        print(&#39;加载模型...&#39;)        tokenizer = AutoTokenizer.from_pretrained(model_path)        model = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)        print(&#39;加载数据...&#39;)        data = []        for msg in gen_data(data_paths):            text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)            model_inputs = tokenizer([text])            input_ids = torch.tensor(model_inputs.input_ids[:max_len], dtype=torch.int)            data.append(dict(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id)))        print(&#39;开始量化...&#39;)        model.quantize(data, cache_examples_on_gpu=False)        print(&#39;导出模型...&#39;)        model.save_quantized(quant_path, use_safetensors=True)        tokenizer.save_pretrained(quant_path)    if __name__ == &#39;__main__&#39;:        run_gptq(            model_path=r&quot;F:\code\github\vllm_docker\models\qwen2_home_control&quot;,            quant_path=r&quot;F:\code\github\vllm_docker\models\qwen2_home_control_gptq_4&quot;,            data_paths=[                r&quot;F:\code\py\ft_qwen1.5b\LLaMA-Factory\data\home_control.json&quot;,                r&quot;F:\code\py\ft_qwen1.5b\LLaMA-Factory\data\identity.json&quot;            ]        )</code></pre><h3 id="AWQ-1"><a href="#AWQ-1" class="headerlink" title="AWQ"></a>AWQ</h3><ul><li>安装依赖</li></ul><pre><code class="shell">pip3 install AutoAWQ==0.2.5</code></pre><ul><li>准备校准数据。这里我们使用我们微调时的数据即可。</li><li>编写量化脚本。将以下脚本保存到llm_awq.py，将参数改为自己的模型路径和校准数据路径后执行。可以参考awq官方项目的样例<a href="https://github.com/casper-hansen/AutoAWQ/blob/main/examples/quantize.py" target="_blank" rel="noopener">quantize.py</a>和qwen2官方文档<a href="https://qwen.readthedocs.io/en/latest/quantization/awq.html" target="_blank" rel="noopener">awq.html</a></li></ul><pre><code class="python">    import json    from awq import AutoAWQForCausalLM    from transformers import AutoTokenizer    def gen_data(train_data_paths):        for path in train_data_paths:            with open(path, &#39;rt&#39;, encoding=&#39;utf-8&#39;) as file:                for item in json.loads(file.read()):                    yield [                        {&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: item[&#39;instruction&#39;]},                        {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: item[&#39;input&#39;]},                        {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: item[&#39;output&#39;]}                    ]    def run_awq(model_path, quant_path, data_paths):        quant_config = {&quot;zero_point&quot;: True, &quot;q_group_size&quot;: 128, &quot;w_bit&quot;: 4, &quot;version&quot;: &quot;GEMM&quot;}        print(&quot;加载模型...&quot;)        tokenizer = AutoTokenizer.from_pretrained(model_path)        model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=&quot;auto&quot;, safetensors=True)        print(&quot;加载校准数据...&quot;)        data = []        for msg in gen_data(data_paths):            text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)            data.append(text.strip())        print(&#39;开始量化并校准...&#39;)        model.quantize(tokenizer, quant_config=quant_config, calib_data=data)        print(&quot;导出量化模型...&quot;)        model.save_quantized(quant_path, safetensors=True, shard_size=&#39;4GB&#39;)        tokenizer.save_pretrained(quant_path)    if __name__ == &#39;__main__&#39;:        run_awq(            model_path=r&quot;F:\code\github\vllm_docker\models\qwen2_home_control&quot;,            quant_path=r&quot;F:\code\github\vllm_docker\models\qwen2_home_control_awq4&quot;,            data_paths=[                r&quot;F:\code\py\ft_qwen1.5b\LLaMA-Factory\data\home_control.json&quot;,                r&quot;F:\code\py\ft_qwen1.5b\LLaMA-Factory\data\identity.json&quot;            ]        )</code></pre><ul><li>脚本说明：<ul><li>参数-<code>version</code>：可选<code>GEMV</code>和<code>GEMM</code>.官方建议如下：<ul><li><code>GEMV</code>:比 GEMM 快 20%，批量大小仅为 1（不适合大上下文）</li><li><code>GEMM</code>:批量大小低于 8 时比 FP16 快得多（适合大上下文）</li></ul></li><li><code>save_quantized</code>参数：<ul><li><code>safetensors</code>: 是否导出<code>safetensors</code>格式模型。默认True</li><li><code>shard_size</code>: 默认分片大小。导出时会按<code>shared_size</code>将模型参数导出为多个小文件。</li></ul></li><li>参数-<code>w_bit</code>： 量化后的位数。</li><li>参数-<code>zero_point</code>： 为True时，量化后的参数范围为: <code>0 ~ (2**w_bit - 1)</code>; 为False是，量化后的参数范围为: <code>-(2 ** (.w_bit - 1))  ~ 2 ** (w_bit - 1) - 1</code>。wit=4，且zero_point=True是，参数范围为<code>0~15</code></li></ul></li></ul><h3 id="Bitsandbytes-1"><a href="#Bitsandbytes-1" class="headerlink" title="Bitsandbytes"></a>Bitsandbytes</h3><ul><li>安装依赖：<code>pip install transformers bitsandbytes</code></li><li>进行模型量化。将以下脚本保存到llm_int4.py，修改模型路径后运行即可进行量化。</li></ul><pre><code class="python">    import torch    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig    def export_int4(model_path, quant_path):        print(&#39;配置量化参数...&#39;)        quantization_config = BitsAndBytesConfig(            load_in_4bit=True,            bnb_4bit_compute_dtype=torch.bfloat16,        )        print(&#39;加载模型...&#39;)        tokenizer = AutoTokenizer.from_pretrained(model_path)        model = AutoModelForCausalLM.from_pretrained(            model_path,            quantization_config=quantization_config        ).eval()        print(&#39;导出模型...&#39;)        model.save_pretrained(quant_path)        tokenizer.save_pretrained(quant_path)    if __name__ == &#39;__main__&#39;:        export_int4(            model_path=r&quot;F:\code\github\vllm_docker\models\qwen2_home_control&quot;,            quant_path=r&quot;F:\code\github\vllm_docker\models\qwen2_home_control_int4&quot;        )</code></pre><h2 id="部署量化后的模型"><a href="#部署量化后的模型" class="headerlink" title="部署量化后的模型"></a>部署量化后的模型</h2><h3 id="运行预测"><a href="#运行预测" class="headerlink" title="运行预测"></a>运行预测</h3><p>可使用以下脚本将模型路径换为不同的模型，进行预测:</p><pre><code class="python">    import torch    from transformers import AutoModelForCausalLM, AutoTokenizer    def run_predict(messages, model, tokenizer):        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)        model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(&#39;cuda&#39;)        generated_ids = model.generate(            model_inputs.input_ids,            max_new_tokens=512        )        generated_ids = [            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)        ]        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]    def predict(mode_dir):        tokenizer = AutoTokenizer.from_pretrained(mode_dir, use_fast=False, trust_remote_code=True)        model = AutoModelForCausalLM.from_pretrained(mode_dir, device_map=&quot;auto&quot;, torch_dtype=torch.bfloat16)        instruction = &#39;你是一个聊天助手&#39;        print(&#39;请输入问题后按回车：&#39;)        while True:            messages = [                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: f&quot;{instruction}&quot;},                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input()}&quot;}            ]            response = run_predict(messages, model, tokenizer)            print(response)    if __name__ == &#39;__main__&#39;:        predict(mode_dir=r&#39;F:\code\github\vllm_docker\models\qwen2_home_control_int4&#39;)</code></pre><h3 id="vllm部署"><a href="#vllm部署" class="headerlink" title="vllm部署"></a>vllm部署</h3><p>将vllm模型部署路径参数修改为量化后模型的路径即可。vllm容器部署方法可参考上一篇blog.</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><blockquote><p>GPTQ arxiv文章: <a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener">https://arxiv.org/abs/2210.17323</a></p><p>GPTQ github项目:<a href="https://github.com/ist-daslab/gptq" target="_blank" rel="noopener">https://github.com/ist-daslab/gptq</a></p><p>AWQ arxiv文章: <a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener">https://arxiv.org/abs/2306.00978</a></p><p>AWQ github项目: <a href="https://github.com/mit-han-lab/llm-awq" target="_blank" rel="noopener">https://github.com/mit-han-lab/llm-awq</a></p><p>Bitsandbytes项目: <a href="https://github.com/bitsandbytes-foundation/bitsandbytes" target="_blank" rel="noopener">https://github.com/bitsandbytes-foundation/bitsandbytes</a></p><p>使用bitsandbytes进行4bit量化：<a href="https://github.com/huggingface/blog/blob/main/4bit-transformers-bitsandbytes.md" target="_blank" rel="noopener">https://github.com/huggingface/blog/blob/main/4bit-transformers-bitsandbytes.md</a></p><p>qwen2文档如何使用awq量化: <a href="https://qwen.readthedocs.io/en/latest/quantization/awq.html" target="_blank" rel="noopener">https://qwen.readthedocs.io/en/latest/quantization/awq.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A100 RTX4090太贵买不起，但是还是想玩大模型怎么办？控制台又显示CUDA OUT OF MEMORY怎么办？显存不够，量化来凑！本篇讲解何为量化，量化的原理和使用三种量化方式（AWQ, GPTQ, Bitsandbytes）对模型进行量化。&lt;/p&gt;
&lt;h2 id=&quot;本篇内容&quot;&gt;&lt;a href=&quot;#本篇内容&quot; class=&quot;headerlink&quot; title=&quot;本篇内容:&quot;&gt;&lt;/a&gt;本篇内容:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;量化是什么&lt;/li&gt;
&lt;li&gt;量化方式简介&lt;ul&gt;
&lt;li&gt;GPTQ&lt;/li&gt;
&lt;li&gt;AWQ&lt;/li&gt;
&lt;li&gt;Bitsandbytes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;对qwen2进行量化并导出:&lt;ul&gt;
&lt;li&gt;GPTQ&lt;/li&gt;
&lt;li&gt;AWQ&lt;/li&gt;
&lt;li&gt;Bitsandbytes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;部署量化后的模型&lt;ul&gt;
&lt;li&gt;执行预测&lt;/li&gt;
&lt;li&gt;vllm部署&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://www.javabin.cn/tags/study/"/>
    
      <category term="大模型" scheme="http://www.javabin.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="AWQ" scheme="http://www.javabin.cn/tags/AWQ/"/>
    
      <category term="GPTQ" scheme="http://www.javabin.cn/tags/GPTQ/"/>
    
      <category term="Bitsandbytes" scheme="http://www.javabin.cn/tags/Bitsandbytes/"/>
    
      <category term="量化大模型" scheme="http://www.javabin.cn/tags/%E9%87%8F%E5%8C%96%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="编程" scheme="http://www.javabin.cn/tags/%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>【大模型-05】大模型部署和加速方式：vllm</title>
    <link href="http://www.javabin.cn/2024/2024_llm_05_ops.html"/>
    <id>http://www.javabin.cn/2024/2024_llm_05_ops.html</id>
    <published>2024-07-18T17:44:12.000Z</published>
    <updated>2024-07-26T17:47:00.399Z</updated>
    
    <content type="html"><![CDATA[<p>大模型虽然非常智能，但是由于需要消耗大量GPU资源，在GPU性能无法提升时，如何提升大模型运行速度，让大模型运行更快呢？目前最好的方法是采用vllm部署来对大模型进行加速。本篇blog将使用vllm部署上一篇我们使用lora微调后导出的模型。</p><h2 id="本篇内容"><a href="#本篇内容" class="headerlink" title="本篇内容:"></a>本篇内容:</h2><ul><li>vllm是什么</li><li>vllm本地部署大模型<ul><li>安装配置vllm</li><li>部署大模型</li><li>测试</li></ul></li><li>接入dify</li></ul><a id="more"></a><h2 id="vllm是什么"><a href="#vllm是什么" class="headerlink" title="vllm是什么"></a>vllm是什么</h2><p>开源的大模型高速推理框架。通过PagedAttention管理缓存张量，可以比Transformers高最高24倍的吞吐量。它的特点有：</p><ul><li>更高的吞吐量。单位时间内能生成更多的token</li><li>兼容多种主流大模型。可以轻松接入自己的模型。</li><li>多卡时，可以支持流水线并行和张量并行。提升了多卡计算的效率，降低了单卡显存的门槛，可进行分布式推理。</li><li>支持流式输出</li><li>兼容 OpenAI API 服务</li></ul><h2 id="vllm本地部署大模型"><a href="#vllm本地部署大模型" class="headerlink" title="vllm本地部署大模型"></a>vllm本地部署大模型</h2><h3 id="安装配置vllm"><a href="#安装配置vllm" class="headerlink" title="安装配置vllm"></a>安装配置vllm</h3><p>本次我们使用docker在windows11+wsl2环境下部署vllm。在此之前需要安装好以下软件：</p><ul><li>cuda11.8. 可从<a href="https://developer.download.nvidia.cn/compute/cuda/11.8.0/local_installers/cuda_11.8.0_522.06_windows.exe" target="_blank" rel="noopener">cuda 11.8下载地址</a>下载。</li><li>cudnn11.x. 可从<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">cudnn下载列表</a>处下载cudnn1.x zip包。解压后需要将文件复制并覆盖到CUDA安装目录，默认为<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8</code></li><li>wsl2: 具体可参考网上的教程进行开启，下面只列大概步骤。<a href="https://developer.nvidia.com/cuda/wsl。" target="_blank" rel="noopener">https://developer.nvidia.com/cuda/wsl。</a><ul><li>需要在BIOS开启CPU虚拟化</li><li>需要在控制面板-程序-功能-开启hyper-V和wsl linux子系统</li><li>需要WSL安装内核升级包。<a href="https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi" target="_blank" rel="noopener">https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi</a></li><li>需要开启将wsl升级为wsl2</li><li>需要从app store安装ubuntu 22.04</li></ul></li><li>docker desktop. 可从<a href="https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe?utm_source=docker&amp;utm_medium=webreferral&amp;utm_campaign=docs-driven-download-win-amd64" target="_blank" rel="noopener">docker下载地址</a>下载。（需要魔法上网）</li></ul><p>建议将wsl内存限制进行修改，默认配置下，可用内存限制的较低，可能会影响服务运行速度。将以下内容保存到.wsl_config文件，并复制到C:\Users\你的用户名 目录下。内存和虚拟内存大小可以根据自己的电脑配置进行调整。</p><pre><code class="shell">[wsl2]memory=12GBswap=8GB</code></pre><h3 id="部署大模型"><a href="#部署大模型" class="headerlink" title="部署大模型"></a>部署大模型</h3><p>编写docker-compose文件，将以下代码存储到docker-compose.yml。</p><pre><code class="yaml">version: &quot;3&quot;services:    vllm-openai:        image: vllm/vllm-openai:latest        restart: always        deploy:            resources:                reservations:                    devices:                        - driver: nvidia                          count: all                          capabilities:                              - gpu        volumes:            - ./models:/opt/models        environment:            - DO_NOT_TRACK=1        ports:            - 8000:8000        ipc: host        command: --model /opt/models/qwen2_home_control --enable-prefix --served-model-name qwen2 --gpu-memory-utilization 0.7</code></pre><p>在docker-compose.yml同目录创建models目录，将上一篇blog中训练好的模型<code>qwen2_home_control</code>移动到<code>models</code>目录下，最终目录结构如下所示:</p><pre><code class="shell">├─docker-compose.yml└─models           └─qwen2_home_control</code></pre><p>启动容器即可。执行：</p><pre><code class="shell">docker compose up -d</code></pre><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>我们测试下调用是否正常。打开 <a href="http://127.0.0.1:8000/docs#/default/create_chat_completion_v1_chat_completions_post" target="_blank" rel="noopener">http://127.0.0.1:8000/docs#/default/create_chat_completion_v1_chat_completions_post</a> 即可查看当前服务的接口文档，点击Try is out，然后提交以下数据，即可测试服务是否正常。</p><pre><code class="json">{  &quot;model&quot;: &quot;qwen2&quot;,  &quot;messages&quot;: [    {      &quot;content&quot;: &quot;你是谁&quot;,      &quot;role&quot;: &quot;user&quot;    }]}</code></pre><p>正常返回内容：</p><pre><code class="json">{  &quot;id&quot;: &quot;chat-9cd47b2d0be94b1bad44a0c4e6c147c0&quot;,  &quot;object&quot;: &quot;chat.completion&quot;,  &quot;created&quot;: 1721831368,  &quot;model&quot;: &quot;qwen2&quot;,  &quot;choices&quot;: [    {      &quot;index&quot;: 0,      &quot;message&quot;: {        &quot;role&quot;: &quot;assistant&quot;,        &quot;content&quot;: &quot;我是 贾维斯，由 reece 开发的人工智能助手。我旨在为用户提供有关技术、科学、艺术和生活的高质量信息。&quot;,        &quot;tool_calls&quot;: []      },      &quot;logprobs&quot;: null,      &quot;finish_reason&quot;: &quot;stop&quot;,      &quot;stop_reason&quot;: null    }  ],  &quot;usage&quot;: {    &quot;prompt_tokens&quot;: 8,    &quot;total_tokens&quot;: 41,    &quot;completion_tokens&quot;: 33  }}</code></pre><h3 id="vllm参数说明"><a href="#vllm参数说明" class="headerlink" title="vllm参数说明"></a>vllm参数说明</h3><ul><li><code>--model</code>: 模型加载路径</li><li><code>--enable-prefix</code>: 启用prefix-cache。开启后可加速token生成。</li><li><code>--served-model-name</code>: 服务模型名称</li><li><code>DO_NOT_TRACK</code>: 关闭vllm使用统计和数据追踪。</li><li><code>--gpu-memory-utilization</code>: 最多占用多少比例的显存来进行缓存，默认为0.9，由于本机显卡还需要测试其他模型，这里暂且给0.7。</li></ul><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><ul><li>docker无法启动。如提示docker engine stopped：<ul><li>检查wsl2是否正常:执行<code>ubuntu.exe</code> 可正常进入linux环境。</li><li>检查CUDA 显卡驱动是否已经安装:执行<code>nvcc --version</code> 可显示cuda版本号。</li><li>开启window相关功能后是否已经重启。</li></ul></li><li>docker pull报错，网络链接超时。这个是因为docker需要魔法上网。开启全局代理即可。</li><li><code>xxxx.dll未加载</code> 下载DirectX repair增强版修复C++库后重启电脑后再试。</li><li>大模型返回的结果重复，异常：注意应该调用<code>v1/chat/completions</code>接口，而不是<code>v1/completions</code></li></ul><h2 id="接入dify"><a href="#接入dify" class="headerlink" title="接入dify"></a>接入dify</h2><p>首先将我们微调后并使用vllm部署的模型接入dify平台。打开dify网页，点击右上角用户—&gt;点击：模型—&gt;选择模型供应商<code>local ai</code>—&gt;添加模型—&gt;选择LLM—&gt;模型名称：qwen2—&gt;基础URL: <a href="http://192.168.1.6:8000（你的局域网IP地址）---&gt;模型类型：CHAT---&gt;保存。" target="_blank" rel="noopener">http://192.168.1.6:8000（你的局域网IP地址）---&gt;模型类型：CHAT---&gt;保存。</a></p><p>接下来我们创建一个空模板的聊天应用来测试接入情况。刷新下页面后，切换到工作室页面，创建空白应用，选择 聊天助手，输入名称和创建。</p><p>我们打开大模型测试来于默认的qwen2:7b比对效果。点击应用右上角的模型qwen2:7b后点击弹出框左下角的 多个模型进行调试，这时将模型2选择为我们刚才添加的模型。</p><p><img src="/photo_2024/llm_05_ops_test1.png" alt=""><br><img src="/photo_2024/llm_05_ops_test2.png" alt=""></p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>dify平台密码忘记：进入dify的docker-api-1容器的控制台，执行<code>flask reset-password</code>,即可重新配置密码。</li></ul><p><img src="/photo_2024/llm_05_ops_reset_password.png" alt=""></p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><blockquote><p>vllm项目： <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener">https://github.com/vllm-project/vllm</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大模型虽然非常智能，但是由于需要消耗大量GPU资源，在GPU性能无法提升时，如何提升大模型运行速度，让大模型运行更快呢？目前最好的方法是采用vllm部署来对大模型进行加速。本篇blog将使用vllm部署上一篇我们使用lora微调后导出的模型。&lt;/p&gt;
&lt;h2 id=&quot;本篇内容&quot;&gt;&lt;a href=&quot;#本篇内容&quot; class=&quot;headerlink&quot; title=&quot;本篇内容:&quot;&gt;&lt;/a&gt;本篇内容:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;vllm是什么&lt;/li&gt;
&lt;li&gt;vllm本地部署大模型&lt;ul&gt;
&lt;li&gt;安装配置vllm&lt;/li&gt;
&lt;li&gt;部署大模型&lt;/li&gt;
&lt;li&gt;测试&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;接入dify&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://www.javabin.cn/tags/study/"/>
    
      <category term="部署大模型" scheme="http://www.javabin.cn/tags/%E9%83%A8%E7%BD%B2%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="加速大模型" scheme="http://www.javabin.cn/tags/%E5%8A%A0%E9%80%9F%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="运维" scheme="http://www.javabin.cn/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="vllm" scheme="http://www.javabin.cn/tags/vllm/"/>
    
  </entry>
  
  <entry>
    <title>【大模型-04】增强大模型自身能力的方式：微调</title>
    <link href="http://www.javabin.cn/2024/2024_llm_04_fine_tuning.html"/>
    <id>http://www.javabin.cn/2024/2024_llm_04_fine_tuning.html</id>
    <published>2024-07-18T12:31:24.000Z</published>
    <updated>2024-07-26T22:02:39.029Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇博客讲了通过RAG，WORKFLOW和AGENT来增强大模型应用的能力，但是对于部分大模型不擅长的特殊领域问题，并不能很好的解决。本篇将通过实战微调qwen2:1.5b模型, 讲解如何通过微调来增加大模型自身能力，为大模型灌入指定的知识。演示通过llama-factory和编写代码两种方式来对大模型进行微调，实现修改大模型自我认知和为大模型添加智能家居控制功能。<br>同时演示大模型微调的一般顺序：环境准备—&gt;数据处理—&gt;微调—&gt;测试—&gt;导出模型—&gt;部署。</p><h2 id="本篇内容"><a href="#本篇内容" class="headerlink" title="本篇内容:"></a>本篇内容:</h2><ul><li>目标</li><li>微调是什么</li><li>为什么需要微调</li><li>微调的技术:<ul><li>全参微调</li><li>lora</li><li>q-lora</li></ul></li><li>准备模型</li><li>微调qwen2:1.5b<ul><li>准备环境</li><li>通过llama-factory一键微调<ul><li>什么是llama-factory</li><li>本地安装llama-factory</li><li>处理数据</li><li>配置微调参数</li><li>开始微调</li><li>测试效果</li><li>导出模型</li></ul></li><li>编写python脚本进行微调<ul><li>配置环境</li><li>编写数据处理脚本</li><li>编写微调脚本</li><li>开始微调</li><li>测试效果</li></ul></li></ul></li><li>部署模型</li></ul><a id="more"></a><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>先明确下我们微调大模型的目标，我们的第一个目标是修改大模型的认知，将大模型的名称和开发者修改为我们自己，这里我会将我本地的模型认知改为：<code>我是AI智能助手 贾维斯， 由reece开发</code>。第二个目标是为大模型添加智能家居控制功能，实现的效果是：当我们告诉大模型要帮我操作家电时，大模型以json格式返回解析到的设备名称，操作的动作以及要操作的目标数值等信息，我们有了这些数据后就可以编写工作流来控制智能家居。<br>比如我们 给大模型发送<code>请调小空气净化器的湿度到1</code>, 大模型应该回复</p><pre><code class="json">{  &quot;intent&quot;:&quot;CONTROL&quot;,  &quot;slots&quot;:[    {&quot;name&quot;:&quot;device&quot;,&quot;normValue&quot;:&quot;airCleaner&quot;,&quot;value&quot;:&quot;空气净化器&quot;},    {&quot;name&quot;:&quot;insType&quot;,&quot;normValue&quot;:&quot;set&quot;,&quot;value&quot;:&quot;调小&quot;},    {&quot;name&quot;:&quot;attr&quot;,&quot;normValue&quot;:&quot;humidity&quot;,&quot;value&quot;:&quot;湿度&quot;},    {&quot;name&quot;:&quot;attrValue&quot;,&quot;normValue&quot;:&quot;1&quot;,&quot;value&quot;:&quot;1&quot;}    ],    &quot;sample&quot;:&quot;请调小空气净化器的湿度到1&quot;}</code></pre><p>由于我们需要在家用环境下运行，GPU资源有限，且按照汽车圈大佬的PPT，车载端对端大模型只有20亿参数，既然2b模型对汽车行车数据处理都没有问题，那我们用1.5B模型作为智能家居控制应该是完全足够的。所以本次使用qwen2:1.5b模型进行微调，预计峰值显存占用8-10GB。</p><p>目标1最终效果，大模型认知修改：<br><img src="/photo_2024/llm_04_fine_tuning_target1.png" alt=""><br>目标2最终效果，智能家居控制数据解析：<br><img src="/photo_2024/llm_04_fine_tuning_target2.png" alt=""></p><h2 id="微调是什么"><a href="#微调是什么" class="headerlink" title="微调是什么"></a>微调是什么</h2><p>一种模型训练的方式，在已经预先训练好的的模型上，使用特定任务或数据集进行训练，来改变模型参数，从而让模型满足新任务的需求。通俗类比就是你买了一只训练好的小狗，你发现它会坐下，打滚，捡球，但是不会打篮球。然后你对它进行了训练，让它在以有基础上，还学会了打篮球，这就是你对小狗进行了微调。 </p><h2 id="为什么需要微调"><a href="#为什么需要微调" class="headerlink" title="为什么需要微调"></a>为什么需要微调</h2><p>我们通过RAG为大模型提供了可以查阅的知识，避免了大模型由于缺少上下文而胡言乱语；通过workflow对复杂任务处理步骤进行了分解，让模型可以更好的处理细分的任务；通过agent可以让模型自主生成任务编排和选择要使用的工具。但是，大模型自身能力并没有提升，比如让大模型从文段里提取数据，或者分类数据，我 们不管如何修改提示词，最终还是会发现，大模型并不能很好的完成这个任务。所以，需要通过微调，增强大模型指定方面的能力，让大模型更符合我们自己的需要。</p><h2 id="微调的技术"><a href="#微调的技术" class="headerlink" title="微调的技术:"></a>微调的技术:</h2><p>对大模型的微调，目前主要使用的方式有：全参微调，LoRA,QLoRA等。</p><h3 id="全参微调"><a href="#全参微调" class="headerlink" title="全参微调"></a>全参微调</h3><p>对于预训练模型全部参数进行微调训练，由于大模型包含的参数规模都比较大，若想对大模型进行全参微调，需要大量高端显卡才能实现，如果没有多张高端显卡，一般家用电脑显存都不能满足要求。</p><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>Low-Rank Adaptation（低秩适配），核心思想是在不改变原有模型权重的情况下，通过添加少量新参数来进行微调，改变原有模型处理特定问题的能力。它最大优势在于，与全参数微调相比，需要训练的参数更少，但仍能获得相当的性能。比如本篇blog中对qwen2 1.5b用lora方式进行微调，微调时峰值显存占用为8-10gb，在RTX4060 16gb下即可完成微调。</p><h3 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h3><p>它能够在节省内存的同时保持速度。其工作原理是首先将大模型进行量化，从而显著减少模型的显存占用。接着，使用LoRA方法对量化的大模型进行微调。</p><h2 id="微调qwen2-1-5b"><a href="#微调qwen2-1-5b" class="headerlink" title="微调qwen2:1.5b"></a>微调qwen2:1.5b</h2><h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><p>由于微调比使用模型进行推理预测所需要的硬件配置更高，所以要实现本篇blog的内容，你至少需要以下硬件配置：</p><ul><li>硬盘有大于50GB剩余空间</li><li>显卡有大于8GB 显存</li><li>为保证推理时间不至于过长，至少需要一款甜品级游戏显卡，如RTX3060 RTX4060以及更高配置的显卡。</li><li>为保证数据预处理时间不至少过长，电脑CPU 至少需要INTER I5以及以上CPU。</li></ul><p>需要安装以下软件环境：</p><ul><li>windows11系统</li><li>安装python3.11。直接python官网下载即可。<ul><li>检查点：执行<code>python -v</code>,可正常查看到python3.11的版本号</li></ul></li><li>安装CUDA和cudnn。<ul><li>安装CUDA:CUDA是一种协助“CPU任务分发+GPU并行处理”的编程模型/平台，用于加速GPU和CPU之间的计算。要安装CUDA, 可先通过执行<code>nvidia-smi|findstr CUDA</code>来查看当前显卡支持的CUDA版本号是多少，我的是12.5。<br>你安装的CUDA版本必须低与你显卡支持的CUDA版本，我这里安装的是CUDA11.8。可以从 <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a> 下载对应版本安装。如果想直接下载11.8版本，可以点击这个链接进行下载：<a href="https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_522.06_windows.exe" target="_blank" rel="noopener">https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_522.06_windows.exe</a></li><li>安装CUDNN：cuDNN是NVIDIA CUDA深度神经网络库，用于GPU加速深度神经网络。从 <a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a> 下载对应版本进行安装，注意cudnn与CUDA之间有版本对应关系，如果你安装的cuda为11.8版本，那你需要安装为cuda11.x版本适配的cudnn。我这里安装的是<code>cudnn 11.8.89</code>.</li><li>检查点：安装好后执行 <code>nvcc --version</code> 可以正常显示cuda和cudnn版本号。如下面输出所示：<pre><code class="shell">L:\code\github\fine_tuning&gt;nvcc --version nvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2022 NVIDIA CorporationBuilt on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022Cuda compilation tools, release 11.8, V11.8.89Build cuda_11.8.r11.8/compiler.31833905_0</code></pre></li></ul></li><li>安装pytorch: 执行<code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</code> 安装 cuda11.8对应的pytorch。注意需要安装cuda版本对应的torch版本</li><li>安装微调相关依赖：<code>pip install modelscope transformers datasets peft accelerate pandas ujson optimum</code></li></ul><p>我本地使用的软硬件环境配置如下：</p><ul><li>系统：windows11</li><li>显卡：RTX4060ti 16GB</li><li>内存：32GB DDR5 5200HZ</li><li>CPU: I5 13490F</li><li>PYTHON: 3.11</li><li>CUDA: 11.8</li><li>CUDNN: 11.8.89</li><li>torch: 2.2.2cu118</li><li>docker desktop, wsl2已安装</li></ul><h3 id="准备模型"><a href="#准备模型" class="headerlink" title="准备模型"></a>准备模型</h3><p>qwen2大模型提供了几种不同版本的模型以适应不同的任务，如:1.标记为base的为基座模型, 指未经过特定任务微调和优化的基础训练模型；2.标记为Instruct的为指令模型，专为遵循指令完成任务而进行优化；3.标记为chat的为聊天模型，专为聊天对话系统进行优化过。 </p><p>在进行微调时，可根据自身需求选择合适的基础模型进行微调，本次我们选择instruct模型进行指令微调。</p><p>如果你的显存够大，也可以选择7B或者更大参数规模的模型进行微调，模型规模和微调需要的显存预估如下图所示(来自:<a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener">LLaMA-Factory readme</a>)：<br><img src="/photo_2024/llm_04_fine_tuning_gpu.png" alt=""></p><p>下载qwen2 1.5b模型，可以通过git clone进行下载，或者编写代码来下载。这里我们使用python代码来从 <a href="https://www.modelscope.cn/models/qwen/Qwen2-1.5B-Instruct" target="_blank" rel="noopener">https://www.modelscope.cn/models/qwen/Qwen2-1.5B-Instruct</a> 下载。编写以下代码存入train.py并运行，运行后会将模型下载到train.py同目录的qwen目录下。</p><pre><code class="python">    from modelscope import snapshot_download    def get_model():        return snapshot_download(&quot;qwen/Qwen2-1.5B-Instruct&quot;, cache_dir=&quot;./&quot;, revision=&quot;master&quot;)    def train():        model_dir = get_model()        print(model_dir)    if __name__ == &#39;__main__&#39;:        train()</code></pre><h3 id="通过llama-factory一键微调"><a href="#通过llama-factory一键微调" class="headerlink" title="通过llama-factory一键微调"></a>通过llama-factory一键微调</h3><h4 id="什么是llama-factory"><a href="#什么是llama-factory" class="headerlink" title="什么是llama-factory"></a>什么是llama-factory</h4><p>一个大型语言模型统一高效微调框架，支持多种模型、集成方法和精度，以及先进算法和实用技巧。实际就是一个打包好了各种大模型微调步骤和方法的开源项目，让用户可以直接在web界面上进行选择后，自动生成配置来调用微调脚本进行微调。让用户不需要编写代码，就可以完成对大模型的训练。</p><h4 id="本地安装llama-factory"><a href="#本地安装llama-factory" class="headerlink" title="本地安装llama-factory"></a>本地安装llama-factory</h4><ul><li>同步llama-factory项目到本地：</li></ul><pre><code class="shell">  git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git  cd LLaMA-Factory  pip install -e &quot;.[torch,metrics]&quot;</code></pre><ul><li>运行llama-factory web界面, 运行成功后会自动调用浏览器打开web界面：</li></ul><pre><code class="shell">llamafactory-cli webui</code></pre><h4 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h4><p>开始训练前，我们需要准备好训练数据，本次我们采用SFT训练方式，即有监督的微调训练大模型。对于我们第一个目标，数据集直接采用llama-factory自带的<code>identity.json</code>进行修改即可。首先我们处理第一个数据集：</p><ul><li>找到<code>LLaMA-Factory\data\identity.json</code>, 全文替换两个字段，将<code>\{\{name\}\}</code>全部替换为你想修改的模型的人工智能助手的名称，将<code>\{\{author\}\}</code>全部替换为你想让模型认为的作者的名称，替换后保存文件即可。</li></ul><p>对于目标2的训练数据，huggingface上已经有人提供了智能家居相关的数据集，我们从<code>https://huggingface.co/datasets/Charles95/smart_home_control</code> 下载即可。</p><ul><li>下载数据集：执行<code>git clone https://huggingface.co/datasets/Charles95/smart_home_control</code> 将训练数据同步到本地。</li><li>数据转换：接着我们需要编写一个简单的脚本将csv数据转换为llama-facotry指令微调数据集类似的数据结构。对于指令微调数据集，我们需要将数据转换为<code>[{&quot;instruction&quot;: &quot;系统指令&quot;, &quot;input&quot;: &quot;人类输入&quot;, &quot;output&quot;:&quot;模型输出&quot;}]</code>这样的结构。其中instruction为系统指令，input为人类的输入语句，output为期望的模型输出。转换的脚本如下所示，将以下脚本复制到convert.py,修改数据路径后执行即可。</li></ul><pre><code class="python">    import csv    import json    def convert_train_data(path):        res = []        with open(path, &#39;rt&#39;, encoding=&#39;utf-8&#39;) as file:            for index, row in enumerate(csv.reader(file)):                if index == 0:                    continue                res.append({                    &#39;instruction&#39;: row[0],                    &#39;input&#39;: row[1],                    &#39;output&#39;: row[2]                })        with open(path + &#39;.json&#39;, &#39;wt&#39;, encoding=&#39;utf-8&#39;) as file:            file.write(json.dumps(res, ensure_ascii=False, indent=4))    if __name__ == &#39;__main__&#39;:        convert_train_data(path=r&#39;L:\code\py\home_control\smart_home_control\raw\train.csv&#39;)</code></pre><ul><li>为llama-factory添加训练数据：数据转换完成后，将<code>train.csv.json</code>改为<code>home_control.json</code>，然后复制到<code>LLaMA-Factory\data\</code>目录下，最后在数据集配置文件<code>dataset_info.json</code>里添加上我们的数据集,只需编辑json文件，加入<code>&quot;home_control&quot;: { &quot;file_name&quot;: &quot;home_control.json&quot; },</code>保存即可。<h4 id="配置微调参数"><a href="#配置微调参数" class="headerlink" title="配置微调参数"></a>配置微调参数</h4>开始训练前，我们需要配置好训练相关的参数，模型和数据集。我们打开刚开浏览器弹出的llama-factory web界面。刷新界面后，按下面的参数进行配置。</li><li>语言：下拉框中选择zh，将界面语言改为中文。</li><li>模型名称：下拉框中选择Qwen2-1.5B，准备微调qwen2:1.5b模型</li><li>模型路径：此处需要填写刚才下载的<code>Qwen2-1.5B-Instruct</code>模型的路径，我这里填的是<code>L:\code\py\ft_qwen1.5b\qwen\Qwen2-1___5B-Instruct</code></li><li>微调方法：lora</li><li>检查点路径：先不填，等微调完成后，修改此处即可加载微调后的模型</li><li>高级设置-量化等级：none。不进行量化。</li><li>高级设置-RoPE 插值方法： none</li><li>高级设置-加速方式: auto. 不进行加速。</li><li>训练-训练阶段： Supervised Fine-Tuning。有监督微调，即需要给模型标注好的数据集。</li><li>训练-数据路径：data。不用调整</li><li>训练-数据集：这里选择我们准备好的两个数据集 <code>identity和home_control</code></li><li>训练-学习率：5e-5。默认即可。</li><li>训练-批处理大小：改为4，越大训练越快，但是越占用显存，如果显存比较大可以改为2的倍数后再往小里改，如16 8 4 2等。</li><li>训练-训练轮数：3。改的越大训练的轮数越多，这里我本地测试，2轮后loss已经趋于稳定在0.04左右。</li><li>lora-使用权重分解的 LoRA: 打钩DoRA。lora的优化版，性能优于 LoRA。<a href="https://developer.nvidia.com/zh-cn/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/" target="_blank" rel="noopener">可参考：高性能微调解决方案 DoRA</a></li><li>LoRA-作用模块：此处不填即可。可配置只对模型哪些层进行微调，可以使用的模块：q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj</li><li>其他保持默认即可。</li></ul><h4 id="开始微调"><a href="#开始微调" class="headerlink" title="开始微调"></a>开始微调</h4><p>开始微调之前，我们先 点击，保存训练参数，将当前设置的参数保存到配置文件，下次使用时点击 载入配置文件 即可恢复当前微调配置。保存参数后，我们点击 预览命令，可查看微调使用的脚本命令。接下来，点击 开始训练 来进行训练。开始训练后，需要切换到刚才打开 llama-factory的命令行窗口，查看是否有报错。 如果有报错的话，需要先解决报错后重新开始训练。</p><ul><li>检查点：当控制台开始打印<code>{&#39;loss&#39;: 1.074231, &#39;grad_norm&#39;: 15.032596588134766, &#39;learning_rate&#39;: 0.0, &#39;epoch&#39;: 29.59, &#39;num_input_tokens_seen&#39;: 1541176}</code>时，说明微调已经开始并正常运行。<br>训练中我们需要查看训练损失（loss，可以理解为当前模型进行预测时，输出结果与标注结果的差异）的变化，如果loss接近于0.1且已经不在变小而只是上下细微波动，此时就可以认为训练结束了，可以中断任务，开始测试效果。训练中每隔100步会保存一次微调的权重，不用担心中断后需要重新开始。最终的训练损失如下：</li></ul><pre><code class="shell">{&#39;loss&#39;: 0.0391, &#39;grad_norm&#39;: 0.39489784836769104, &#39;learning_rate&#39;: 3.4807362379317025e-05, &#39;epoch&#39;: 1.85, &#39;num_input_tokens_seen&#39;: 1074992}</code></pre><h4 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h4><p>训练完成之后，我们在web界面点击 检查点路径，选择刚才训练完成的检查点，然后选择chat页面, 点击 加载模型。然后可以输入系统提示词和输入来进行测试模型效果。本地测试效果如下：<br>目标1测试效果，大模型认知修改：<br><img src="/photo_2024/llm_04_fine_tuning_target1.png" alt=""><br>目标2测试效果，智能家居控制数据解析：<br><img src="/photo_2024/llm_04_fine_tuning_target2.png" alt=""></p><p>如果我们想评估大模型效果，可以在Evaluate/Predict页面进行自动评估和输出报告。可以将home_control数据集里的test.csv也转换并加入到llama-facotry数据集，然后用该数据集作为测试集来进行验证模型效果。 </p><h4 id="导出模型"><a href="#导出模型" class="headerlink" title="导出模型"></a>导出模型</h4><p>切换到 export页面，输入导出目录，如<code>qwen2_home_control</code>，点击导出，即可导出训练好的模型。以<code>qwen2_home_control</code>为例，导出后的模型将存放在<code>LLaMA-Factory\qwen2_home_control</code>目录下。</p><p>导出的模型我们可以使用vllm或者用fastapi编写一个http server来进行部署。通过vllm进行模型部署和加速将在下一篇进行讲解，这里我们先编写一个简单的python预测脚本加载模型来进行测试。将以下脚本保存到predict.py，且将model_dir改为你自己导出的模型路径即可使用预测脚本进行测试。如果想更换系统指令，修改instruction变量即可。</p><pre><code class="python">    import torch    from transformers import AutoModelForCausalLM, AutoTokenizer    def run_predict(messages, model, tokenizer):        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)        model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(&#39;cuda&#39;)        generated_ids = model.generate(            model_inputs.input_ids,            max_new_tokens=512        )        generated_ids = [            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)        ]        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]    def predict(model_dir):        tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)        model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=&quot;auto&quot;, torch_dtype=torch.bfloat16)        instruction = &#39;你是一个聊天助手&#39;        print(&#39;请输入问题后按回车：&#39;)        while True:            messages = [                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: f&quot;{instruction}&quot;},                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input()}&quot;}            ]            response = run_predict(messages, model, tokenizer)            print(response)    if __name__ == &#39;__main__&#39;:        predict(model_dir=r&#39;L:\code\py\ft_qwen1.5b\LLaMA-Factory\qwen2_home_control&#39;)</code></pre><p>测试输出示例: <img src="/photo_2024/llm_04_fine_tuning_predict.png" alt=""></p><h3 id="编写python脚本进行微调"><a href="#编写python脚本进行微调" class="headerlink" title="编写python脚本进行微调"></a>编写python脚本进行微调</h3><p>上面我们尝试了使用llama-factory的web界面来进行微调训练，接下来我们不依赖任何外部脚本和工具，编写一个python训练脚本来对模型进行训练。</p><h4 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h4><p>使用之前安装好的python环境和依赖即可。</p><h4 id="编写微调脚本"><a href="#编写微调脚本" class="headerlink" title="编写微调脚本"></a>编写微调脚本</h4><p>我们编写一个微调脚本，按照：加载模型权重—&gt;配置lora—&gt;配置训练参数—&gt;加载训练数据—&gt;开始训练的顺序来进行模型的训练。将以下脚本保存到<code>train.py</code>,修改<code>local_dir</code>为下载qwen2:1.5b的根目录，将<code>train_data_paths</code>改为训练文件路径，运行脚本即可。</p><pre><code class="python">    import json    import torch    import pandas as pd    import tqdm    from modelscope import AutoTokenizer    from peft import LoraConfig, TaskType, get_peft_model    from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq    from datasets import Dataset    def load_data(tokenizer, train_data_paths):        max_length = 512        res = []        data = []        for path in train_data_paths:            with open(path, &#39;rt&#39;, encoding=&#39;utf-8&#39;) as file:                data.extend(json.loads(file.read()))        for item in tqdm.tqdm(data):            instruction = tokenizer(                f&quot;&lt;|im_start|&gt;system\n{item[&#39;instruction&#39;]}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{item[&#39;input&#39;]}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&quot;,                add_special_tokens=False,            )            response = tokenizer(f&quot;{item[&#39;output&#39;]}&quot;, add_special_tokens=False)            input_ids = instruction[&quot;input_ids&quot;] + response[&quot;input_ids&quot;] + [tokenizer.pad_token_id]            attention_mask = (                    instruction[&quot;attention_mask&quot;] + response[&quot;attention_mask&quot;] + [1]            )            labels = [-100] * len(instruction[&quot;input_ids&quot;]) + response[&quot;input_ids&quot;] + [tokenizer.pad_token_id]            if len(input_ids) &gt; max_length:                input_ids = input_ids[:max_length]                attention_mask = attention_mask[:max_length]                labels = labels[:max_length]            res.append({&quot;input_ids&quot;: input_ids, &quot;attention_mask&quot;: attention_mask, &quot;labels&quot;: labels})        return Dataset.from_list(res)    def train(local_dir, train_data_paths):        print(&#39;加载模型权重...&#39;)        tokenizer = AutoTokenizer.from_pretrained(fr&quot;{local_dir}\qwen\Qwen2-1___5B-Instruct&quot;, use_fast=False, trust_remote_code=True)        model = AutoModelForCausalLM.from_pretrained(rf&quot;{local_dir}\qwen\Qwen2-1___5B-Instruct&quot;, device_map=&quot;auto&quot;, torch_dtype=torch.bfloat16)        model.enable_input_require_grads()        print(&#39;配置lora...&#39;)        config = LoraConfig(            task_type=TaskType.CAUSAL_LM,            target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],            inference_mode=False,            r=8,            lora_alpha=16,            lora_dropout=0,        )        model = get_peft_model(model, config)        print(&#39;配置训练参数...&#39;)        args = TrainingArguments(            output_dir=&quot;./output/Qwen2_1_5&quot;,            per_device_train_batch_size=4,            gradient_accumulation_steps=4,            logging_steps=10,            num_train_epochs=2,            save_steps=100,            learning_rate=5e-5,            save_on_each_node=True,            gradient_checkpointing=True,            report_to=&quot;none&quot;,        )        print(&#39;加载训练数据...&#39;)        train_dataset = load_data(tokenizer, train_data_paths)        print(&#39;开始训练...&#39;)        trainer = Trainer(            model=model,            args=args,            train_dataset=train_dataset,            data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),        )        trainer.train()    if __name__ == &#39;__main__&#39;:        train(            local_dir=r&#39;L:\code\py\ft_qwen1.5b&#39;,            train_data_paths=[r&quot;L:\code\py\ft_qwen1.5b\LLaMA-Factory\data\home_control.json&quot;, r&quot;L:\code\py\ft_qwen1.5b\LLaMA-Factory\data\identity.json&quot;]        )</code></pre><h4 id="开始微调-1"><a href="#开始微调-1" class="headerlink" title="开始微调"></a>开始微调</h4><p>开始微调后，正常日志如图所示：<br><img src="/photo_2024/llm_04_fine_tuning_train.png" alt=""></p><h4 id="测试效果-1"><a href="#测试效果-1" class="headerlink" title="测试效果"></a>测试效果</h4><p>在刚才的predict.py中的加载模型代码的下方，再添加一行微调模型的加载代码即可<code>model = PeftModel.from_pretrained(model, model_id=&quot;./output/Qwen2_1_5/checkpoint-200&quot;)</code>,其中<code>./output/Qwen2_1_5/checkpoint-200</code>为保存的微调模型数据路径，最终的predict.py如下所示： </p><pre><code class="python">    import torch    from transformers import AutoModelForCausalLM, AutoTokenizer    def run_predict(messages, model, tokenizer):        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)        model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(&#39;cuda&#39;)        generated_ids = model.generate(            model_inputs.input_ids,            max_new_tokens=512        )        generated_ids = [            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)        ]        return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]    def predict(model_dir):        tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)        model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=&quot;auto&quot;, torch_dtype=torch.bfloat16)        model = PeftModel.from_pretrained(model, model_id=&quot;./output/Qwen2_1_5/checkpoint-200&quot;)        instruction = &#39;你是一个聊天助手&#39;        print(&#39;请输入问题后按回车：&#39;)        while True:            messages = [                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: f&quot;{instruction}&quot;},                {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input()}&quot;}            ]            response = run_predict(messages, model, tokenizer)            print(response)    if __name__ == &#39;__main__&#39;:        predict(mode_dir=r&#39;L:\code\py\ft_qwen1.5b\LLaMA-Factory\qwen2_home_control&#39;)</code></pre><h2 id="部署模型"><a href="#部署模型" class="headerlink" title="部署模型"></a>部署模型</h2><p>将在下一章讲解如何通过vllm部署和加速大模型。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><blockquote><p>llama-factory项目：<a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md" target="_blank" rel="noopener">https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md</a></p><p>chatglm修改自我认知的例子：<a href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/examples/alter_self_cognition.md" target="_blank" rel="noopener">https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/examples/alter_self_cognition.md</a></p><p>qwen微调脚本： <a href="https://github.com/QwenLM/Qwen/blob/main/finetune.py" target="_blank" rel="noopener">https://github.com/QwenLM/Qwen/blob/main/finetune.py</a></p><p>LoRA: Low-Rank Adaptation of Large Language Models: <a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener">https://arxiv.org/abs/2106.09685</a></p><p>高性能微调解决方案 DoRA：<a href="https://developer.nvidia.com/zh-cn/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning" target="_blank" rel="noopener">https://developer.nvidia.com/zh-cn/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning</a></p><p>智能家居控制数据集：<a href="https://huggingface.co/datasets/Charles95/smart_home_control" target="_blank" rel="noopener">https://huggingface.co/datasets/Charles95/smart_home_control</a></p><p>模型Qwen2-1.5B-Instruct：<a href="https://www.modelscope.cn/models/qwen/Qwen2-1.5B-Instruct" target="_blank" rel="noopener">https://www.modelscope.cn/models/qwen/Qwen2-1.5B-Instruct</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇博客讲了通过RAG，WORKFLOW和AGENT来增强大模型应用的能力，但是对于部分大模型不擅长的特殊领域问题，并不能很好的解决。本篇将通过实战微调qwen2:1.5b模型, 讲解如何通过微调来增加大模型自身能力，为大模型灌入指定的知识。演示通过llama-factory和编写代码两种方式来对大模型进行微调，实现修改大模型自我认知和为大模型添加智能家居控制功能。&lt;br&gt;同时演示大模型微调的一般顺序：环境准备—&amp;gt;数据处理—&amp;gt;微调—&amp;gt;测试—&amp;gt;导出模型—&amp;gt;部署。&lt;/p&gt;
&lt;h2 id=&quot;本篇内容&quot;&gt;&lt;a href=&quot;#本篇内容&quot; class=&quot;headerlink&quot; title=&quot;本篇内容:&quot;&gt;&lt;/a&gt;本篇内容:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;目标&lt;/li&gt;
&lt;li&gt;微调是什么&lt;/li&gt;
&lt;li&gt;为什么需要微调&lt;/li&gt;
&lt;li&gt;微调的技术:&lt;ul&gt;
&lt;li&gt;全参微调&lt;/li&gt;
&lt;li&gt;lora&lt;/li&gt;
&lt;li&gt;q-lora&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;准备模型&lt;/li&gt;
&lt;li&gt;微调qwen2:1.5b&lt;ul&gt;
&lt;li&gt;准备环境&lt;/li&gt;
&lt;li&gt;通过llama-factory一键微调&lt;ul&gt;
&lt;li&gt;什么是llama-factory&lt;/li&gt;
&lt;li&gt;本地安装llama-factory&lt;/li&gt;
&lt;li&gt;处理数据&lt;/li&gt;
&lt;li&gt;配置微调参数&lt;/li&gt;
&lt;li&gt;开始微调&lt;/li&gt;
&lt;li&gt;测试效果&lt;/li&gt;
&lt;li&gt;导出模型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;编写python脚本进行微调&lt;ul&gt;
&lt;li&gt;配置环境&lt;/li&gt;
&lt;li&gt;编写数据处理脚本&lt;/li&gt;
&lt;li&gt;编写微调脚本&lt;/li&gt;
&lt;li&gt;开始微调&lt;/li&gt;
&lt;li&gt;测试效果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;部署模型&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://www.javabin.cn/tags/study/"/>
    
      <category term="大模型" scheme="http://www.javabin.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="编程" scheme="http://www.javabin.cn/tags/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="微调" scheme="http://www.javabin.cn/tags/%E5%BE%AE%E8%B0%83/"/>
    
      <category term="lora" scheme="http://www.javabin.cn/tags/lora/"/>
    
      <category term="llama-factory" scheme="http://www.javabin.cn/tags/llama-factory/"/>
    
  </entry>
  
  <entry>
    <title>【大模型-03】通过dify学习大模型应用能力增强方式：RAG,WORKFLOW,AGENT</title>
    <link href="http://www.javabin.cn/2024/2024_llm_03_dify.html"/>
    <id>http://www.javabin.cn/2024/2024_llm_03_dify.html</id>
    <published>2024-07-17T11:12:44.000Z</published>
    <updated>2024-07-26T22:02:18.118Z</updated>
    
    <content type="html"><![CDATA[<p>当前的大模型主要来处理文本信息，它能够连续预测文本序列中的下一个词，以生成完整的回答。尽管以聊天的形式提供服务是大多数用户对这些大模型的普遍印象，但这种方法并不足以解决超出文本范畴的复杂问题。现实世界问题的解决通常涉及多个方面，包括私有知识的获取、意图的准确理解、任务的细分、任务的有序安排、工具的有效使用、结果的综合汇总以及数据的处理等。</p><p>然而，大型模型自身并不具备私有数据，例如本地文档或网页信息，同时它们通常支持的上下文长度有限，且无法主动调用工具，这限制了它们解决问题的能力。为了克服这些限制，引入了RAG（Retrieval-Augmented Generation，检索增强生成），WORKFLOW(工作流)和AGENT（智能体）技术，这些技术旨在提升模型处理复杂问题的能力。</p><p>本文将首先介绍这些关键技术，然后通过配置和体验dify来了解大模型的应用技术。最终，我们将通过workflow+RAG技术创建一个小说主角对话应用—我是韩跑跑，展示这些技术的实际应用和效果。</p><h2 id="本篇内容"><a href="#本篇内容" class="headerlink" title="本篇内容:"></a>本篇内容:</h2><ul><li>dify是什么</li><li>什么是RAG</li><li>RAG相关技术：<ul><li>embedding</li><li>知识库</li></ul></li><li>工作流</li><li>AGENT相关技术：<ul><li>任务编排</li><li>工具调用</li></ul></li><li>配置和体验dify<ul><li>本地部署dify</li><li>配置本地大模型和embedding模型</li><li>配置知识库</li><li>配置workflow</li></ul></li><li>应用<ul><li>创建一个小说角色扮演应用，可通过RAG检索知识库数据模拟小说中的角色与用户聊天。</li></ul></li><li>优化<ul><li>部署和调用本地embedding模型：BGE-1.5</li></ul></li><li>引入自身项目<ul><li>将应用嵌入个人网站</li><li>将应用作为后端服务</li></ul></li></ul><a id="more"></a><h2 id="dify是什么"><a href="#dify是什么" class="headerlink" title="dify是什么"></a>dify是什么</h2><p>dify是开源的大语言模型 (LLM) 应用开发平台，融合了后端即服务（Backend as Service）和 LLMOps 的理念，使开发者可以快速搭建生产级的生成式 AI 应用。 即使你是非技术人员，也能参与到 AI 应用的定义和数据运营过程中。</p><p>通俗的说就是一款让用户可以在web界面上简单创建大模型应用，使用大模型应用相关技术的平台。可以在dify平台通过拖拽和填写表单创建知识库，知识检索，任务编排，工作流配置，构建agent。</p><p>dify提供了直接云平台的访问和本地部署两种方式，本篇使用本地部署方式进行体验。其他问题可查阅dify<a href="https://docs.dify.ai/v/zh-hans" target="_blank" rel="noopener">相关文档</a>。</p><p>下图就是我创建的一个与小说主角对话的dify应用, 图片左边是工作流的编排，右边是调试窗口。用户输入后，程序会按照工作流开始执行，先查询知识库，再根据上下文回答问题，最后输出给用户。<br><img src="/photo_2024/llm_03_dify_firstlook.png" alt=""></p><p>提供类似功能的开源产品还有：<code>fastgpt, ragflow</code>等。</p><h2 id="什么是RAG"><a href="#什么是RAG" class="headerlink" title="什么是RAG"></a>什么是RAG</h2><p>RAG,即检索增强生成，是指为大模型提供数据源检索信息的能力。由于大模型不具有本地文本等相关数据，无法基于已有数据进行问题的回复。比如如果让大模型扮演一个客服，大模型无法回复关于公司产品的信息，如果让大模型告诉某公司财报信息，大模型也无法获取到最新的数据。 而RAG可以为大模型提供数据源，并通过<code>embedding</code>的方式检索到最相关的数据，让大模型基于这些检索出的上下文进行回复，以使大模型的回复更加符合实际，且防止大模型回复更新不及时或虚构的数据。实现RAG的常见的步骤为:</p><ul><li>1.对文档等数据进行分段。常见的分段方式为按段落分割，分割后按固定字数进行分段，并在分段时重叠一部分，以防止部分上下文语义因为拆分而被破坏。dify默认的分割方式为按段落分割，按500字进行分段，重叠50字。一般可编写python代码或者调用<code>langchain</code>来处理文本。</li><li>2.对分段数据进行<code>embedding</code>。将分段数据传给<code>embedding模型</code>，获取到分段数据语义的向量表示。你可以理解为将一段文本转换为一个向量，当查询时，如果输入问题转换的向量与某个分段文本的向量最相近，那么就可以认为输入问题与这条分段数据可能在描述同样的事情。处理中文数据上比较热门的embedding方式为：调用大模型提供的embedding接口或者调用embedding模型如<code>bge</code>来实现。</li><li>3.知识库管理。通过向量数据库保存文本和向量的关系，可通过向量快速检索引擎检索对应数据。比较热门的向量数据库有：<code>milvus</code>，<code>qdrant</code> 以及<code>postgresql</code>(支持普通sql和向量检索)，也可以使用本地文件向量检索库如<code>faiss</code>。</li><li>4.embedding检索：将用户输入问题传给embedding模型获取到语义向量，调用向量数据库查询出前几个最相似的数据。</li><li>5.将检索数据作为上下文来构造提示词：此时可将检索到的多个分段数据作为上下文，拼接到提示词里，让大模型智能根据提示词来回答问题且不要进行虚构。</li></ul><p>我们比对没有使用RAG和使用了RAG时，模型的回复，可以看到模型的回复已经符合已有数据里的真实情况，而不是胡言乱语。<br><img src="/photo_2024/llm_03_dify_why_rag.png" alt=""></p><h2 id="RAG相关技术："><a href="#RAG相关技术：" class="headerlink" title="RAG相关技术："></a>RAG相关技术：</h2><h3 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h3><p>embedding实际就是将一段现实世界的文本转换为向量表示的过程。实现了用几百个数值来表示一段文本，且对于语义相似的文本，他们的向量表示更相似。比如：<code>我吃饭了</code>和 <code>吃饭了吗？</code> 得到 向量表示就比<code>我在打篮球</code>更相似，这样在你输入 <code>吃饭了吗？</code> 进行查询时，就更可能查询到文段 <code>我吃饭了</code>而不是 <code>我在打篮球</code>。这里只讲可能帮助你应用这项技术的宏观原理，不再赘述具体的实现细节，有兴趣可以网络查询<code>embedding transfomer 原理</code>, <code>独热编码</code>, <code>词嵌入</code>等关键词进行了解。</p><p>embedding的应用离不开向量数据库，目前比较热门的向量数据库有milvus,qdrant，支持向量检索的数据库有postgresql,支持本地文件向量存储的库有faiss。对于选择哪款向量数据库，需要根据可提供的机器配置，公司使用的技术栈，项目规模等选择。<br>目前来看，如果你打算为公司技术栈引入一款高性能向量数据库且需要处理大量向量数据，milvus和qdrant都有不错的性能。而如果你的项目比较小，且不想引入新的向量数据库技术，或者不想部署一个向量数据库，而且已经使用了postgresql数据库，可使用postgresql数据提供的pgvector来进行向量检索。 如果你的项目非常小，不想搭建任何向量数据库，直接使用faiss就行。</p><p>如果想获取向量数据库性能报告，可参考：<a href="https://zilliz.com/vector-database-benchmark-tool?database=ZillizCloud%2CMilvus%2CElasticCloud%2CPgVector%2CPinecone%2CQdrantCloud%2CWeaviateCloud&amp;dataset=medium&amp;filter=none%2Clow%2Chigh&amp;tab=1" target="_blank" rel="noopener">https://zilliz.com/vector-database-benchmark-tool?database=ZillizCloud%2CMilvus%2CElasticCloud%2CPgVector%2CPinecone%2CQdrantCloud%2CWeaviateCloud&amp;dataset=medium&amp;filter=none%2Clow%2Chigh&amp;tab=1</a></p><h3 id="知识库"><a href="#知识库" class="headerlink" title="知识库"></a>知识库</h3><p>为大模型提供的上下文数据源。一般会将同样类型的文档放入同一知识库内，来方便为大模型提供更相关的上下文数据。</p><p>dify知识库配置界面：<br><img src="/photo_2024/llm_03_dify_datasource.png" alt=""></p><h2 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h2><p>将多个任务按程序流程进行执行，并获得最终输出。dify提供了低代码实现程序执行流程的界面，如blog开始的图片展示的就是通过拖拽和配置来创建的任务工作流。<br>在工作流中我们可以通过变量来引用上一个任务的输出，通过变量来引用上下文数据。</p><p>工作流常见的节点解释：</p><ul><li>开始： 开始节点，可以配置一些变量在后续流程中使用。</li><li>知识检索：通过查询变量（可以为用户输入或者上一个节点的输出）调用embedding模型获取到向量后在知识库中检索出前N个最相似文段。输出变量为result对象。<img src="/photo_2024/llm_03_dify_datasource_conf.png" alt=""></li><li>LLM: 通过变量和上下文构造提示词，调用大模型获取输出，输出变量为result对象。</li><li>直接回复：直接将变量内容或者指定文本返回为用户。</li><li>条件分支：即代码中的if else，可配置根据变量来让程序执行不同分支。</li><li>问题分类器：默认已经构造了提示词，会调用大模型来对输入文本进行分类，可根据不同分类执行不同的分支。</li><li>参数提取器：需要构造提示词，来调用大模型来对参数转换为指定json等结构，方便调用http等工具</li><li>结束：结束节点，输出指定变量</li></ul><h2 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h2><h3 id="任务编排"><a href="#任务编排" class="headerlink" title="任务编排"></a>任务编排</h3><p>通过提示词让大模型自主对任务进行拆分和规划，自动生成工作流。但是自己测试效果并不如人工编排工作流的效果更好，原因一个是本地部署的qwen7b对于这种复杂任务处理的并不好，并不一定会经常按照约定输出，如果换为最强大的gpt4o来规划任务，成本比较高。另一个原因是自动生成的工作流和期望的需求有点差异，自己编排更符合期望的需求。所以本篇只做了解即可，实际应用中还是采用自己编排任务。</p><h3 id="工具调用"><a href="#工具调用" class="headerlink" title="工具调用"></a>工具调用</h3><p>dify已经提供了各种工具以供大模型调用，在提示词中告诉模型哪些工具可以使用，让模型生成工作流和需要使用的工具后，程序自动执行工作流和调用工具，完成复杂任务，比如联网搜索，代码执行，图片转换等。</p><p>dify支持的部分工具如下图所示：<br><img src="/photo_2024/llm_03_dify_tools.png" alt=""></p><h2 id="配置和体验dify"><a href="#配置和体验dify" class="headerlink" title="配置和体验dify"></a>配置和体验dify</h2><p>接下来我们本地部署dify,并配置好blog开头所示的dify应用。</p><h3 id="本地部署dify"><a href="#本地部署dify" class="headerlink" title="本地部署dify"></a>本地部署dify</h3><ul><li>环境：我们在windows11下下安装和运行所有程序</li><li>安装docker: 首先你需要安装docker desktop, 这一步网络上有太多的教程，直接搜索<code>windows11安装docker desktop</code>即可获得相关教程。<ul><li>检查点：如果你能正常启动docker desktop，说明你已经完成这一步。</li></ul></li><li>安装git: 下载git并安装<ul><li>检查点：按win+r输入cmd打开命令行窗口，输入<code>git -v</code>执行，可正常获取到git程序的版本号，我的是<code>git version 2.43.0.windows.1</code></li></ul></li><li>本地安装dify</li></ul><p>执行以下代码：</p><pre><code class="shell">git clone https://github.com/langgenius/dify.gitcd dify/dockercp .env.example .envdocker compose up -d</code></pre><pre><code>- 检查点： 浏览器打开`http://127.0.0.1`，可正常看到首次用户注册界面。</code></pre><ul><li>注册用户并进入平台</li></ul><h3 id="配置本地大模型和embedding模型"><a href="#配置本地大模型和embedding模型" class="headerlink" title="配置本地大模型和embedding模型"></a>配置本地大模型和embedding模型</h3><p>开始之前我们需要将本地ollama启动的qwen2模型接入dify。</p><ul><li>点击右上角用户—&gt;点击设置—&gt;点击模型供应商</li><li>选择模型供应商 <code>ollama</code>—&gt;添加模型—&gt;选择LLM—&gt;输入模型名称：qwen2:7b—&gt;填写基础URL（这里需要填写ollama服务的ip和端口，IP地址为你电脑局域网内的ip地址，你可以按win+r输入cmd打开命令行窗口后执行<code>ipconfig</code>来获取本地ip地址，我的为:<a href="http://192.168.12.1:11434" target="_blank" rel="noopener">http://192.168.12.1:11434</a> ） —&gt;模型类型选择：对话—&gt;其他保持默认—&gt;点击保存。<ul><li>检查点：如果保存时没有提示错误，即完成。如果提示错误，需要检查你的ip和端口是否正常，以及本地ollama和qwen2模型是否已经启动。</li></ul></li><li>选择模型供应商<code>ollama</code>—&gt;添加模型—&gt;选择Text Embedding—&gt;模型名称：qwen2:7b—&gt;基础URL: <a href="http://192.168.12.1:11434---&gt;保存" target="_blank" rel="noopener">http://192.168.12.1:11434---&gt;保存</a><ul><li>检查点：如果保存时没有提示错误，即完成。如果提示错误，需要检查你的ip和端口是否正常，以及本地ollama和qwen2模型是否已经启动。</li></ul></li></ul><h3 id="配置知识库"><a href="#配置知识库" class="headerlink" title="配置知识库"></a>配置知识库</h3><p>这一步我们找一些文档存入知识库。</p><ul><li>选择数据源：点击知识库—&gt;创建知识库—&gt;导入已有文本—&gt;选择下载的小说进行上传（这里不建议直接放入整本小说，文本处理和embedding时间会比较长）</li><li>文本分段配置：选择索引方式：高质量—&gt;检索方式选择：向量检索—&gt;TOP K设置改为6—&gt;点击 保存并开始处理</li><li>查看知识库等待索引完成</li><li>测试召回：在知识库文档列表查看页面点击 召回测试—&gt;输入你想提问的问题—&gt;查看输出文档是否和给出的问题是否相关。（默认提供的embedding可能并不是很精准，我们后续会将dify的embedding模型改为本地部署的BGE模型）</li></ul><h3 id="配置工作流"><a href="#配置工作流" class="headerlink" title="配置工作流"></a>配置工作流</h3><p>上面步骤完成后，我们就可以创建一个应用来实现blog开始的需求了。</p><ul><li>创建应用：点击：工作室—&gt;点击：创建空白应用—&gt;选择：聊天助手，且勾选工作流编排；或者选择工作流—&gt;输入应用名字：我是韩跑跑—&gt;保存</li><li>实现工作流节点 知识检索：删除默认工作流，只保留开始节点后。创建新节点：知识检索，将当前节点链接到开始节点，然后配置输入变量为<code>sys.query</code>,即用户输入。知识库选择刚才创建的知识库。</li><li>实现工作流节点 LLM:创建LLM节点，将当前节点链接到知识检索节点。选择创建的模型qwen2:7b—&gt;上下文选择：知识检索—&gt;提示词输入：”你现在扮演的是韩立，是一名修仙者，你的性格是谨慎和小心的，回复问题时言简意核，回复时你应该以在下，阁下等玄幻小说风格回复。我给出的相关片段都是你的记忆。你只能基于给出的片段回答问题，如果无法回答，你应该表示自己记不清了。你必须尽力扮演韩立，不能让别人发现你回复的漏洞。比如我问他你：你是谁，你应该回答：在下韩立，阁下所为何事？  注意：你的回复最好保持在20个字内。越短越好“。然后输入斜杠<code>/</code>，在弹出的变量列表里选择<code>上下文</code>。</li><li>实现工作流节点 添加直接输出节点：创建直接输入节点，将当前节点链接到LLM。点击节点，输入<code>/</code>后选择变量LLM下的text。</li><li>调试工作流：点击：调试和预览—&gt;输入要提问的问题即可开始调试。<ul><li>问题1-回复问题和期望不一样：修改大模型提示词，将要求加入提示词，限制模型输出。</li><li>问题2-回复问题不符合知识库内容：可以通过查看回复下关联的知识库文段，来确认检索是否正常。如果检索的内容相关度较低，需要更换性能更好的embedding模型，</li><li>问题3-回复随机过大：如果回复感觉随机性过大，不稳定，可以点击LLM节点—&gt;点击qwen2模型–&gt;降低Temperature 参数。</li></ul></li></ul><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="创建一个小说角色扮演应用-我是韩跑跑"><a href="#创建一个小说角色扮演应用-我是韩跑跑" class="headerlink" title="创建一个小说角色扮演应用-我是韩跑跑"></a>创建一个小说角色扮演应用-我是韩跑跑</h3><p>在上面的操作完成后，我们点击 <code>发布</code>就可以创建好一个小说角色扮演的应用—我是韩跑跑。在工作室内选择 我是韩跑跑 应用，即可开始和小说主角进行聊天。</p><p><img src="/photo_2024/llm_03_dify_chat.png" alt=""></p><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>使用时发现一些问题，比如知识库检索太差了，经常检索出不相干的数据，这里我们换一个效果更好的embedding模型，并本地部署后接入dify。</p><h3 id="部署本地bge模型"><a href="#部署本地bge模型" class="headerlink" title="部署本地bge模型"></a>部署本地bge模型</h3><ul><li>安装好python3.11。可在python官网下载python3.11并安装。<ul><li>检查点：按win+r输入cmd打开命令行窗口，执行<code>python --version</code>后可正常看到python版本</li></ul></li><li>安装python依赖: 命令行执行<code>pip3 install modelscope fastapi FlagEmbedding pydantic uvicorn</code></li><li>创建embedding服务脚本：将以下python脚本保存到bge.py。以下脚本会自动从ModelScope下载bge-large-zh-v1.5模型，并保存到bge.py同目录的bge目录下。同时调用fastapi实现了一个简单的embedding接口。</li></ul><pre><code class="python">      import os      from fastapi import FastAPI      from modelscope import snapshot_download      from FlagEmbedding import FlagModel      from pydantic import BaseModel      from starlette.requests import Request      model_dir = snapshot_download(&quot;AI-ModelScope/bge-large-zh-v1.5&quot;, revision=&#39;master&#39;, local_dir=&#39;./bge&#39;)      model = FlagModel(model_dir, use_fp16=True)      app = FastAPI()      BATCH_SIZE = int(os.environ.get(&#39;EMBEDDING_BATCH_SIZE&#39;, 2))      class EmbeddingData(BaseModel):          input: str      @app.post(&quot;/embeddings&quot;)      async def get_embedding(data: EmbeddingData):          embedding = model.encode(data.input, batch_size=BATCH_SIZE)          return {              &#39;data&#39;: [{&#39;embedding&#39;: embedding.tolist(), &#39;index&#39;: 0, &#39;object&#39;: &#39;embedding&#39;}],              &quot;object&quot;: &quot;list&quot;,              &quot;model&quot;: &quot;bge&quot;,              &quot;usage&quot;: {                  &quot;prompt_tokens&quot;: 0,                  &quot;total_tokens&quot;: 0              }          }</code></pre><ul><li>启动embedding服务：在bge.py同目录，执行<code>uvicorn bge:app --host 0.0.0.0 --port 3001</code></li></ul><h3 id="接入dify"><a href="#接入dify" class="headerlink" title="接入dify"></a>接入dify</h3><ul><li>点击右上角用户—&gt;点击：模型—&gt;选择模型供应商<code>local ai</code>—&gt;添加模型—&gt;选择Text Embedding—&gt;模型名称：bge—&gt;基础URL: <a href="http://192.168.12.1:3001---&gt;保存" target="_blank" rel="noopener">http://192.168.12.1:3001---&gt;保存</a><ul><li>检查点：点击保存后dify会将字符<code>ping</code>传给Embedding服务，测试embedding接口输出是否正常，如果保存没有报错，那就说明此步骤已经完成。</li></ul></li><li>修改embedding方式：在知识库内选择设置—&gt;embedding模型选择：bge—&gt;保存</li><li>重新索引：点击:文档—&gt;点击文档列表右边的三个点—&gt;选择分段设置—&gt;保存并处理</li><li>测试召回</li><li>重新测试刚才的应用</li></ul><h2 id="引入自身项目"><a href="#引入自身项目" class="headerlink" title="引入自身项目"></a>引入自身项目</h2><h3 id="将dify应用嵌入个人网站"><a href="#将dify应用嵌入个人网站" class="headerlink" title="将dify应用嵌入个人网站"></a>将dify应用嵌入个人网站</h3><p>我们可以将这个应用嵌入网站作为一个网站助手来增加自己网站的功能，或者作为浏览器插件使用。下面我们导出创建的应用为chrome插件，之后我们可以在浏览器中每个网页的的右下角召唤出韩跑跑应用。</p><ul><li>导出嵌入代码：打开：韩跑跑应用—&gt;点击：发布下拉框的嵌入网站—&gt;选择第三个：chrome插件—&gt;安装chrome插件—&gt;复制提供的chatbot url—&gt;点击安装的dify插件并输入url—&gt;保存</li><li>测试：刷新任意页面后点击右下角的图标即可使用韩跑跑应用。</li></ul><p><img src="/photo_2024/llm_03_dify_chrome.png" alt=""></p><p>注意：虽然dify使用的是apache 2.0开源协议，但是有附加条件：</p><ul><li>1.你不能将dify做为和dify官方一样的平台为用户提供服务。</li><li>2.如果是将应用嵌入自己的网站，不能去掉dify logo。</li></ul><h3 id="将应用作为后端服务"><a href="#将应用作为后端服务" class="headerlink" title="将应用作为后端服务"></a>将应用作为后端服务</h3><p>我们可以将这个创建的应用作为一个后端服务，按dify的协议约定，这种模式是可以商用的。</p><ul><li>查看后端api文档：打开：我是韩跑跑应用—&gt;点击：发布下拉框的访问api。即可查看后端接口文档。 </li><li>按文档提供的方法调用对应接口即可。</li></ul><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><blockquote><p>向量数据库性能对比：<a href="https://zilliz.com/vector-database-benchmark-tool?database=ZillizCloud%2CMilvus%2CElasticCloud%2CPgVector%2CPinecone%2CQdrantCloud%2CWeaviateCloud&amp;dataset=medium&amp;filter=none%2Clow%2Chigh&amp;tab=1" target="_blank" rel="noopener">https://zilliz.com/vector-database-benchmark-tool?database=ZillizCloud%2CMilvus%2CElasticCloud%2CPgVector%2CPinecone%2CQdrantCloud%2CWeaviateCloud&amp;dataset=medium&amp;filter=none%2Clow%2Chigh&amp;tab=1</a></p><p>bge模型： <a href="https://www.modelscope.cn/models/AI-ModelScope/bge-large-zh-v1.5" target="_blank" rel="noopener">https://www.modelscope.cn/models/AI-ModelScope/bge-large-zh-v1.5</a></p><p>dify平台: <a href="https://docs.dify.ai/v/zh-hans" target="_blank" rel="noopener">https://docs.dify.ai/v/zh-hans</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当前的大模型主要来处理文本信息，它能够连续预测文本序列中的下一个词，以生成完整的回答。尽管以聊天的形式提供服务是大多数用户对这些大模型的普遍印象，但这种方法并不足以解决超出文本范畴的复杂问题。现实世界问题的解决通常涉及多个方面，包括私有知识的获取、意图的准确理解、任务的细分、任务的有序安排、工具的有效使用、结果的综合汇总以及数据的处理等。&lt;/p&gt;
&lt;p&gt;然而，大型模型自身并不具备私有数据，例如本地文档或网页信息，同时它们通常支持的上下文长度有限，且无法主动调用工具，这限制了它们解决问题的能力。为了克服这些限制，引入了RAG（Retrieval-Augmented Generation，检索增强生成），WORKFLOW(工作流)和AGENT（智能体）技术，这些技术旨在提升模型处理复杂问题的能力。&lt;/p&gt;
&lt;p&gt;本文将首先介绍这些关键技术，然后通过配置和体验dify来了解大模型的应用技术。最终，我们将通过workflow+RAG技术创建一个小说主角对话应用—我是韩跑跑，展示这些技术的实际应用和效果。&lt;/p&gt;
&lt;h2 id=&quot;本篇内容&quot;&gt;&lt;a href=&quot;#本篇内容&quot; class=&quot;headerlink&quot; title=&quot;本篇内容:&quot;&gt;&lt;/a&gt;本篇内容:&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;dify是什么&lt;/li&gt;
&lt;li&gt;什么是RAG&lt;/li&gt;
&lt;li&gt;RAG相关技术：&lt;ul&gt;
&lt;li&gt;embedding&lt;/li&gt;
&lt;li&gt;知识库&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;工作流&lt;/li&gt;
&lt;li&gt;AGENT相关技术：&lt;ul&gt;
&lt;li&gt;任务编排&lt;/li&gt;
&lt;li&gt;工具调用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;配置和体验dify&lt;ul&gt;
&lt;li&gt;本地部署dify&lt;/li&gt;
&lt;li&gt;配置本地大模型和embedding模型&lt;/li&gt;
&lt;li&gt;配置知识库&lt;/li&gt;
&lt;li&gt;配置workflow&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;应用&lt;ul&gt;
&lt;li&gt;创建一个小说角色扮演应用，可通过RAG检索知识库数据模拟小说中的角色与用户聊天。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;优化&lt;ul&gt;
&lt;li&gt;部署和调用本地embedding模型：BGE-1.5&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;引入自身项目&lt;ul&gt;
&lt;li&gt;将应用嵌入个人网站&lt;/li&gt;
&lt;li&gt;将应用作为后端服务&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://www.javabin.cn/tags/study/"/>
    
      <category term="大模型" scheme="http://www.javabin.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="程序" scheme="http://www.javabin.cn/tags/%E7%A8%8B%E5%BA%8F/"/>
    
      <category term="agent" scheme="http://www.javabin.cn/tags/agent/"/>
    
      <category term="dify" scheme="http://www.javabin.cn/tags/dify/"/>
    
      <category term="RAG" scheme="http://www.javabin.cn/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>【大模型-02】实现简单大模型agent--浏览器划词翻译工具</title>
    <link href="http://www.javabin.cn/2024/2024_llm_02_text_translate.html"/>
    <id>http://www.javabin.cn/2024/2024_llm_02_text_translate.html</id>
    <published>2024-07-16T15:33:11.000Z</published>
    <updated>2024-07-26T22:02:05.540Z</updated>
    
    <content type="html"><![CDATA[<p>最近看到一个概念比较火—大模型agent，我也体验了下<a href="https://www.coze.cn/" target="_blank" rel="noopener">扣子</a>和<a href="https://docs.dify.ai/" target="_blank" rel="noopener">dify</a>两个大模型agent创建平台，大模型agent给我的感觉就是一个以大模型为核心驱动的任务执行器，输入任务目标，它通过任务流和调用各种工具自动完成各个任务序列并最终完成指定目标。<br>目前来看它非常依赖大模型自身的能力，比如逻辑推理/任务拆分/任务实现等能力，如果有一个强大的大模型作为核心，那可以实现非常智能的agent。同时它依赖支持的工具种类，所支持的工具越多，那么它的能力越强。本篇blog来实现一个最简单的agent—浏览器划词翻译工具,<br>它不使用任何工具，任务序列也只有一个，使用的能力也只有大模型本身的语言翻译能力。最终实现的效果是：用户在浏览器上选中部分文本，agent自动将文本翻译为中文并在界面显示。</p><h2 id="本篇内容："><a href="#本篇内容：" class="headerlink" title="本篇内容："></a>本篇内容：</h2><ul><li>了解大模型agent概念</li><li>油猴脚本开发划词翻译工具</li><li>调用本地大模型实现自动翻译</li></ul><a id="more"></a><h2 id="任务目标"><a href="#任务目标" class="headerlink" title="任务目标"></a>任务目标</h2><p>由于目标较为简单，且需要在浏览器中运行，本篇blog不使用agent平台创建agent。将通过油猴脚本实现一个简单的浏览器划词翻译的大模型agent，自动将用户选中的文本翻译为中文并显示。 </p><p>最终实现效果：<img src="/photo_2024/llm_02_agent_translate.png" alt=""></p><h2 id="什么是大模型agent"><a href="#什么是大模型agent" class="headerlink" title="什么是大模型agent"></a>什么是大模型agent</h2><p>大模型agent是一个高级的人工智能系统，利用大模型来理解任务目标，并通过任务序列和调用各种工具完成任务目标。在目前看来，理想的大模型agent结构为：大模型核心+记忆能力+工具调用+任务规划。</p><ul><li>大模型核心：是大模型agent的大脑，负责信息处理，任务拆分，任务意图理解等重要功能。</li><li>记忆能力： 存储任务序列处理中的上下文信息。</li><li>工具调用： 通过调用多种工具来实现不同的任务步骤，比如浏览器调用，代码解释器执行，图片生成等。</li><li>任务规划： 通过工作流或者模型预测的方式编排任务序列，以实现最终的任务目标</li></ul><p><img src="/photo_2024/llm_02_agent_struct.png" alt=""></p><h2 id="什么是油猴插件"><a href="#什么是油猴插件" class="headerlink" title="什么是油猴插件"></a>什么是油猴插件</h2><p>油猴插件是一款免费的chrome浏览器插件。 可以替网页加入些新功能、修正网页错误、组合来自不同网页的数据。用户可以自己编写脚本来实现对网页页面的修改和增加功能。油猴插件通过chrome浏览器进行安装，<br>安装链接为：<a href="https://chromewebstore.google.com/detail/%E7%AF%A1%E6%94%B9%E7%8C%B4/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=zh-CN&amp;utm_source=ext_sidebar" target="_blank" rel="noopener">https://chromewebstore.google.com/detail/%E7%AF%A1%E6%94%B9%E7%8C%B4/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=zh-CN&amp;utm_source=ext_sidebar</a></p><h2 id="完整步骤"><a href="#完整步骤" class="headerlink" title="完整步骤"></a>完整步骤</h2><h3 id="整个功能的流程"><a href="#整个功能的流程" class="headerlink" title="整个功能的流程"></a>整个功能的流程</h3><ul><li>用户开启翻译插件。在页面右上角勾选启用按钮后，插件才开始工作，避免影响用户其他操作。</li><li>用户在选中一段文本并松开鼠标时，获取到选中的文本。</li><li>用待翻译文本构造提示词。</li><li>将提示词通过ajax发送给本地大模型服务。</li><li>解析流式返回的数据。</li><li>弹出提示框，显示翻译信息。</li><li>用户关闭提示框。</li><li>用户关闭插件。</li></ul><h3 id="实现油猴脚本"><a href="#实现油猴脚本" class="headerlink" title="实现油猴脚本"></a>实现油猴脚本</h3><p>由于脚本我已经实现了，就先直接放上最终脚本，在后续章节再对脚本开发做说明</p><pre><code class="js">// ==UserScript==// @name         自动翻译选中文本// @namespace    http://javabin.cn// @version      1.0// @description  自动翻译选中的文本// @author       reece// @match        *://*/*// @grant        GM_addStyle// @grant        GM_xmlhttpRequest// @connect      127.0.0.1// ==/UserScript==(function () {    &#39;use strict&#39;;    // CSS样式    GM_addStyle(`        .llm_trans_popup {            position: fixed;            background-color: #fff;            -webkit-font-smoothing: antialiased;            padding: 10px;            line-height: 1.8;            text-indent:2em;            letter-spacing;2px;            word-spacing: 5px;            border: 1px solid #ccc;            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);            border-radius: 5px;            z-index: 999999;            white-space: pre-wrap;            word-break: break-word;            font-family: -apple-system, blinkmacsystemfont, &quot;Helvetica Neue&quot;, helvetica, &quot;segoe ui&quot;, arial, roboto, &quot;PingFang SC&quot;, miui, &quot;Hiragino Sans GB&quot;, &quot;Microsoft Yahei&quot;, sans-serif;            font-weight: 400;            width: 30%;            display:none;            min-height: 10%;            right: 20px;            top: 20px;        }        .llm_trans_close {            position: absolute;            top: 5px;            right: 5px;            font-size: 20px;            cursor: pointer;        }        .llm_trans_close:hover {            color: red;        }    `);    async function translateByLLM(text) {        const url = &quot;http://127.0.0.1:11434/api/chat&quot;;        try {            const decoder = new TextDecoder();            GM_xmlhttpRequest({                method: &#39;POST&#39;,                url: url,                responseType: &#39;stream&#39;,                data: JSON.stringify({                    &#39;model&#39;: &#39;qwen2:7b&#39;,                    &#39;messages&#39;: [{ &#39;role&#39;: &#39;system&#39;, &#39;content&#39;: `你是一个专业英语翻译器，在接下来的对话中，请将我发送你的所有文本翻译为中文并输出，请不要回复任何其他内容，如果无法翻译，请原样输出。` }, { &#39;role&#39;: &#39;user&#39;, &#39;content&#39;: text.trim() }]                }),                onloadstart: async function (r) {                    const reader = r.response.getReader();                    resetText(&quot;&quot;);                    while (true) {                        const { value, done } = await reader.read();                        if (done) {                            break;                        }                        const data = decoder.decode(value).split(&quot;\n&quot;).filter(item =&gt; item.trim().length).map(item =&gt; JSON.parse(item.trim())[&#39;message&#39;][&#39;content&#39;]).join(&quot;&quot;);                        appendText(data);                    }                }            });        } catch (error) {            console.error(&#39;Error:&#39;, error);        }    }    function appendText(text) {        document.getElementById(&#39;translationText&#39;).textContent = `${document.getElementById(&#39;translationText&#39;).textContent}${text}`;    }    function resetText(text) {        document.getElementById(&#39;translationText&#39;).textContent = text;    }    function activePopup() {        document.querySelector(&quot;.llm_trans_popup&quot;).style.display = &#39;block&#39;;        resetText(&quot;正在翻译...&quot;);    }    function initCheckbox() {        const checkbox = document.createElement(&#39;input&#39;);        checkbox.type = &#39;checkbox&#39;;        checkbox.id = &#39;llm_trans_flag&#39;;        checkbox.checked = false; // 默认启用        const label = document.createElement(&#39;label&#39;);        label.htmlFor = &#39;llm_trans_flag&#39;;        label.textContent = &#39;划词翻译&#39;;        const container = document.createElement(&#39;div&#39;);        container.style.position = &#39;fixed&#39;;        container.style.top = &#39;5px&#39;;        container.style.right = &#39;5px&#39;;        container.style.zIndex = &#39;999999999&#39;;        container.appendChild(checkbox);        container.appendChild(label);        document.body.appendChild(container);    }    function initTranslationPopup() {        let popup = document.querySelector(&quot;.llm_trans_popup&quot;);        if (!popup) {            popup = document.createElement(&#39;div&#39;);            popup.className = &#39;llm_trans_popup&#39;;            popup.innerHTML = &#39;&lt;span class=&quot;llm_trans_close&quot;&gt;&amp;times;&lt;/span&gt;&lt;p id=&quot;translationText&quot;&gt;正在翻译...&lt;/p&gt;&#39;;            document.body.appendChild(popup);            // 关闭弹出窗口            const closeBtn = popup.querySelector(&#39;.llm_trans_close&#39;);            closeBtn.addEventListener(&#39;click&#39;, function () {                popup.style.display = &#39;none&#39;;            });        }        popup.style.display = &#39;none&#39;;    }    initCheckbox();    initTranslationPopup();    // 监听文本选中事件    document.addEventListener(&#39;mouseup&#39;, async function (event) {        if (!document.querySelector(&quot;#llm_trans_flag&quot;).checked) {            console.log(&quot;未启用翻译插件，不执行操作&quot;);            return;        }        const selectedText = window.getSelection().toString().trim();        if (selectedText) {            activePopup();            await translateByLLM(selectedText);        }    });})();</code></pre><h3 id="安装脚本"><a href="#安装脚本" class="headerlink" title="安装脚本"></a>安装脚本</h3><p>打开油猴插件，点击新建脚本，将上面的脚本复制到编辑器内并按ctrl+s保存，即可安装好脚本</p><h3 id="测试翻译功能"><a href="#测试翻译功能" class="headerlink" title="测试翻译功能"></a>测试翻译功能</h3><ul><li>打开网页，此时网页右上角会出现<code>划词翻译</code>的勾选框，勾选<code>划词翻译</code>。</li><li>按住鼠标左键选中浏览器中的文本，松开鼠标左键后会自动调用大模型进行翻译。</li><li>点击弹出框右上角x即可关闭翻译框</li></ul><h2 id="油猴脚本开发说明"><a href="#油猴脚本开发说明" class="headerlink" title="油猴脚本开发说明"></a>油猴脚本开发说明</h2><h3 id="定义脚本信息"><a href="#定义脚本信息" class="headerlink" title="定义脚本信息"></a>定义脚本信息</h3><p>在脚本的开头需要定义好脚本的相关信息，相关字段的说明如下</p><pre><code class="js">// ==UserScript==// @name         自动翻译选中文本       # 脚本的名称，会显示到脚本信息中// @namespace    http://javabin.cn   # 脚本的命名空间// @version      1.0                 # 脚本的版本，会显示到脚本信息中// @description  自动翻译选中的文本      #  描述，会显示到脚本信息中// @author       reece               # 作者，会显示到脚本信息中// @match        *://*/*             # 当匹配到哪些域名的网站时，该脚本会才被载入// @grant        GM_addStyle         # 申请的权限，此处申请了 样式注入的权限// @grant        GM_xmlhttpRequest   # 申请的权限，此处申请了  发起ajax请求的权限// @connect      127.0.0.1           # 对于启用内容安全策略的网页，自动修改内容安全策略并将该地址加入// ==/UserScript==</code></pre><ul><li>@connect 127.0.0.1：由于部分网站如github开启了内容安全策略（CSP），也就是浏览器在请求后端服务时，服务器会在HTTP请求头内返回：Content-Security-Policy: ‘github.com;xxx.com;’，来告诉浏览器哪些网站的ajax请求和资源加载可以被允许，这样的目的是为了<br>防止XSS注入攻击。由于油猴脚本在进行http请求时，请求的url和CSP设置不一致，就会导致请求被浏览器拒绝。所以需要通过这个设置来让油猴插件修改网页内容安全策略，让不拦截指定url。</li></ul><h3 id="初始化勾选框"><a href="#初始化勾选框" class="headerlink" title="初始化勾选框"></a>初始化勾选框</h3><p>我们需要一个勾选框来支持用户自主控制是否开启插件。 通过实现<code>initCheckbox</code>方法，通过js操作网页dom对象，在页面右上角添加一个勾选框和标签。</p><h3 id="初始化弹出框"><a href="#初始化弹出框" class="headerlink" title="初始化弹出框"></a>初始化弹出框</h3><p>通过油猴插件提供的<code>GM_addStyle</code>接口添加样式信息。通过调用<code>initTranslationPopup</code>初始化一个隐藏的翻译信息显示窗口。</p><h3 id="监听鼠标事件"><a href="#监听鼠标事件" class="headerlink" title="监听鼠标事件"></a>监听鼠标事件</h3><p>监听鼠标的弹起事件，当鼠标弹起时，检查是否有选中的内容，如果有的话，开始执行任务。</p><h3 id="大模型调用"><a href="#大模型调用" class="headerlink" title="大模型调用"></a>大模型调用</h3><p>通过油猴插件通过的<code>GM_xmlhttpRequest</code>将提示词和要翻译内容POST给大模型服务接口，接着对流式返回的数据进行解析处理，将内容显示到弹出框。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><blockquote><p>油猴插件： <a href="https://chromewebstore.google.com/detail/%E7%AF%A1%E6%94%B9%E7%8C%B4/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=zh-CN&amp;utm_source=ext_sidebar" target="_blank" rel="noopener">https://chromewebstore.google.com/detail/%E7%AF%A1%E6%94%B9%E7%8C%B4/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=zh-CN&amp;utm_source=ext_sidebar</a></p><p>dify平台: <a href="https://docs.dify.ai/" target="_blank" rel="noopener">https://docs.dify.ai/</a></p><p>扣子平台：<a href="https://www.coze.cn/home" target="_blank" rel="noopener">https://www.coze.cn/home</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近看到一个概念比较火—大模型agent，我也体验了下&lt;a href=&quot;https://www.coze.cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;扣子&lt;/a&gt;和&lt;a href=&quot;https://docs.dify.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;dify&lt;/a&gt;两个大模型agent创建平台，大模型agent给我的感觉就是一个以大模型为核心驱动的任务执行器，输入任务目标，它通过任务流和调用各种工具自动完成各个任务序列并最终完成指定目标。&lt;br&gt;目前来看它非常依赖大模型自身的能力，比如逻辑推理/任务拆分/任务实现等能力，如果有一个强大的大模型作为核心，那可以实现非常智能的agent。同时它依赖支持的工具种类，所支持的工具越多，那么它的能力越强。本篇blog来实现一个最简单的agent—浏览器划词翻译工具,&lt;br&gt;它不使用任何工具，任务序列也只有一个，使用的能力也只有大模型本身的语言翻译能力。最终实现的效果是：用户在浏览器上选中部分文本，agent自动将文本翻译为中文并在界面显示。&lt;/p&gt;
&lt;h2 id=&quot;本篇内容：&quot;&gt;&lt;a href=&quot;#本篇内容：&quot; class=&quot;headerlink&quot; title=&quot;本篇内容：&quot;&gt;&lt;/a&gt;本篇内容：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;了解大模型agent概念&lt;/li&gt;
&lt;li&gt;油猴脚本开发划词翻译工具&lt;/li&gt;
&lt;li&gt;调用本地大模型实现自动翻译&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://www.javabin.cn/tags/study/"/>
    
      <category term="大模型" scheme="http://www.javabin.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="程序" scheme="http://www.javabin.cn/tags/%E7%A8%8B%E5%BA%8F/"/>
    
      <category term="agent" scheme="http://www.javabin.cn/tags/agent/"/>
    
  </entry>
  
  <entry>
    <title>【大模型-01】本地部署qwen2模型+集成聊天界面ChatGPT-Next-Web（无显卡也可运行）</title>
    <link href="http://www.javabin.cn/2024/2024_llm_01_first_look.html"/>
    <id>http://www.javabin.cn/2024/2024_llm_01_first_look.html</id>
    <published>2024-07-15T12:00:24.000Z</published>
    <updated>2024-07-26T22:01:54.708Z</updated>
    
    <content type="html"><![CDATA[<p>本系列内容从自己的笔记整理，阅读难度会从新手小白逐渐递进。所有内容如无必要不会研究或者分析具体的实现原理，只从工程实现的角度用大模型解决实际问题。</p><h2 id="本篇内容："><a href="#本篇内容：" class="headerlink" title="本篇内容："></a>本篇内容：</h2><ul><li>大模型相关基础概念解释</li><li>在本地部署开源大模型qwen2，并与大模型进行聊天。不限制设备类型和是否有显卡</li><li>安装聊天界面ChatGPT-Next-Web，并通过该界面与大模型交互</li></ul><a id="more"></a><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><ul><li>qwen2: qwen2是阿里云发布的开源模型，性能超过美国最强的开源模型Llama3-70B，也超过众多中国闭源大模型，性能非常好，所以本次选择qwen2来进行本地部署</li><li>模型参数2b 7b：2b 7b等代表了模型的参数规模，2b即20亿参数，7b即70亿参数，可类比为人脑的神经元数量（实际指模型中的变量数量，它们在训练过程中被调整以适应训练数据）。选择过大的模型，需要更大的显存才能运行。</li><li>量化/quantization： 模型参数本来以16位浮点数表示，为了速度和压缩显存占用，在舍弃一定精度的情况下，将16位浮点数映射到 8位 4位 整数/浮点数上。</li><li>q4_0量化： 将原有参数转换为4比特0位小数。本次使用的是q4_0量化。</li><li>上下文长度/context length: 代表模型最大支持的记忆长度，为token数量，在这个长度内，模型可以基于已有对话回复当前话题。</li><li>ChatGPT-Next-Web：与大模型交互的聊天界面，类似chatgpt web界面，但是提供了更丰富的配置功能。</li></ul><h2 id="安装ollama"><a href="#安装ollama" class="headerlink" title="安装ollama"></a>安装ollama</h2><p>ollama已经封装好了部署模型的所有步骤，且下载速度非常快，安装ollama后可以很容易的部署各个开源大模型。</p><h3 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h3><p>在开始之前需要配置一些环境变量，程序安装好后会用到这些环境变量。默认以windows进行说明。</p><ul><li>windows下按<code>win+R</code>键弹出运行窗口，输入cmd回车</li><li>在弹出的命令行窗口内输入：<code>setx OLLAMA_MODELS D:\models</code>, 来修改环境变量OLLAMA_MODELS，将模型下载位置修改到D:\models目录，你也可以改为其他容量比较大的磁盘下的目录</li><li>设置ollama服务启动配置：在弹出的命令行窗口内输入执行</li></ul><pre><code class="cmd">setx OLLAMA_HOST 0.0.0.0setx OLLAMA_ORIGINS *</code></pre><ul><li>关闭命令窗口</li></ul><h3 id="下载安装ollama"><a href="#下载安装ollama" class="headerlink" title="下载安装ollama"></a>下载安装ollama</h3><ul><li>windows：下载 <a href="https://ollama.com/download/OllamaSetup.exe" target="_blank" rel="noopener">https://ollama.com/download/OllamaSetup.exe</a> 后双击安装包进行安装</li><li>linux：进入bash后执行<code>curl -fsSL https://ollama.com/install.sh | sh</code></li><li>mac: 下载 <a href="https://ollama.com/download/Ollama-darwin.zip" target="_blank" rel="noopener">https://ollama.com/download/Ollama-darwin.zip</a></li></ul><h3 id="显卡类型和模型选择"><a href="#显卡类型和模型选择" class="headerlink" title="显卡类型和模型选择"></a>显卡类型和模型选择</h3><p>ollama支持在CPU上运行开源大模型，但是运行速度比较慢，如果你的电脑确实没有显卡，建议安装 qwen2:1.5b。如果你的显卡显存大于6GB，建议安装qwen2:7b。</p><ul><li>无显卡: qwen2:1.5b,以CPU运行. 实测在云主机1G内存 1CPU下，qwen2:1.5b模型运行速度为10tokens/s左右</li><li>显卡显存小于6GB: qwen2:1.5b,以GPU运行</li><li>显卡显存大于6GB: qwen2:7b 以GPU运行。实测在I5+32GB+RTX4060TI16GB显存下，qwen2:7b模型运行速度非常快，deepseek-coder-v2:15b也运行很快。</li></ul><h2 id="安装本地模型"><a href="#安装本地模型" class="headerlink" title="安装本地模型"></a>安装本地模型</h2><ul><li>windows下按<code>win+R</code>键弹出运行窗口，输入cmd回车</li><li>命令行窗口内输入：<code>ollama run qwen2:7b</code>回车。如果是安装1.5b，输入 <code>ollama run qwen2:1.5b</code>。后续运行模型也是执行同样的命令。</li><li>等待模型下载完毕后会，在命令行窗口内即可和大模型进行聊天。</li></ul><p><img src="/photo_2024/llm_01_cmd.png" alt=""></p><h3 id="查看本地模型信息"><a href="#查看本地模型信息" class="headerlink" title="查看本地模型信息"></a>查看本地模型信息</h3><ul><li>新开一个命令行窗口执行 <code>ollama show qwen2:7b</code>可查看当前模型的信息</li></ul><h3 id="接口调用"><a href="#接口调用" class="headerlink" title="接口调用"></a>接口调用</h3><p>测试命令行调用chat接口，可以看到接口已经可以访问</p><pre><code class="cmd">curl http://localhost:11434/api/chat -d &quot;{\&quot;model\&quot;:\&quot;qwen2:7b\&quot;,\&quot;messages\&quot;:[{\&quot;role\&quot;:\&quot;user\&quot;,\&quot;content\&quot;:\&quot;你是谁\&quot;}]}&quot;</code></pre><h3 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h3><ul><li>查看当前安装的模型： <code>ollama list</code></li><li>安装其他模型：在 <a href="https://ollama.com/library" target="_blank" rel="noopener">https://ollama.com/library</a> 上查询模型名称，执行<code>ollama run xx:xx</code>进行安装，如安装gemma2:9b模型，<code>ollama run gemma2:9b</code></li><li>查询运行的模型：<code>ollama ps</code></li></ul><h2 id="集成ChatGPT-Next-Web界面"><a href="#集成ChatGPT-Next-Web界面" class="headerlink" title="集成ChatGPT-Next-Web界面"></a>集成ChatGPT-Next-Web界面</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>由于是大模型初次体验，为了简单且节约时间，这里直接使用打包好的程序安装。</p><ul><li>windows: 下载 <a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases/download/v2.13.0/NextChat_2.13.0_x64-setup.exe" target="_blank" rel="noopener">https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases/download/v2.13.0/NextChat_2.13.0_x64-setup.exe</a> 安装</li><li>linux: 下载 <a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases/download/v2.13.0/next-chat_2.13.0_amd64.deb" target="_blank" rel="noopener">https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases/download/v2.13.0/next-chat_2.13.0_amd64.deb</a> 安装</li><li>mac: 下载 <a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases/download/v2.13.0/next-chat_2.13.0_amd64.AppImage" target="_blank" rel="noopener">https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/releases/download/v2.13.0/next-chat_2.13.0_amd64.AppImage</a> 安装</li></ul><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>打开ChatGPT-Next-Web界面 设置，进行基础配置：</p><ul><li>配置服务商：选择Open Ai</li><li>自定义模型名：选择 qwen2:7b</li><li>模型：选择 qwen2:7b</li><li>接口地址：<a href="http://127.0.0.1:11434" target="_blank" rel="noopener">http://127.0.0.1:11434</a></li></ul><p>关闭配置页面。所有工作已经完成， 可以试试在ChatGPT-Next-Web 里点击 新的聊天，来与大模型进行聊天了。在聊天时可以选择面具来使用预制的提示词让大模型进行角色扮演，以更好的完成指定的工作。</p><p><img src="/photo_2024/llm_next_web.png" alt=""></p><h2 id="问题解决："><a href="#问题解决：" class="headerlink" title="问题解决："></a>问题解决：</h2><ul><li>ChatGPT-Next-Web无法聊天,提示<code>failed to fetch</code>: 可能是你没有执行<code>setx OLLAMA_ORIGINS *</code>，或者是你搞乱了步骤，先安装了ollama,然后在执行环境变量设置后没有重启ollama.<ul><li>解决方法-查看环境变量是否正常：执行<code>set OLLAMA_ORIGINS</code> 检查输出是否是<code>OLLAMA_ORIGINS=*</code></li><li>解决方法-重启ollama，且重新打开新命令行窗口运行ollma run。</li></ul></li><li>ChatGPT-Next-Web无法聊天,提示<code>gpt3.5tubo xxxx</code>: 选择模型有误，在聊天窗口点击模型切换按钮，选择qwen2:7b模型</li><li>模型运行缓慢：检查显卡是否正常<ul><li>显卡检查：执行<code>nvidia-smi</code>查看显存占用和显卡使用情况</li></ul></li></ul><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><blockquote><p>ollama官网： <a href="https://ollama.com/" target="_blank" rel="noopener">https://ollama.com/</a></p><p>ollama-FAQ: <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md" target="_blank" rel="noopener">https://github.com/ollama/ollama/blob/main/docs/faq.md</a></p><p>qwen2文档：<a href="https://qwen.readthedocs.io/en/latest/getting_started/quickstart.html" target="_blank" rel="noopener">https://qwen.readthedocs.io/en/latest/getting_started/quickstart.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本系列内容从自己的笔记整理，阅读难度会从新手小白逐渐递进。所有内容如无必要不会研究或者分析具体的实现原理，只从工程实现的角度用大模型解决实际问题。&lt;/p&gt;
&lt;h2 id=&quot;本篇内容：&quot;&gt;&lt;a href=&quot;#本篇内容：&quot; class=&quot;headerlink&quot; title=&quot;本篇内容：&quot;&gt;&lt;/a&gt;本篇内容：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;大模型相关基础概念解释&lt;/li&gt;
&lt;li&gt;在本地部署开源大模型qwen2，并与大模型进行聊天。不限制设备类型和是否有显卡&lt;/li&gt;
&lt;li&gt;安装聊天界面ChatGPT-Next-Web，并通过该界面与大模型交互&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://www.javabin.cn/tags/study/"/>
    
      <category term="大模型" scheme="http://www.javabin.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="qwen2" scheme="http://www.javabin.cn/tags/qwen2/"/>
    
      <category term="程序" scheme="http://www.javabin.cn/tags/%E7%A8%8B%E5%BA%8F/"/>
    
      <category term="ChatGPT-Next-Web" scheme="http://www.javabin.cn/tags/ChatGPT-Next-Web/"/>
    
  </entry>
  
  <entry>
    <title>海岛躺平生活，玻璃海，潜水，游艇出海，斯米兰群岛--记泰国普吉岛自由行的10日</title>
    <link href="http://www.javabin.cn/2024/thailand.html"/>
    <id>http://www.javabin.cn/2024/thailand.html</id>
    <published>2024-05-13T12:00:24.000Z</published>
    <updated>2024-07-19T17:12:30.365Z</updated>
    
    <content type="html"><![CDATA[<p>人生3万多天，到底该怎么样度过一生？我觉得人生应该是多彩的，而不是一种颜色，总要有一段时间在经历人生低谷，有一段时间在努力实现自己的想法。但也需要一段时间去忘掉一切，去体验不一样的人生。换一种环境，换一种生活态度，体验不一样的异域生活。躺在海边看落日晚霞，乘着船感受海风轻抚，感受浪花拍岸；去远离大陆的海岛边潜入海底，和五彩斑斓的鱼群一起，只活在当下，感受当下。</p><h2 id="速读主要行程安排"><a href="#速读主要行程安排" class="headerlink" title="速读主要行程安排"></a>速读主要行程安排</h2><ul><li>卡塔泳池别墅酒店躺平5日。白天泳池游泳，体验泰式马杀鸡，海边沙滩躺平，酒店躺平，泳池边沙滩椅休息，感受当地美食；傍晚去感受卡塔日落晚霞，逛卡塔夜市，主街道逛街。</li><li>巴东海滩4日：白天：一日逛街；一日出海皇帝岛，珊瑚岛，神仙半岛；一日出海斯米兰群岛；晚上：酒店无边泳池落日，逛巴东马林夜市，感受当地美食</li><li>普集镇1日：普吉老街打卡拍照，远离景区感受当地美食和风情，扫货当地特产或美食带走</li></ul><h2 id="读完本篇将知道（目录）："><a href="#读完本篇将知道（目录）：" class="headerlink" title="读完本篇将知道（目录）："></a>读完本篇将知道（目录）：</h2><ul><li>主要行程安排</li><li>主要需要考虑的？</li><li>10日行程安排都有哪些体验？</li><li>第一次出国海岛游如何选岛？选择巴厘岛还是普吉岛？</li><li>泰国入境需要注意？</li><li>如何选择普吉岛海滩？</li><li>斯米兰群岛如何选船？</li><li>皇帝岛如何选船？</li><li>如何订外卖？</li><li>如何选择酒店？</li><li>离开时的购物？</li><li>退税？</li></ul><a id="more"></a><h2 id="主要需要考虑的"><a href="#主要需要考虑的" class="headerlink" title="主要需要考虑的"></a>主要需要考虑的</h2><p>从6年前就打算安排一次出国旅行，但是因为各种事情，只申请了护照就搁置了。这次由于有充足的时间，就打算完成之前的计划。在开始规划之前，我对于这次行程总的想法是：</p><ul><li>1.地点：必须是海边或者海岛</li><li>2.安全：考虑目前的国际形式，和自身的人生安全，可选的国家必须足够安全，或者说从公开资料+心理上都可以认同是足够安全的。</li><li>3.行程：行程必须尽可能宽松，不特种兵，以感受不一样的生活为主，不打卡不必要的人造景点。大于50%的时间无强制的景点打卡目标，可以自由活动，感受当地风土人情和无任何压力的海岛 阳光 沙滩的躺平生活。</li><li>4.金钱成本预算：小于2w。</li><li>5.时间成本预算：总10-15天。搭乘飞机时间小于5-6小时，如果高于这个时间，必须安排中转休息。避开国内节假日。</li><li>6.天气：必须是当季旱季，且大部分时间是晴天。</li><li>7.当地体验：旅游开发成熟，有各种基础设施，但没有过度开发。</li></ul><p>各个主要景点选择的依据：</p><ul><li>泰国：比较安全，没有不稳定因素，国人去的比较多，而且又是免签的，且距离上比较近。</li><li>普吉岛：4月是旱季 5月开始是雨季，4月末或者5月初去刚好是旱季和雨季的交接。附近就有各种小岛可以去玩水和潜水。开发比较成熟。</li><li>珊瑚岛-皇帝岛-神仙半岛：距离比较近，容易前往；水质比较好，海底生物较多，潜水体验好；行程比较轻松，可以安排船上活动；可以在神仙半岛附近欣赏海上落日；不需要早起。</li><li>斯米兰群岛：泰国顶流海岛，水质非常好，景色超级好；缺点：距离较远，且需要早起。</li><li>普集镇：可观光打卡拍照，可感受当地风土人情，购物方便。</li></ul><h2 id="10日行程安排都有哪些体验？"><a href="#10日行程安排都有哪些体验？" class="headerlink" title="10日行程安排都有哪些体验？"></a>10日行程安排都有哪些体验？</h2><ul><li>地点-卡塔海滩(day1-day5)：卡塔海滩相较于巴东海滩，大部分是自由行的俄罗斯人，他们在酒店或沙滩一躺就是一下午。海滩上都是躺平度假的，海上活动不多。<ul><li>度假酒店：卡塔海滩比较安静，建筑都比较新，酒店有很多直通泳池的房间和别墅可以选择，非常适合度假。<br><img src="/photo_2024/kata_resort.jpg" alt=""></li><li>卡塔海滩日落：傍晚时卡塔海滩的日落和晚霞非常美。<br><img src="/photo_2024/kata_setting_sun.jpg" alt=""></li><li>卡塔夜市：晚上的可以去夜市，酒吧街体验当地风情，也可以在街道逛街，或者在餐厅享用当地美食。<br><img src="/photo_2024/kata_yeshi.jpg" alt=""></li><li>服务-马杀鸡：来泰国必须感受下当地的特色服务：泰式按摩。可以让你全身得到放松。</li></ul></li><li>地点-巴东海滩(day6-day9)：巴东比较嘈杂，好多建筑比较破旧和拥挤，适合喜欢热闹的人。<ul><li>马林夜市：美食非常多，面积比较大，非常热闹<br><img src="/photo_2024/bd_mlys.jpg" alt=""></li><li>巴东夜市：美食逛街</li><li>巴东海滩：水质感觉比卡塔反而好一点，可以玩的海上活动比较多。</li><li>西蒙人妖秀：老少皆宜的人妖表演，无少儿不宜的内容。门口可以自费和人妖合影。</li><li>天皇秀：[WARNING]不适合少儿或者一家人一起观看的节目，基本就是给国人搞的猎奇节目，非常夸张。</li><li>江西冷购物中心：大型商场，化妆品，服装，百货，餐饮。有优衣库等。</li></ul></li><li>地点-皇帝岛(day7)：<ul><li>浮潜/潜水：皇帝岛附近水质很好，水底很很多鱼群和珊瑚，可以浮潜/潜水感受海底世界。<br><img src="/photo_2024/hdd_fq.jpg" alt=""></li><li>游艇出海：皇帝岛离海岸不远，可以安排顶流游艇出海，体验船上活动。船上一般会提供私人卡位，早餐，午餐，下午茶，饮料，水果，摄影师，无人机航拍，皮划艇，水上推进器，救生衣，浮潜面罩，表演或者组织的活动。</li><li>珊瑚岛：皇帝岛一日游行程一般会经过珊瑚岛和神仙半岛，会安排在珊瑚岛登岛游玩，可在珊瑚岛玩水或者拍照。</li><li>神仙半岛日落：返程会经过神仙半岛，可欣赏海上日落。<br><img src="/photo_2024/hdd_setting_sun.jpg" alt=""></li></ul></li><li>地点-斯米兰(day9)：<ul><li>浮潜/潜水：斯米兰水质非常好，可以轻松看到玻璃海，果冻海，潜水可以看到鱼群，珊瑚和海龟。<br><img src="/photo_2024/similan.jpg" alt=""></li><li>游艇出海: 这边应该都是seastar公司运营的，各个渠道app下单的人都会在seastar码头散客拼团。可以选择中等游艇出海，船越大越不会晕船。<br><img src="/photo_2024/seastar.jpg" alt=""></li><li>打卡风帆石观景点：可以登岛后前往风帆石观景点欣赏海滩全景，景色非常不错。<br><img src="/photo_2024/fengfanshi.jpg" alt=""></li><li>斯米兰登岛：可以在斯米兰岛海边游玩，打卡鸟巢，秋千，海边躺床等。</li></ul></li><li>地点-普集镇(day10)：<ul><li>普吉老街：非常特色的建筑风格，拍结婚照的挺多。白色的宫殿，黄色的钟楼，红色的房子等，拍照很出片。下午的晚霞非常好看，和街道上广告牌上的一模一样。晚上街道很热闹，很多小店门口拍照很好看。<br><img src="/photo_2024/pj_resort.jpg" alt=""></li><li>普吉镇晚霞：普集镇可以欣赏到绝美的海岛小镇晚霞。各色的建筑和天空的晚霞，和油画一样。<br><img src="/photo_2024/普吉镇的晚霞.jpg" alt=""></li><li>普吉当地美食：远离旅游景点，在super cheap附近可以看到很多当地的小餐馆和路边摊，味道非常好，价格也便宜的离谱。<br><img src="/photo_2024/pj_caidan.jpg" alt=""></li><li>青蛙夜市：当地很有特点的集装箱夜市，物价比较贵，适合网红打卡</li><li>周末夜市：很热闹的夜市</li><li>big c 超市: 面积很大，物品很丰富，适合采购物资。</li></ul></li></ul><h2 id="第一次出国海岛游如何选岛？选择巴厘岛还是普吉岛？"><a href="#第一次出国海岛游如何选岛？选择巴厘岛还是普吉岛？" class="headerlink" title="第一次出国海岛游如何选岛？选择巴厘岛还是普吉岛？"></a>第一次出国海岛游如何选岛？选择巴厘岛还是普吉岛？</h2><p>两个海岛比较：</p><table><thead><tr><th>项目</th><th>巴厘岛</th><th>普吉岛         </th></tr></thead><tbody><tr><td>签证</td><td>落地签200元</td><td><b>免签</b>    </td></tr><tr><td>国人</td><td>多</td><td><b>很多</b>         </td></tr><tr><td>可玩性</td><td>很好</td><td>很好         </td></tr><tr><td>玩法</td><td><b>火山-瀑布-海岛-潜水-酒店度假</b></td><td>海岛-潜水-酒店度假</td></tr><tr><td>最好的景点</td><td>佩尼达岛</td><td><b>斯米兰群岛</b>         </td></tr><tr><td>行程宽松</td><td>紧凑</td><td><b>宽松</b>         </td></tr><tr><td>消费</td><td>高</td><td><b>低</b>         </td></tr><tr><td>距离</td><td>近</td><td><b>远</b>         </td></tr><tr><td>航班</td><td>少</td><td><b>多</b></td></tr><tr><td>插头转换器</td><td>需要</td><td><b>不需要</b></td></tr></tbody></table><h2 id="泰国入境需要注意和准备？"><a href="#泰国入境需要注意和准备？" class="headerlink" title="泰国入境需要注意和准备？"></a>泰国入境需要注意和准备？</h2><ul><li>签证：免签。只需要护照即可，不需要其他任何东西。两边的海关都只检查了护照。</li><li>泰铢货币兑换：国际汇率为5左右。可以提前在国内银行预约兑换3000元左右的泰铢，汇率比较好。也可以在酒店旁边街道上的兑换厅兑换。国内机场的携程外币兑换汇率最低，还需要手续费，不建议。普吉岛机场的外币兑换也不建议。汇率划算的顺序：国内银行提前预约兑换&gt;国外街道上的兑换厅&gt;普吉岛机场&gt;国内机场。</li><li>手机卡：AIS网络信号比较好，可以在拼夕夕花35元买10天不限流量+30分钟通话的AIS手机卡。在到达普吉机场后，再换上手机卡，手机开机后会自动激活，短信里会有当前手机的卡号和密码，以及套餐信息。</li><li>必备app：<ul><li>bolt: 打车必备app, 也是远距离打车最便宜的方式，都是一口价订单，汽车起步价40元人民币，可以现金支付。多个人带行李的直接叫bolt就行，人少的可以叫摩托车，价格可以便宜一大半，短距离多个人的可以直接去路边问突突车。</li><li>foodpanda: 外卖必备app，也是最便宜的外卖app。直接下单后选择现金支付，等骑手到了后可以在app内线聊天，见到骑手后直接现金支付即可。</li><li>google地图：必备地图类app，各种酒店评分和评价，餐厅评分，详细路线等都有展示。高德信息较少，在普吉岛非常不好用。</li><li>google翻译+google: 可以一键进行中文和泰语的互译，也可以直接语音互译，还支持拍照菜单进行自动翻译。拍照翻译依赖google 搜索，必须先安装google 搜索app。</li><li>grab: 打车+外卖app，支持支付宝。打车比bolt贵不少。</li><li>booking: 酒店app,可以用来比价，但是我没有下单过，主要用携程和飞猪下单。</li><li>klook: 旅游app，如果想报个外语团或者遇到全是外国人的旅游团，可以在这个上报。我没有在这个上下单过。</li><li>飞猪app: 报一日游需要用到</li><li>携程：订酒店和机票</li><li>航旅纵横：机票在线值机，查看飞机信息。<br><img src="/photo_2024/pj_app.jpg" alt=""></li></ul></li><li>支付宝和微信支付：大部分都不支持。<ul><li>支持的：711便利店，999超市，big c等。</li><li>有限支持（需要最低消费或者手续费）：999超市，部分当地便利店。</li></ul></li><li>信用卡：大部分店铺都支持visa信用卡支付。</li><li>墨镜：建议携带，普吉岛太阳非常刺眼。近视的可以提前在网上买带度数的墨镜。</li><li>充电宝：可携带2w毫安。不能托运。</li><li>插头转换器：不需要，这边的插头和国内一样，直接就可以使用。</li><li>防晒：普吉岛紫外线非常强，浮潜两小时后，我肩膀和小腿被晒伤。晒伤后会非常刺痛，皮肤会发红发黑，然后开始蜕皮。所以必须带防晒霜和防晒喷雾，以及遮阳伞。如果浮潜的话，必须带带浮潜袜，浮潜服，不然皮肤会很快晒伤。</li><li>现金： 网上说是每人要准备10000泰铢或者2000元人民币，不过我通过海关时，并没有人检查。但是我还是准备了2000人民币现金。</li><li>相机：建议携带手机或者gopro，携带微单相机非常不便。</li><li>gopro或者水下相机: 可以携带，浮潜时可拍摄到水底世界。不带也可以，用手机+防水袋也可以拍到，或者让随船的潜水员帮你拍，或者船上花40元租一天gopro, 或者让导游将今天排到的视频发你。</li><li>手机防水袋：建议拼夕夕提前购买，当地超市价格为20-25元人民币。</li><li>浮潜服：浮潜必须带，防止晒伤</li><li>浮潜袜：斯米兰群岛禁止穿鞋上岛，最好带上浮潜袜，不然只能光脚上岛，中午时分岛上石头非常烫脚。</li><li>海岛度假风短袖短裤：建议网上提前买好，当地市场里也有卖，但是比淘宝拼多多贵，需要自己砍价。</li><li>泳裤：酒店游泳需要用到</li><li>泳镜/潜水面罩: 看个人是否需要。有泳镜可以在泳池玩得更舒服;有面罩+呼吸器的话，可以自己去海边浮潜。</li><li>接机：距离比较远，或者晚上很晚到的话可以在携程app预约接机服务，距离远时携程比bolt叫车便宜，而且携程提供中文服务。到达普吉岛机场后按app图示到达指定位置后，会有向导接你，在墙上找到自己英文名字的纸指给他，然后等15分钟，就会有司机来接你去酒店。</li><li>当地物价：一些基本物品的参考（2024年5月）</li></ul><table><thead><tr><th>项目</th><th>当地价格（泰铢）</th><th>折合人民币 （元）</th></tr></thead><tbody><tr><td>水果冰沙</td><td>500</td><td>10  </td></tr><tr><td>混合水果冰沙</td><td>700</td><td>12        </td></tr><tr><td>披萨</td><td>300-400</td><td>60-80         </td></tr><tr><td>KFC套餐（外卖）</td><td>100</td><td>20</td></tr><tr><td>冬阴功汤（餐厅）</td><td>300</td><td>60   </td></tr><tr><td>冬阴功汤（商场）</td><td>120</td><td>24        </td></tr><tr><td>冬阴功汤（小店）</td><td>80</td><td>16         </td></tr><tr><td>牛肉盖浇饭（商场）</td><td>100</td><td>20         </td></tr><tr><td>牛肉盖浇饭（小店）</td><td>80</td><td>16         </td></tr><tr><td>打卤面（路边摊）</td><td>70</td><td>14         </td></tr><tr><td>可乐</td><td>15-20</td><td>3-4         </td></tr><tr><td>辣条/锅巴</td><td>20</td><td>4</td></tr><tr><td>龙虾面</td><td>龙虾称重</td><td>100-300</td></tr><tr><td>衣服</td><td>可3-5折砍价</td><td>-</td></tr><tr><td>手机防水袋</td><td>100-125</td><td>20-25</td></tr></tbody></table><ul><li>航班：吉祥航空上海直飞普吉岛</li><li>返程机票：海关未检查</li><li>酒店入住回执单：海关未检查</li><li>注意：街道上有绿色叶子标识的店铺是大x麻店，注意远离。如果在国外尝试这个，回国海关时会抽检，抽检如果不合格，你们当地会过几个月后再次检查你，如果还不合格，会按咱国家法律处理(拼车时听别人说的，仅供参考)。</li><li>过敏：一周后，我和家人皮肤偶尔会出现瘙痒，网上说是紫外线过敏，但是对于我来说，情况并不严重。如果是短期旅行，应该不是问题。但是如果过敏情况比较严重，还是建议去当地药店或医院进行治疗。</li></ul><h2 id="如何选择普吉岛海滩？"><a href="#如何选择普吉岛海滩？" class="headerlink" title="如何选择普吉岛海滩？"></a>如何选择普吉岛海滩？</h2><p>巴东海滩比较热闹，海滩水上活动多，适合喜欢热闹的人前往；卡塔卡伦比较安静，适合亲子和酒店度假。我的想法是先直接接机送我到离机场最远的海滩，然后逐渐靠近机场，比如：奈汉–&gt;卡塔–&gt;卡伦–&gt;巴东–&gt;普集镇—&gt;机场</p><table><thead><tr><th>项目</th><th>巴东海滩</th><th>卡塔/卡伦海滩  </th></tr></thead><tbody><tr><td>拥挤</td><td>是</td><td>否      </td></tr><tr><td>破旧</td><td>是</td><td>否      </td></tr><tr><td>安静</td><td>否</td><td>是   </td></tr><tr><td>水质</td><td>好</td><td>好      </td></tr><tr><td>热闹程度</td><td>很热闹</td><td>主街道热闹      </td></tr><tr><td>适合活动</td><td>夜市-酒吧-逛街-人妖秀-沙滩活动</td><td>度假-酒店-沙滩躺平-按摩-夜市      </td></tr><tr><td>特色活动</td><td>人妖秀-马林夜市-巴东夜市-沙滩活动-按摩</td><td>卡塔夜市-按摩-酒店泳池躺平-亲子活动-日落     </td></tr></tbody></table><h2 id="皇帝岛如何选船？"><a href="#皇帝岛如何选船？" class="headerlink" title="皇帝岛如何选船？"></a>皇帝岛如何选船？</h2><p>皇帝岛选船可选预算范围内最好的即可。平台的话我选的是飞猪app, 因为之前在国内定过几次跟团游，飞猪上评分和销量比较靠前的团都还不错，导游人比较好，就算有卖东西的地方，不买也不会说啥。而携程5.0分+销量最高的团导游一路上都在PUA旅客，不卖的已经相当于一种诅咒了，而且5.0分评分是导游自己评价的，导游会要走旅客手机自己评价，非常扯淡。不过泰国的跟团游我报的两个都感觉还不错，没有任何隐藏消费或者，和商品详情页面描述竟然完全一致，一丝不差，这在国内真是不敢相信。</p><p>飞猪销量比较好的一日游有一个七加和懒猫，我当时订的比较晚，可选的只有七加的狂想曲号和快艇。就在七加订了狂想曲号。</p><p> <img src="/photo_2024/hdd_hangpai.jpg" alt=""></p><h3 id="狂想曲号899元一人，免费的服务有："><a href="#狂想曲号899元一人，免费的服务有：" class="headerlink" title="狂想曲号899元一人，免费的服务有："></a>狂想曲号899元一人，免费的服务有：</h3><ul><li>酒店接送到码头</li><li>码头提供早餐点心</li><li>两个男模服务员全程服务</li><li>上船就会欢迎并送上饮料</li><li>船上是不拼桌的私人沙发卡位，空间非常宽敞，可以坐或者躺</li><li>提供不限量水果</li><li>提供不限量饮料</li><li>DJ打碟</li><li>自助午餐</li><li>下午茶</li><li>泡泡派对</li><li>船上跳舞</li><li>摄影师不限量拍摄，无人航拍视频</li><li>皮划艇，水上推进器，救生衣，浮潜面罩</li><li>载客只有20人,非常宽敞，中间和船尾活动区域很大</li><li>中文导游</li></ul><h3 id="收费服务："><a href="#收费服务：" class="headerlink" title="收费服务："></a>收费服务：</h3><ul><li>深潜</li><li>吧台点餐</li><li>酒</li></ul><h3 id="小费："><a href="#小费：" class="headerlink" title="小费："></a>小费：</h3><ul><li>每人50泰铢，一家人的话100泰铢。</li></ul><h3 id="行程："><a href="#行程：" class="headerlink" title="行程："></a>行程：</h3><ul><li>酒店接到码头</li><li>吃早点</li><li>上船拍照，吃午餐</li><li>登珊瑚岛</li><li>皇帝岛浮潜</li><li>下午茶</li><li>跳舞</li><li>泡泡派对</li><li>神仙岛日落</li><li>送回酒店</li></ul><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>全程DJ打碟</li><li>摄影师全程跟拍，无人机跟拍，视频拍摄</li><li>全程男服服务员服务</li><li>环境很好，私人卡位，舒适性很好</li><li>水上玩具比较多</li><li>行程不赶</li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>没有登皇帝岛沙滩，只是在皇帝岛附近浮潜</li><li>布丁和下午菜的杯子非常迷你，只有一丝丝，虽然可以继续加。</li></ul><h3 id="选船参考"><a href="#选船参考" class="headerlink" title="选船参考"></a>选船参考</h3><p>  <img src="/photo_2024/hdd_xuanchuan1.jpg" alt=""></p><h2 id="斯米兰群岛如何选船？"><a href="#斯米兰群岛如何选船？" class="headerlink" title="斯米兰群岛如何选船？"></a>斯米兰群岛如何选船？</h2><p>斯米兰比较远，住巴东海滩的话需要早上6点坐车去seastar码头，然后散客拼团前往斯米兰群岛游玩。我选的是七加的699元的蓝鲸号，但是码头上看着都是一样的船，而且一船会拉40-70人，跟公交船差不多，所以我不太确定斯米兰选船是否有必要。看说明更好的船应该只是有两层甲板或者加了一些滑梯之类的水上玩具。</p><h3 id="蓝鲸号的行程安排："><a href="#蓝鲸号的行程安排：" class="headerlink" title="蓝鲸号的行程安排："></a>蓝鲸号的行程安排：</h3><ul><li>6点巴东海滩车接</li><li>8点30 到达码头。前往中文区排队签到，司机会给一张纸条，上面的号码就是签到号，签到后会发一个彩色的手环，同样颜色手环的都是同一条船上的。</li><li>吃自助早餐</li><li>9点坐船前往斯米兰群岛x号岛附近 浮潜<br><img src="/photo_2024/sml_fq.jpg" alt=""></li><li>前往斯米兰群岛4号岛吃午饭。 可在海边玩水 拍照<br><img src="/photo_2024/sml_2.jpg" alt=""></li><li>前往斯米兰群岛x号岛 登岛 打卡观景点 风帆石<br><img src="/photo_2024/sml_ffs.jpg" alt=""></li><li>前往斯米兰群岛x号岛附近 浮潜，这边看到海龟的几率确实很大，我去的当天就有海龟一直在浮潜点附近游荡。<br><img src="/photo_2024/fq.jpg" alt=""></li><li>6点 司机送回酒店</li></ul><h3 id="蓝鲸号提供的免费服务："><a href="#蓝鲸号提供的免费服务：" class="headerlink" title="蓝鲸号提供的免费服务："></a>蓝鲸号提供的免费服务：</h3><ul><li>酒店-码头的接送</li><li>自助早餐</li><li>岛上午饭</li><li>自助晚餐</li><li>船上矿泉水，可乐，水果</li><li>船上下午茶点心</li><li>救生衣，浮潜面罩</li><li>中文导游</li><li>如果当天登岛后不返回，可以第二天接上返回</li></ul><h3 id="小费"><a href="#小费" class="headerlink" title="小费"></a>小费</h3><ul><li>50泰铢每人</li></ul><h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul><li>seastar运营，服务比较一致</li><li>速度快，稳</li></ul><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul><li>船和公交船一样，拉的人比较多，座位排列和高铁一样，ABC DF一排五个。一船拉40-70人</li><li>在船上没法自由活动，只有船尾有一点点空间可以活动，但是船开的非常快，去船尾不安全。</li></ul><h2 id="如何订外卖？"><a href="#如何订外卖？" class="headerlink" title="如何订外卖？"></a>如何订外卖？</h2><p> 直接foodpanda下单后选择现金支付，等骑手到了后可以在app内线聊天，见到骑手后直接现金支付即可。</p><h2 id="如何选择酒店？"><a href="#如何选择酒店？" class="headerlink" title="如何选择酒店？"></a>如何选择酒店？</h2><ul><li>酒店度假：选直通泳池房或者泳池别墅，适合整日待在酒店度假的人。开门就是泳池和躺椅，非常方便。价位：最高</li><li>海边躺平：有私人沙滩的酒店，离海滩最近，最方便，提供免费沙滩躺椅和玩水工具。价位：最高</li><li>打卡拍照/泳池落日：选带屋顶无边泳池的酒店。屋顶无边泳池拍照很出片，可以在泳池直接看落日。价位：高</li><li>休闲度假：海景房，适合喜欢在屋里或者躺床上看海的人。价位: 一般</li><li>出海：普通酒店即可，适合白天出海一整天，晚上才回来的情况。价位：低</li></ul><h2 id="离开时的购物？"><a href="#离开时的购物？" class="headerlink" title="离开时的购物？"></a>离开时的购物？</h2><ul><li>大部分人推荐一些药品，感觉不是很需要</li><li>大部分东西国内都有，没有选购的欲望</li><li>一些零食，如：肉松锅巴，榴莲干，芒果干，鱿鱼条，小老板海苔等味道不错。</li><li>可以在big c, super cheap, 711等超市， 一次性买齐。</li></ul><h2 id="退税？"><a href="#退税？" class="headerlink" title="退税？"></a>退税？</h2><ul><li>在泰国购买的东西，当地居民需要交当地的税，但是外国旅客可以申请退税。</li><li>一天内在同一店铺消费超过2000泰铢就可以申请退税</li><li>在店铺用小票申请，然后在机场办理退税</li><li>免税店没便宜多少，几乎没啥人。国内电商平台618期间或者拼多多百亿补贴反而更优惠。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;人生3万多天，到底该怎么样度过一生？我觉得人生应该是多彩的，而不是一种颜色，总要有一段时间在经历人生低谷，有一段时间在努力实现自己的想法。但也需要一段时间去忘掉一切，去体验不一样的人生。换一种环境，换一种生活态度，体验不一样的异域生活。躺在海边看落日晚霞，乘着船感受海风轻抚，感受浪花拍岸；去远离大陆的海岛边潜入海底，和五彩斑斓的鱼群一起，只活在当下，感受当下。&lt;/p&gt;
&lt;h2 id=&quot;速读主要行程安排&quot;&gt;&lt;a href=&quot;#速读主要行程安排&quot; class=&quot;headerlink&quot; title=&quot;速读主要行程安排&quot;&gt;&lt;/a&gt;速读主要行程安排&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;卡塔泳池别墅酒店躺平5日。白天泳池游泳，体验泰式马杀鸡，海边沙滩躺平，酒店躺平，泳池边沙滩椅休息，感受当地美食；傍晚去感受卡塔日落晚霞，逛卡塔夜市，主街道逛街。&lt;/li&gt;
&lt;li&gt;巴东海滩4日：白天：一日逛街；一日出海皇帝岛，珊瑚岛，神仙半岛；一日出海斯米兰群岛；晚上：酒店无边泳池落日，逛巴东马林夜市，感受当地美食&lt;/li&gt;
&lt;li&gt;普集镇1日：普吉老街打卡拍照，远离景区感受当地美食和风情，扫货当地特产或美食带走&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;读完本篇将知道（目录）：&quot;&gt;&lt;a href=&quot;#读完本篇将知道（目录）：&quot; class=&quot;headerlink&quot; title=&quot;读完本篇将知道（目录）：&quot;&gt;&lt;/a&gt;读完本篇将知道（目录）：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;主要行程安排&lt;/li&gt;
&lt;li&gt;主要需要考虑的？&lt;/li&gt;
&lt;li&gt;10日行程安排都有哪些体验？&lt;/li&gt;
&lt;li&gt;第一次出国海岛游如何选岛？选择巴厘岛还是普吉岛？&lt;/li&gt;
&lt;li&gt;泰国入境需要注意？&lt;/li&gt;
&lt;li&gt;如何选择普吉岛海滩？&lt;/li&gt;
&lt;li&gt;斯米兰群岛如何选船？&lt;/li&gt;
&lt;li&gt;皇帝岛如何选船？&lt;/li&gt;
&lt;li&gt;如何订外卖？&lt;/li&gt;
&lt;li&gt;如何选择酒店？&lt;/li&gt;
&lt;li&gt;离开时的购物？&lt;/li&gt;
&lt;li&gt;退税？&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="游记" scheme="http://www.javabin.cn/tags/%E6%B8%B8%E8%AE%B0/"/>
    
      <category term="生活" scheme="http://www.javabin.cn/tags/life/"/>
    
      <category term="泰国" scheme="http://www.javabin.cn/tags/%E6%B3%B0%E5%9B%BD/"/>
    
      <category term="普吉岛" scheme="http://www.javabin.cn/tags/%E6%99%AE%E5%90%89%E5%B2%9B/"/>
    
      <category term="海岛游" scheme="http://www.javabin.cn/tags/%E6%B5%B7%E5%B2%9B%E6%B8%B8/"/>
    
      <category term="斯米兰" scheme="http://www.javabin.cn/tags/%E6%96%AF%E7%B1%B3%E5%85%B0/"/>
    
  </entry>
  
  <entry>
    <title>【AirTag】为低功耗蓝牙设备添加AirTag功能</title>
    <link href="http://www.javabin.cn/2024/airtag.html"/>
    <id>http://www.javabin.cn/2024/airtag.html</id>
    <published>2024-03-07T14:12:23.000Z</published>
    <updated>2024-03-11T17:10:40.233Z</updated>
    
    <content type="html"><![CDATA[<p>最近想买个自行车，在网上看了些攻略后发现自行车有个缺点就是容易丢，必须人不离车，在我看来这样反而非常麻烦，完全没有了骑行的乐趣。</p><p>随后又在b站上看到有给车装定位装置的，但是一般的车用定位器有两个缺点：1. 非常大，非常明显，很容易被发现然后被拆掉；2.非常耗电，自身待机只有一周，一个每周都要充电的智能设备在我看来是非常不智能的。</p><p>最终发现只有AirTag非常适合，体积小，只有硬币大小，可以直接塞到不起眼的地方；再一个功耗很低，可以待机1年多。但是AirTag只支持ios系统，且必须是iphone12以上才行，如果用为了定位还得再带一部手机，那更难受了。</p><p>虽然麻烦，但还是买了两个AirTag打算测试一下，在等AirTag送货的期间，查了下资料发现完全可以自己改造一个AirTag， 并且在安卓手机上获取位置信息。</p><p>所以这篇blog主要记录如何为低功耗蓝牙设备添加AirTag功能，且从安卓手机端获取定位记录。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>目录</li><li>需要准备<ul><li>硬件</li><li>软件</li></ul></li><li>步骤<ul><li>部署服务端项目</li><li>安装手机端</li><li>生成advertisement key</li><li>修改固件</li><li>编译</li></ul></li><li>FindMy网络<ul><li>原理</li><li>蓝牙协议</li></ul></li></ul><a id="more"></a><h2 id="需要准备"><a href="#需要准备" class="headerlink" title="需要准备"></a>需要准备</h2><h3 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h3><p>硬件由于我手头只有TLSR8359芯片的墨水屏电子价签，所以只需要TLSR8359芯片的墨水屏电子价签即可。墨水屏芯片本身耗电特别低，在显示静态图片的情况下，可以待机一到两年。</p><h3 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h3><ul><li>墨水屏固件项目 <a href="https://github.com/reece15/stellar-L3N-etag" target="_blank" rel="noopener">https://github.com/reece15/stellar-L3N-etag</a>  (该项目对固件代码进行了修改，已经添加了对AirTag协议的支持，其他低功耗蓝牙设备固件也可进行类似的操作)</li><li>服务端/手机端项目 <a href="https://github.com/dchristl/macless-haystack" target="_blank" rel="noopener">https://github.com/dchristl/macless-haystack</a> </li></ul><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="部署服务端项目"><a href="#部署服务端项目" class="headerlink" title="部署服务端项目"></a>部署服务端项目</h3><p>参考该项目 <a href="https://github.com/dchristl/macless-haystack" target="_blank" rel="noopener">https://github.com/dchristl/macless-haystack</a> ， 使用docker将anisette-v3，macless-haystack部署到服务器。（在这之前你需要在 <a href="https://appleid.apple.com/" target="_blank" rel="noopener">https://appleid.apple.com/</a> 上注册一个新的apple id, 注意：不建议使用你自己正在使用的apple id）</p><h3 id="安装手机端"><a href="#安装手机端" class="headerlink" title="安装手机端"></a>安装手机端</h3><ul><li>在 <a href="https://github.com/dchristl/macless-haystack/releases" target="_blank" rel="noopener">https://github.com/dchristl/macless-haystack/releases</a> 下载适合自己手机的apk安装到手机</li><li>在 app的设置中配置自己的服务端地址，一般为 <a href="http://{你的服务器地址}:6176" target="_blank" rel="noopener">http://{你的服务器地址}:6176</a></li></ul><h3 id="生成advertisement-key"><a href="#生成advertisement-key" class="headerlink" title="生成advertisement key"></a>生成advertisement key</h3><ul><li>在app中创建accessories，导出advertisement key(base64)</li><li>在安装有python的linux/windows服务器上执行 <code>python3 -c &#39;import base64; print(&quot;,&quot;.join(hex(i) for i in base64.b64decode(&quot;你导出的advertisement key&quot;)))&#39;</code>获取到输出的advertisement key数组</li></ul><h3 id="修改固件"><a href="#修改固件" class="headerlink" title="修改固件"></a>修改固件</h3><ul><li>将 <a href="https://github.com/reece15/stellar-L3N-etag" target="_blank" rel="noopener">https://github.com/reece15/stellar-L3N-etag</a> 项目同步到本地</li><li>使用python脚本输出的advertisement key数组替换掉 stellar-L3N-etag 项目Firmware/src/ble.c 文件中 PUB_KEY数组内的数据</li><li>将stellar-L3N-etag 项目 Firmware/src/ble.c 中 AIR_TAG_OPEN 的值改为 1</li></ul><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><ul><li>进入 stellar-L3N-etag 项目Firmware目录，执行<code>makeit.exe clean &amp;&amp; makeit.exe -j12</code></li><li>将生成的固件刷入设备即可（具体可参考stellar-L3N-etag readme）</li></ul><h1 id="FindMy网络"><a href="#FindMy网络" class="headerlink" title="FindMy网络"></a>FindMy网络</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ul><li>苹果手机默认会打开蓝牙，作为FindMy网络中的无数个客户端</li><li>AirTag设备默认会广播自己内置的advertisement key， 当附近的苹果设备接受到这个广播信息时，就会使用收到的advertisement key（公钥）加密自己的位置信息，然后上传到apple服务器。详细代码可参考: <a href="https://github.com/reece15/stellar-L3N-etag/blob/e04181b22c15b4094523f18e8099bea0c33368d3/Firmware/src/ble.c#L218" target="_blank" rel="noopener">https://github.com/reece15/stellar-L3N-etag/blob/e04181b22c15b4094523f18e8099bea0c33368d3/Firmware/src/ble.c#L218</a>  以及 <a href="https://github.com/reece15/stellar-L3N-etag/blob/e04181b22c15b4094523f18e8099bea0c33368d3/Firmware/src/ble.c#L128" target="_blank" rel="noopener">https://github.com/reece15/stellar-L3N-etag/blob/e04181b22c15b4094523f18e8099bea0c33368d3/Firmware/src/ble.c#L128</a></li><li>advertisement key的生成者可使用<code>hash(advertisement key)</code>从苹果服务器查询加密的位置信息列表，然后使用私钥解密出真实的位置信息。详细代码可参考: <a href="https://github.com/biemster/FindMy/blob/113ebf4017729b92a381624c1932065588c3ebde/request_reports.py#L71" target="_blank" rel="noopener">https://github.com/biemster/FindMy/blob/113ebf4017729b92a381624c1932065588c3ebde/request_reports.py#L71</a><br><img src="/photo_2024/findmy_network.png" alt=""></li></ul><h2 id="蓝牙协议"><a href="#蓝牙协议" class="headerlink" title="蓝牙协议"></a>蓝牙协议</h2><ul><li>低功耗蓝牙设备需要将自己的MAC地址设置为advertisement key的一部分，且通过发送长度为30的广播包来传输advertisement key的另一部分</li><li>具体协议说明以PUB_KEY=0x49,0x88,0x0,0x7a,0x27,0xac,0x38,0xb7,0x16,0x55,0x3c,0xc8,0x57,0x62,0x93,0xc3,0x95,0xef,0x3f,0x63,0x70,0xb2,0xa3,0x96,0x6d,0x4c,0x1a,0x7d 举例：</li><li>蓝牙设备MAC 地址</li></ul><table><thead><tr><th>index</th><th>值 (举例)</th><th>备注</th></tr></thead><tbody><tr><td>0</td><td>0xc9</td><td>advertisement key的0位的低6位</td></tr><tr><td>1</td><td>0x88007A27AC</td><td>advertisement key的1-5位</td></tr></tbody></table><ul><li>广播协议</li></ul><table><thead><tr><th>index</th><th>值  (举例)</th><th>备注</th></tr></thead><tbody><tr><td>0</td><td>0x1e</td><td>总长度  （固定）</td></tr><tr><td>1</td><td>0xff</td><td>厂商自定义格式</td></tr><tr><td>2-3</td><td>0x4c00</td><td>苹果公司ID</td></tr><tr><td>4</td><td>0x12</td><td>设备类型</td></tr><tr><td>5</td><td>0x19</td><td>数据长度</td></tr><tr><td>6</td><td>0x00</td><td>状态信息</td></tr><tr><td>7-29</td><td>0x38B716553CC8576293C3953F3F6370B2A3966D4C1A</td><td>advertisement key的6-28位</td></tr><tr><td>30</td><td>0x01</td><td>advertisement key的0位的高2位</td></tr><tr><td>31</td><td>0x00</td><td>计数</td></tr></tbody></table><h2 id="资料、参考"><a href="#资料、参考" class="headerlink" title="资料、参考"></a>资料、参考</h2><blockquote><p>github项目（支持安卓端查询定位数据） <a href="https://github.com/dchristl/macless-haystack" target="_blank" rel="noopener">https://github.com/dchristl/macless-haystack</a><br>github项目（支持MAC端查询定位数据） <a href="https://github.com/malmeloo/openhaystack" target="_blank" rel="noopener">https://github.com/malmeloo/openhaystack</a><br>AirTag协议 <a href="https://support.apple.com/en-gb/guide/security/sece994d0126/web" target="_blank" rel="noopener">https://support.apple.com/en-gb/guide/security/sece994d0126/web</a><br>原理 <a href="https://arxiv.org/pdf/2103.02282.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2103.02282.pdf</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近想买个自行车，在网上看了些攻略后发现自行车有个缺点就是容易丢，必须人不离车，在我看来这样反而非常麻烦，完全没有了骑行的乐趣。&lt;/p&gt;
&lt;p&gt;随后又在b站上看到有给车装定位装置的，但是一般的车用定位器有两个缺点：1. 非常大，非常明显，很容易被发现然后被拆掉；2.非常耗电，自身待机只有一周，一个每周都要充电的智能设备在我看来是非常不智能的。&lt;/p&gt;
&lt;p&gt;最终发现只有AirTag非常适合，体积小，只有硬币大小，可以直接塞到不起眼的地方；再一个功耗很低，可以待机1年多。但是AirTag只支持ios系统，且必须是iphone12以上才行，如果用为了定位还得再带一部手机，那更难受了。&lt;/p&gt;
&lt;p&gt;虽然麻烦，但还是买了两个AirTag打算测试一下，在等AirTag送货的期间，查了下资料发现完全可以自己改造一个AirTag， 并且在安卓手机上获取位置信息。&lt;/p&gt;
&lt;p&gt;所以这篇blog主要记录如何为低功耗蓝牙设备添加AirTag功能，且从安卓手机端获取定位记录。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;目录&lt;/li&gt;
&lt;li&gt;需要准备&lt;ul&gt;
&lt;li&gt;硬件&lt;/li&gt;
&lt;li&gt;软件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;步骤&lt;ul&gt;
&lt;li&gt;部署服务端项目&lt;/li&gt;
&lt;li&gt;安装手机端&lt;/li&gt;
&lt;li&gt;生成advertisement key&lt;/li&gt;
&lt;li&gt;修改固件&lt;/li&gt;
&lt;li&gt;编译&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FindMy网络&lt;ul&gt;
&lt;li&gt;原理&lt;/li&gt;
&lt;li&gt;蓝牙协议&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="固件" scheme="http://www.javabin.cn/tags/%E5%9B%BA%E4%BB%B6/"/>
    
      <category term="AirTag" scheme="http://www.javabin.cn/tags/AirTag/"/>
    
      <category term="墨水屏" scheme="http://www.javabin.cn/tags/%E5%A2%A8%E6%B0%B4%E5%B1%8F/"/>
    
  </entry>
  
  <entry>
    <title>postgresql迁移到lightdb[安装lightdb+postgis环境以及兼容性测试]</title>
    <link href="http://www.javabin.cn/2023/lightdb_postgis.html"/>
    <id>http://www.javabin.cn/2023/lightdb_postgis.html</id>
    <published>2023-09-21T12:47:24.000Z</published>
    <updated>2024-03-11T14:49:03.605Z</updated>
    
    <content type="html"><![CDATA[<p>打算将<code>postgresql</code>数据库迁移到国产数据库<code>lightdb</code>，测试后发现官方提供的<code>lightdb-X</code>最新版存在一些问题，而<code>23.1</code>版本只需要做很小的调整就可以将基于postgresql的后端代码迁移到lightdb。以下内容使用版本为 <code>lightdb-X</code>的<code>23.1</code>版本，<code>centos7 x64</code>系统。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>目录</li><li>postgresql迁移到lightdb-x-23.1版本需要调整</li><li>安装lightdb</li><li>编译安装postgis</li><li>遇到的问题</li></ul><a id="more"></a><h2 id="postgresql迁移到lightdb-x-23-1版本需要调整"><a href="#postgresql迁移到lightdb-x-23-1版本需要调整" class="headerlink" title="postgresql迁移到lightdb-x-23.1版本需要调整"></a>postgresql迁移到lightdb-x-23.1版本需要调整</h2><ul><li>由于插件<code>pg_trgm</code>名称修改为<code>lt_trgm</code>, 所以需要将<code>pg_trgm</code>插件创建相关代码需要调整为<code>lt_trgm</code></li><li>由于<code>sqlalchemy</code> 在pg方言实现中，校验了<code>pg server</code>版本名称，而ligthdb版本号不符合格式要求，需要修改下sqlalchemy源码来跳过检查</li><li>需要编译安装postgis插件</li><li>官方未提供docker部署方式</li><li>如果切换到lightdb-x最新版，还有一些兼容性问题需要解决</li></ul><h2 id="安装lightdb"><a href="#安装lightdb" class="headerlink" title="安装lightdb"></a>安装lightdb</h2><h3 id="安装基础工具和官方文档未提及的库"><a href="#安装基础工具和官方文档未提及的库" class="headerlink" title="安装基础工具和官方文档未提及的库"></a>安装基础工具和官方文档未提及的库</h3><pre><code class="shell">    yum install unzip wget bzip2 epel-release.noarch -y    yum groupinstall &quot;Development Tools&quot;    yum install curl-devel libzstd.x86_64 libxml2-devel.x86_64 -y</code></pre><h3 id="安装ligthdb依赖-（同官方文档一致）"><a href="#安装ligthdb依赖-（同官方文档一致）" class="headerlink" title="安装ligthdb依赖 （同官方文档一致）"></a>安装ligthdb依赖 （同<a href="http://www.light-pg.com/docs/LightDB_Install_Manual/current/index.html" target="_blank" rel="noopener">官方文档</a>一致）</h3><pre><code class="shell">    # 1.下载数据库安装包    cd /opt/    mkdir lightdb    cd lightdb/    wget https://www.hs.net/lightdb/download/lightdb-x-13.8-23.1-10555-el7.x86_64.zip    unzip lightdb-x-13.8-23.1-10555-el7.x86_64.zip    cd lightdb-x-13.8-23.1-10555-el7.x86_64    # 2.设置防火墙    iptables -A INPUT -p tcp --dport 5432 -j ACCEPT    iptables -A INPUT -p udp --dport 123 -j ACCEPT    firewall-cmd --permanent --add-port=123/udp    firewall-cmd --permanent --add-port=5432/tcp    # 2.或者关闭防火墙    systemctl stop firewalld.service    systemctl disable firewalld.service    systemctl stop NetworkManager.service    systemctl disable NetworkManager.service    # 3.关闭selinux    sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config    setenforce 0    # 4.设置时区    timedatectl set-timezone Asia/Shanghai    # 5.安装依赖    yum install -y procps-ng    yum install -y coreutils    # 数据库的运行依赖    yum install -y readline    yum install -y zlib    yum install -y libxml2    yum install -y openssl-libs    yum install -y uuid    yum install -y c-ares libpcap snappy # tshark    yum install -y ncurses-libs # iftop    yum install -y libnl3 # keepalived ipv6    yum install -y libzstd # canopy    yum install -y sysstat    yum install -y json-c    yum install -y libicu    yum install -y bc    # 6.添加lightdb用户    groupadd lightdb    useradd -g lightdb -m lightdb    passwd lightdb    mkdir -p /usr/local/lightdb    chown -R lightdb:lightdb /usr/local/lightdb    mkdir -p /data/lightdb_data    chown -R lightdb:lightdb /data/lightdb_data    chown -R lightdb:lightdb /opt/lightdb    tee -a /etc/sudoers &lt;&lt;&lt; &quot;lightdb ALL=(ALL)       NOPASSWD:ALL&quot;    # 7.设置内核参数    echo &quot;kernel.shmmni=4096&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;kernel.shmmax=$(expr $(getconf _PHYS_PAGES) / 2 \* $(getconf PAGE_SIZE))&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;kernel.shmall=$(expr $(getconf _PHYS_PAGES) / 2)&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;kernel.sem=500 2048000 200 4096&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;fs.aio-max-nr=1048576&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;fs.file-max=524288&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;vm.swappiness=5&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;vm.overcommit_memory=2&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;vm.overcommit_ratio=75&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;vm.dirty_background_ratio=5&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;vm.dirty_ratio=40&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;vm.dirty_expire_centisecs=500&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;vm.dirty_writeback_centisecs=250&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;net.core.somaxconn=2000&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;net.ipv4.tcp_max_syn_backlog=2000&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;net.ipv4.tcp_tw_reuse=1&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;net.ipv4.tcp_syn_retries=3&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;net.ipv4.tcp_retries2=5&quot; &gt;&gt; /etc/sysctl.conf    echo &quot;net.ipv4.tcp_slow_start_after_idle=0&quot; &gt;&gt; /etc/sysctl.conf    sysctl -p    echo &quot;lightdb hard core   unlimited&quot; &gt;&gt; /etc/security/limits.conf    echo &quot;lightdb soft core   unlimited&quot; &gt;&gt; /etc/security/limits.conf    echo &quot;lightdb hard nofile 524288&quot;    &gt;&gt; /etc/security/limits.conf    echo &quot;lightdb soft nofile 524288&quot;    &gt;&gt; /etc/security/limits.conf    echo &quot;lightdb hard nproc  16384&quot;     &gt;&gt; /etc/security/limits.conf    echo &quot;lightdb soft nproc  16384&quot;     &gt;&gt; /etc/security/limits.conf    # 然后su - lightdb切换到lightdb用户使设置生效，运行ulimit -c、ulimit -n和ulimit -u命令确认设置生效。    # 8.设置Swap交换区大小    dd if=/dev/zero of=/swap bs=1M count=2048    mkswap -f /swap    chmod 0600 /swap    swapon /swap    tee -a /etc/fstab &lt;&lt;&lt; &quot;/swap swap swap defaults 0 0&quot;</code></pre><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>执行<code>./install.sh</code>后按<a href="http://www.light-pg.com/docs/LightDB_Install_Manual/current/install.html#id13" target="_blank" rel="noopener">官方文档</a>给出的说明，进行选择。</p><h3 id="启动一个数据库实例"><a href="#启动一个数据库实例" class="headerlink" title="启动一个数据库实例"></a>启动一个数据库实例</h3><pre><code class="shell">    su - ligthdb    nohup /usr/local/lightdb/lightdb-x/13.8-23.1/bin/lightdb -D /data/lightdb_data &amp;</code></pre><h3 id="检查数据库是否安装正常"><a href="#检查数据库是否安装正常" class="headerlink" title="检查数据库是否安装正常"></a>检查数据库是否安装正常</h3><p>链接数据库并查看数据库版本号。</p><pre><code class="shell">    ltsql -p 5432 -hlocalhost -c &#39;select version();&#39;</code></pre><h2 id="编译安装postgis"><a href="#编译安装postgis" class="headerlink" title="编译安装postgis"></a>编译安装postgis</h2><p>postgis需要手动编译安装postgis以及依赖的geos、sqlite3、proj。版本：<code>geos-3.10.2, sqlite3-3.43.1, proj-8.2.1, postgis-3.4.0, lighdb-x-23.1</code></p><h3 id="安装依赖geos"><a href="#安装依赖geos" class="headerlink" title="安装依赖geos"></a>安装依赖geos</h3><pre><code class="shell">    cd /opt/lightdb/    wget http://download.osgeo.org/geos/geos-3.10.2.tar.bz2    tar -xjvf geos-3.10.2.tar.bz2    cd geos-3.10.2    cmake3 .    make    make install    # 检查是否正常安装    geos-config --version</code></pre><h3 id="安装依赖sqlite3"><a href="#安装依赖sqlite3" class="headerlink" title="安装依赖sqlite3"></a>安装依赖sqlite3</h3><pre><code class="shell">    curl -k -O https://www.sqlite.org/2023/sqlite-autoconf-3430100.tar.gz    tar xzf sqlite-autoconf-3430100.tar.gz    cd sqlite-autoconf-3430100    ./configure    make    make install    # 检查是否正常安装    sqlite3 --version    # 设置pkgconfig路径（编译proj和postgis时会用到）    export PKG_CONFIG_PATH=/usr/lib64/pkgconfig:/usr/share/pkgconfig:/usr/local/lib/pkgconfig:/usr/local/lib64/pkgconfig:/usr/local/lightdb/lightdb-x/13.8-23.1/lib/pkgconfig:/usr/local/lightdb/lightdb-x/13.8-23.1/tools/lib64/pkgconfig:/opt/lightdb/lightdb-x-13.8-23.1-10555-el7.x86_64/lightdb-x/13.8-23.1/lib/pkgconfig:/opt/lightdb/lightdb-x-13.8-23.1-10555-el7.x86_64/lightdb-x/13.8-23.1/tools/lib64/pkgconfig</code></pre><h3 id="安装依赖proj"><a href="#安装依赖proj" class="headerlink" title="安装依赖proj"></a>安装依赖proj</h3><pre><code class="shell">    yum install libtiff-devel.x86_64    wget https://download.osgeo.org/proj/proj-8.2.1.tar.gz    tar zxf proj-8.2.1.tar.gz    cd proj-8.2.1    ./configure    make    make install    # 检查是否正常安装    proj</code></pre><h3 id="安装postgis"><a href="#安装postgis" class="headerlink" title="安装postgis"></a>安装postgis</h3><pre><code class="shell">    wget https://download.osgeo.org/postgis/source/postgis-3.4.0.tar.gz    tar xzf postgis-3.4.0.tar.gz    cd postgis-3.4.0    # 配置并指定lt_config位置    ./configure --without-json --without-protobuf --without-raster --without-topology --with-pgconfig=/usr/local/lightdb/lightdb-x/13.8-23.1/bin/lt_config    make -j8    make install    # 设置静态库路径    su - lightdb    tee -a ~/.bashrc &lt;&lt;&lt; &quot;export LD_LIBRARY_PATH=/usr/local/lib64:/usr/lib64:/usr/local/lib/:/usr/lib/:/usr/local/lightdb/lightdb-x/13.8-23.1/lib/3rd:/usr/local/lightdb/lightdb-x/13.8-23.1/lib:/usr/local/lightdb/lightdb-x/13.8-23.1/lib/ltext:/usr/local/lightdb/lightdb-x/13.8-23.1/tools/lib64:$LD_LIBRARY_PATH&quot;</code></pre><h3 id="检查postgis是否正常安装"><a href="#检查postgis是否正常安装" class="headerlink" title="检查postgis是否正常安装"></a>检查postgis是否正常安装</h3><pre><code class="shell">    ltsql -p 5432 -hlocalhost    CREATE EXTENSION postgis;    SELECT postgis_full_version();</code></pre><h2 id="迁移遇到的问题"><a href="#迁移遇到的问题" class="headerlink" title="迁移遇到的问题"></a>迁移遇到的问题</h2><ul><li>Q: 从<code>postgresql</code>迁移到<code>lightdb</code>,应该使用哪个版本?<ul><li>A: 在<code>centos7 x64</code>系统下测试了最新版的<code>lightdb-X</code>， 最新版<code>lightdb-A</code>以及<code>23.1</code>版本<code>lightdb-x</code>数据库，目前看<code>23.1</code>版本的<code>ligthdb-x</code>数据库可以在代码改动非常小的情况下迁移到ligthdb。</li></ul></li><li>Q: 安装<code>proj</code>时报错<code>sqlite3</code>找不到,但是<code>sqlite3</code>已经正常安装<ul><li>A: 未正确设置<code>PKG_CONFIG_PATH</code>, 解决：执行<code>find / -name &quot;pkgconfig&quot; -print</code>查找相关<code>pkgconfig</code>路径，设置到环境变量<code>PKG_CONFIG_PATH</code>, </li></ul></li><li>Q: <code>postgis</code>安装成功后，查询时报错<code>libgeos, libpq.so.5</code>等so文件不存在<ul><li>A: 执行 <code>find /usr/ -name *libgeos*</code>和<code>find /usr/ -name *ibpq.so*</code>将相关路径加入环境变量<code>LD_LIBRARY_PATH</code>，然后重启数据库</li></ul></li><li>Q: <code>sqlalchemy</code>库报错。<ul><li>A: <code>sqlalchemy</code> 在pg方言实现中，校验了<code>pg server</code>版本名称，而版本号<code>LightDB 13.8-23.1 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44), 64-bit</code>不符合正则表达式要求。 解决：需要<code>patch</code>下<code>sqlalchemy</code>源码，跳过检查即可。</li></ul></li><li>Q: <code>pg_trgm</code>插件报错。<ul><li>A: 名称变更为<code>lt_trgm</code>。解决：调整名称和相关的代码。</li></ul></li><li>Q:<code>level</code>相关字段似乎和内置字段名称冲突（lightdb-X最新版）<ul><li>A：在使用lightdb最新版(<code>LightDB1.0-x-V202302-00-000-el7-x86_64</code>)时，以前库里的<code>level</code>相关字段无法正常工作，似乎会和内置字段名称冲突。解决：需要修改migration语句，为相关字段添加反引号</li></ul></li><li>Q: <code>btree_gist</code>插件未提供。 (<code>lightdb-A</code>最新版) <ul><li>A: 需提供<code>btree_gist</code>插件</li></ul></li><li>Q:<code>log</code>相关字段似乎和内置字段名称冲突（<code>lightdb-A</code>最新版）<ul><li>A：在使用<code>lightdb</code>最新版(<code>lightdb</code>提供的<code>lightdb-A</code>类型数据库)时，以前库里的<code>level</code>相关字段无法正常工作，似乎会和内置字段名称冲突。解决：需要修改migration语句，为相关字段添加反引号</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;打算将&lt;code&gt;postgresql&lt;/code&gt;数据库迁移到国产数据库&lt;code&gt;lightdb&lt;/code&gt;，测试后发现官方提供的&lt;code&gt;lightdb-X&lt;/code&gt;最新版存在一些问题，而&lt;code&gt;23.1&lt;/code&gt;版本只需要做很小的调整就可以将基于postgresql的后端代码迁移到lightdb。以下内容使用版本为 &lt;code&gt;lightdb-X&lt;/code&gt;的&lt;code&gt;23.1&lt;/code&gt;版本，&lt;code&gt;centos7 x64&lt;/code&gt;系统。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;目录&lt;/li&gt;
&lt;li&gt;postgresql迁移到lightdb-x-23.1版本需要调整&lt;/li&gt;
&lt;li&gt;安装lightdb&lt;/li&gt;
&lt;li&gt;编译安装postgis&lt;/li&gt;
&lt;li&gt;遇到的问题&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="运维" scheme="http://www.javabin.cn/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="postgresql" scheme="http://www.javabin.cn/tags/postgresql/"/>
    
      <category term="信创" scheme="http://www.javabin.cn/tags/%E4%BF%A1%E5%88%9B/"/>
    
      <category term="lightdb" scheme="http://www.javabin.cn/tags/lightdb/"/>
    
      <category term="数据库迁移" scheme="http://www.javabin.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>Milvus 数据迁移和备份</title>
    <link href="http://www.javabin.cn/2023/milvus_mirror.html"/>
    <id>http://www.javabin.cn/2023/milvus_mirror.html</id>
    <published>2023-09-18T11:47:24.000Z</published>
    <updated>2024-03-11T14:49:03.606Z</updated>
    
    <content type="html"><![CDATA[<p>将本地milvus中的数据迁移到远程milvus服务中的操作流程。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>目录</li><li>主要操作步骤</li><li>详细操作步骤<ul><li>安装/配置milvus-backup</li><li>备份数据</li><li>安装/配置rclone/mc</li><li>迁移数据</li><li>恢复数据</li><li>修复索引</li></ul></li></ul><a id="more"></a><h2 id="主要操作步骤"><a href="#主要操作步骤" class="headerlink" title="主要操作步骤"></a>主要操作步骤</h2><p>通过尝试提供的milvus-dm等数据迁移工具，发现只有milvus-backup可以在最新版server下正常运行而不报错。</p><p>迁移数据的主要操作流程为：</p><ul><li>1.使用milvus-backup create 命令，备份数据到本地minio</li><li>2.使用rclone sync/mc mirror迁移数据到远程minio</li><li>3.使用milvus-backup restore命令，恢复数据</li><li>4.重新创建索引</li></ul><h2 id="详细操作步骤"><a href="#详细操作步骤" class="headerlink" title="详细操作步骤"></a>详细操作步骤</h2><h3 id="安装-配置milvus-backup"><a href="#安装-配置milvus-backup" class="headerlink" title="安装/配置milvus-backup"></a>安装/配置milvus-backup</h3><ol><li>安装编译milvus-backup</li></ol><pre><code class="shell">  git clone git@github.com:zilliztech/milvus-backup.git  cd milvus-backup  go get  go build</code></pre><ol><li>配置milvus-backup</li></ol><p>这里需要配置待备份的milvus和关联minio信息，以及backup数据存储位置。</p><pre><code class="yml"># Configures the system log output.log:  level: info # Only supports debug, info, warn, error, panic, or fatal. Default &#39;info&#39;.  console: true  file:    rootPath: &quot;logs/backup.log&quot;http:  simpleResponse: true# milvus proxy address, compatible to milvus.yamlmilvus:  address: localhost  # 配置milvus地址   port: 19530    # 配置milvus端口   authorizationEnabled: false  # tls mode values [0, 1, 2]  # 0 is close, 1 is one-way authentication, 2 is two-way authentication.  tlsMode: 0  user: &quot;root&quot;  password: &quot;Milvus&quot;# Related configuration of minio, which is responsible for data persistence for Milvus.minio:  address: localhost # Address of MinIO/S3  # 配置minio地址  port: 9000   # Port of MinIO/S3   # 配置minio端口   accessKeyID: minioadmin # accessKeyID of MinIO/S3     # 配置minio keyid  secretAccessKey: minioadmin # MinIO/S3 encryption string   # 配置minio  key   useSSL: false # Access to MinIO/S3 with SSL  useIAM: false  cloudProvider: &quot;aws&quot;  iamEndpoint: &quot;&quot;  bucketName: &quot;a-bucket&quot; # Milvus Bucket name in MinIO/S3, make it the same as your milvus instance  rootPath: &quot;files&quot; # Milvus storage root path in MinIO/S3, make it the same as your milvus instance  backupBucketName: &quot;backup&quot;  # 配置存储备份的bucket    backupRootPath: &quot;backup&quot;  # 配置存储备份的根路径backup:    maxSegmentGroupSize: 2G</code></pre><h3 id="备份数据"><a href="#备份数据" class="headerlink" title="备份数据"></a>备份数据</h3><p>执行milvus-backup create命令创建备份，如果按提供的配置，最终创建的云存储数据可在<code>http://localhost:9001/browser/backup/YmFja3VwLw==</code> 查看到</p><pre><code class="shell">  cd milvus-backup  ./milvus-backup create -n data_20130920</code></pre><h3 id="安装-配置rclone-mc"><a href="#安装-配置rclone-mc" class="headerlink" title="安装/配置rclone/mc"></a>安装/配置rclone/mc</h3><p>rclone/mc是一个用于同步云平台文件和目录的命令行工具。</p><ul><li>1 安装rclone或者mc（mc似乎比rclone慢，而且实际使用中频繁卡死在99.99%左右，非常难受）</li></ul><pre><code class="shell">wget https://downloads.rclone.org/v1.64.0/rclone-v1.64.0-linux-amd64.zipunzip rclone-v1.64.0-linux-amd64.zipcd rclone-v1.64.0-linux-amd64sudo cp rclone /usr/bin/sudo chmod 755 /usr/bin/rclonesudo mkdir -p /usr/local/share/man/rclonesudo cp rclone.1 /usr/local/share/man/rclone/sudo mandb</code></pre><ul><li>2 配置rclone</li></ul><p>编辑~/.config/rclone/rclone.conf文件，写入相关minio服务的相关配置。下面的配置文件中local_minio为本地minio服务配置，test_minio为测试环境minio服务配置。</p><pre><code class="yml">[local_minio]type = s3provider = minioadminenv_auth = falseaccess_key_id = minioadminsecret_access_key = minioadminregion = cn-east-1endpoint = http://localhost:9000location_constraint =server_side_encryption =[test_minio]type = s3provider = minioadminenv_auth = falseaccess_key_id = minioadminsecret_access_key = minioadminregion = cn-east-1endpoint = http://192.168.0.1:9000location_constraint =server_side_encryption =</code></pre><ul><li>3 测试rclone配置是否ok</li></ul><p>查看测试环境所有bucket: <code>rclone lsd test_minio</code></p><p>查看本地环境所有bucket: <code>rclone lsd local_minio</code></p><h3 id="迁移数据"><a href="#迁移数据" class="headerlink" title="迁移数据"></a>迁移数据</h3><p>通过rclone sync 将local_minio上的backup桶同步到test_minio上的backup桶.</p><pre><code class="shell">rclone sync -P local_minio:/backup test_minio:/backup</code></pre><h3 id="恢复数据"><a href="#恢复数据" class="headerlink" title="恢复数据"></a>恢复数据</h3><ul><li>创建test_backup.yml文件。设置test_minio服务相关配置</li></ul><pre><code class="yml"># Configures the system log output.log:  level: info # Only supports debug, info, warn, error, panic, or fatal. Default &#39;info&#39;.  console: true  file:    rootPath: &quot;logs/test_backup.log&quot;http:  simpleResponse: true# milvus proxy address, compatible to milvus.yamlmilvus:  address: 192.168.0.1  # 【配置milvus地址】  port: 19530    # 配置milvus端口   authorizationEnabled: false  # tls mode values [0, 1, 2]  # 0 is close, 1 is one-way authentication, 2 is two-way authentication.  tlsMode: 0  user: &quot;root&quot;  password: &quot;Milvus&quot;# Related configuration of minio, which is responsible for data persistence for Milvus.minio:  address: 192.168.0.1 # Address of MinIO/S3  # 【配置minio地址】  port: 9000   # Port of MinIO/S3   # 配置minio端口   accessKeyID: minioadmin # accessKeyID of MinIO/S3     # 配置minio keyid  secretAccessKey: minioadmin # MinIO/S3 encryption string   # 配置minio  key   useSSL: false # Access to MinIO/S3 with SSL  useIAM: false  cloudProvider: &quot;aws&quot;  iamEndpoint: &quot;&quot;  bucketName: &quot;a-bucket&quot; # 【配置bucket名称】  rootPath: &quot;files&quot; # Milvus storage root path in MinIO/S3, make it the same as your milvus instance  backupBucketName: &quot;backup&quot;  # 配置存储备份的bucket    backupRootPath: &quot;backup&quot;  # 配置存储备份的根路径backup:    maxSegmentGroupSize: 2G</code></pre><ul><li>执行恢复命令<code>./milvus-backup --config test_backup.yml restore -c test_collection -n data_20130920</code></li></ul><h2 id="修复索引"><a href="#修复索引" class="headerlink" title="修复索引"></a>修复索引</h2><p>链接远程milvus服务，使用pymilvus sdk来重新创建刚刚导入的集合test_collection的索引</p><pre><code class="python3">  from pymilvus import Collection, connections  connections.connect(host=&#39;localhost&#39;, port=&#39;19530&#39;)  collection = Collection(name=&#39;test_collection&#39;)  collection.create_index(&#39;embedding&#39;, {      &quot;index_type&quot;: &quot;IVF_FLAT&quot;,      &quot;metric_type&quot;: &quot;IP&quot;,      &quot;params&quot;: {&quot;nlist&quot;: 1024},  })</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将本地milvus中的数据迁移到远程milvus服务中的操作流程。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;目录&lt;/li&gt;
&lt;li&gt;主要操作步骤&lt;/li&gt;
&lt;li&gt;详细操作步骤&lt;ul&gt;
&lt;li&gt;安装/配置milvus-backup&lt;/li&gt;
&lt;li&gt;备份数据&lt;/li&gt;
&lt;li&gt;安装/配置rclone/mc&lt;/li&gt;
&lt;li&gt;迁移数据&lt;/li&gt;
&lt;li&gt;恢复数据&lt;/li&gt;
&lt;li&gt;修复索引&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="运维" scheme="http://www.javabin.cn/tags/%E8%BF%90%E7%BB%B4/"/>
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="milvus" scheme="http://www.javabin.cn/tags/milvus/"/>
    
      <category term="数据迁移" scheme="http://www.javabin.cn/tags/%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>【墨水屏】2.9寸墨水屏固件适配、二次开发，主控芯片：TLSR8359</title>
    <link href="http://www.javabin.cn/2022/epaper.html"/>
    <id>http://www.javabin.cn/2022/epaper.html</id>
    <published>2022-07-23T04:12:23.000Z</published>
    <updated>2024-03-11T15:05:51.362Z</updated>
    
    <content type="html"><![CDATA[<p>从闲鱼上淘了几块电子价签，发现大部分都是msp430的主控，要进行编程还比较麻烦，手头并没有设备。但是其中有一款主控为TLSR8359的2.9寸设备，github已经有相近型号固件的项目，只需要进行简单适配应该就可以运行。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>目录</li><li>最终适配好的项目<ul><li>下载项目</li><li>编译</li><li>烧录</li><li>蓝牙上传图片</li></ul></li><li>主控芯片TLSR8359<ul><li>运行流程(软件)</li></ul></li><li>墨水屏屏幕驱动方式<ul><li>三色墨水屏原理</li><li>屏幕驱动协议</li><li>分辨率适配</li><li>图像处理方式</li><li>常用绘图操作</li></ul></li><li>远程控制<ul><li>蓝牙交互方式</li></ul></li><li>图像抖动算法<ul><li>为何需要抖动</li><li>bayer 黑白</li><li>floydsteinberg 黑白 多色</li><li>Atkinson 黑白 多色</li></ul></li><li>资料、参考<ul><li>屏幕驱动资料</li><li>主控芯片资料</li><li>抖动算法资料</li></ul></li></ul><a id="more"></a><h1 id="目录-1"><a href="#目录-1" class="headerlink" title="目录"></a>目录</h1><h2 id="最终适配好的项目"><a href="#最终适配好的项目" class="headerlink" title="最终适配好的项目"></a>最终适配好的项目</h2><p>原始项目已适配了2.3寸墨水屏，但在2.9寸墨水屏上存在一些问题，且图片上传方式有些麻烦。</p><h3 id="下载项目"><a href="#下载项目" class="headerlink" title="下载项目"></a>下载项目</h3><p><a href="https://github.com/reece15/stellar-L3N-etag" target="_blank" rel="noopener">github项目</a></p><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>系统：windows</p><pre><code class="cmd">    cd Firmware    makeit.exe clean &amp;&amp; makeit.exe -j12</code></pre><p>Firmware目录下会生成.bin格式的固件</p><h3 id="烧录"><a href="#烧录" class="headerlink" title="烧录"></a>烧录</h3><ul><li>拆开电池后盖查看主控是否为TLSR8359</li><li>焊接 GND, VCC, RX, RTS四根线。</li><li>使用usb2ttl模块(CH340)链接焊接的四根线。其中rx 链接 tx, tx链接 rx, vcc链接3.3v, GND链接 GND。RTS飞线和芯片CH340G第三脚链接（也可不焊，烧录前手动和GND连一下）。</li><li>打开<a href="https://atc1441.github.io/ATC_TLSR_Paper_UART_Flasher.html，" target="_blank" rel="noopener">https://atc1441.github.io/ATC_TLSR_Paper_UART_Flasher.html，</a> 波特率选择默认 460800，Atime默认，文件选择Firmware/ATC_Paper.bin</li><li>先点击unlock,再点击write to flush,等待完成。成功后，屏幕会自动刷新。</li></ul><h3 id="蓝牙上传图片"><a href="#蓝牙上传图片" class="headerlink" title="蓝牙上传图片"></a>蓝牙上传图片</h3><ul><li>运行 <code>cd web_tools &amp;&amp; python -m http.server</code></li><li>打开 <a href="http://127.0.0.1:8000" target="_blank" rel="noopener">http://127.0.0.1:8000</a> 后在页面上链接蓝牙</li><li>选择图片并上传，上传后可添加文字或者手动绘制文字。也可设置抖动算法。</li><li>发送到设备，等待屏幕刷新</li></ul><p><img src="/photo_2022/web.jpg" alt=""></p><h2 id="主控芯片TLSR8359"><a href="#主控芯片TLSR8359" class="headerlink" title="主控芯片TLSR8359"></a>主控芯片TLSR8359</h2><h3 id="运行流程-软件"><a href="#运行流程-软件" class="headerlink" title="运行流程(软件)"></a>运行流程(软件)</h3><p>执行流程：</p><ul><li>进入 主函数：main.c 的<code>int main (void)</code><ul><li>硬件初始化</li></ul></li><li>进入 app.c 的<code>main_loop</code>函数<ul><li>获取时间</li><li>获取温度，电量</li><li>更新屏幕内容 进入 epd.c <code>epd_update</code>函数<ul><li>根据设置的场景类型，调用不同场景处理函数，将场景显示到屏幕。</li><li>调用obd库对屏幕进行绘制。然后在EPD_Display函数中根据不同设备，调用不同的绘制函数。</li></ul></li><li>更新LED 状态</li><li>休眠屏幕设备</li></ul></li></ul><p>硬件部分可参考文末，TLSR8359 说明文档，或者从官网下载demo项目 2_4g_proprietary_sdk。</p><h2 id="墨水屏屏幕驱动方式"><a href="#墨水屏屏幕驱动方式" class="headerlink" title="墨水屏屏幕驱动方式"></a>墨水屏屏幕驱动方式</h2><h3 id="三色墨水屏原理"><a href="#三色墨水屏原理" class="headerlink" title="三色墨水屏原理"></a>三色墨水屏原理</h3><p>微胶囊电泳显示技术。墨水屏内部为悬浮在液体中的三色带电纳米微粒，通电后受电场影响移动而形成图案。本身不发光，通过反射环境光来显示图案。</p><p>程序上，要显示黑白色，只需要按位设置01数据，然后通过SPI写入屏幕RAM即可。其中1 代表 白色，0 代表黑色，即一字节可显示8个像素。</p><p>对于三色屏幕，内部有两片RAM，分别存储 黑白色和 红色。颜色对应关系：</p><table><thead><tr><th>黑白</th><th>红</th><th>显示颜色</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>黑</td></tr><tr><td>1</td><td>0</td><td>白</td></tr><tr><td>0</td><td>1</td><td>红</td></tr><tr><td>1</td><td>1</td><td>红</td></tr></tbody></table><h3 id="屏幕驱动协议"><a href="#屏幕驱动协议" class="headerlink" title="屏幕驱动协议"></a>屏幕驱动协议</h3><p>屏幕可参考SSD1680 的方式进行驱动，具体可参考 SSD1680.pdf的第39页，9.Operation Flow and Code Sequence ，详细命令的讲解可参考第20页命令表。</p><p>屏幕刷新流程为：</p><ul><li>通过SPI控制屏幕</li><li>发送0x12 进行SW重置 并等待10ms</li><li>发送0x01 驱动输出控制</li><li>发送0x11  设置数据扫描模式  00: Y轴自减，X轴自减；01: Y轴自减，X轴自增；10: Y轴自增，X轴自减；11: Y轴自增，X轴自增；</li><li>发送0x44  设置屏幕X轴 开始和结束位置。 计算方式 : 屏幕宽度/8 - 1 然后转换为 2字节数据。</li><li>发送0x45  设置屏幕Y轴 开始和结束位置。 计算方式 : (屏幕长度 - 1) 然后转换为 4字节数据。</li><li>发送0x3c Border Waveform Control</li><li>发送0x21 0x22 刷新控制</li><li>发送0x4E 设置RAM X轴 写入位置， 发送0X4F 设置RAM Y轴写入位置</li><li>发送0x24 开始写入 黑白 屏幕</li><li>发送0x26 开始写入 红 屏幕</li><li>发送0x22 0x20 显示数据</li><li>发送0x10 休眠屏幕</li></ul><h3 id="分辨率适配"><a href="#分辨率适配" class="headerlink" title="分辨率适配"></a>分辨率适配</h3><p>创建新设备的epd_bwr_xxx.c 和 .h文件, 修改 0x44 0x45 0x24 0x26命令处发送的屏幕大小相关数据。即可适配新的屏幕类型。</p><h3 id="图像处理方式"><a href="#图像处理方式" class="headerlink" title="图像处理方式"></a>图像处理方式</h3><p>普通图片数据需要 旋转90度，才是实际可正常显示的输入数据。</p><p>python转换图片为屏幕RAM数据：</p><pre><code class="python">    from PIL import Image    from PIL.Image import Dither # noqa    def bytes2hex(_bytes):        return &#39;&#39;.join(hex(item)[2:].zfill(2) for item in _bytes)    def image2hex(image, width=296, height=128, dither=Dither.NONE):        if isinstance(image, str):            image = Image.open(image)        image = image.resize((width, height)).rotate(90, expand=True)        # image.show()        return bytes2hex(image.resize((height, width)).convert(&#39;1&#39;, dither=dither).tobytes())    if __name__ == &#39;__main__&#39;:        path = &#39;1.png&#39;        print(image2hex(Image.open(path), dither=Dither.FLOYDSTEINBERG)</code></pre><h3 id="常用绘图操作"><a href="#常用绘图操作" class="headerlink" title="常用绘图操作"></a>常用绘图操作</h3><pre><code class="C">    // 绘制英文文字 从 x=1, y=17处开始绘制。不反色。    obdWriteStringCustom(&amp;obd, (GFXfont *)&amp;Dialog_plain_16, 1, 17, (char *)buff, 1);    // 绘制矩形 从 x=0, y=25处开始绘制 直到 295，27。颜色为黑色，且进行填充    obdRectangle(&amp;obd, 0, 25, 295, 27, 1, 1);    // 清空屏幕    obdFill(&amp;obd, 0, 0);</code></pre><h2 id="远程控制"><a href="#远程控制" class="headerlink" title="远程控制"></a>远程控制</h2><h3 id="蓝牙交互方式"><a href="#蓝牙交互方式" class="headerlink" title="蓝牙交互方式"></a>蓝牙交互方式</h3><p>TLSR8359支持低功耗蓝牙，可在app_att.c中添加蓝牙交互接口,读写权限。调用 bls_att_pushNotifyData 发送数据。</p><h2 id="图像抖动算法"><a href="#图像抖动算法" class="headerlink" title="图像抖动算法"></a>图像抖动算法</h2><h3 id="为何需要抖动"><a href="#为何需要抖动" class="headerlink" title="为何需要抖动"></a>为何需要抖动</h3><p>由于墨水屏一般只有 黑白或者黑白红，无法显示全部颜色，如果直接进行二值化，显示的图片将丢失很多明暗细节。可通过牺牲图像分辨率，通过控制局部像素点的稀疏来达到明暗变化的效果（黑白旧报纸或者黑白打印机的显示方式）</p><h3 id="bayer-黑白"><a href="#bayer-黑白" class="headerlink" title="bayer 黑白"></a>bayer 黑白</h3><p>取一个NxN的矩阵，将当前像素的x y坐标分别 对N 取余，可将xy 对应到 NxN中的一个位置，算出矩阵对应位置值+当前颜色值，根据其和阈值的大小关系决定当前位置显示黑色还是白色。</p><pre><code class="js">    const bayerThresholdMap = [        [  15, 135,  45, 165 ],        [ 195,  75, 225, 105 ],        [  60, 180,  30, 150 ],        [ 240, 120, 210,  90 ]      ];    const imageData = ctx.getImageData(0, 0, width, height);    const w = imageData.width;    //  遍历imageData currentPixel 为index    const x = currentPixel/4 % w;    const y = Math.floor(currentPixel/4 / w);    const map = Math.floor( (imageData.data[currentPixel] + bayerThresholdMap[x%4][y%4]) / 2 );    imageData.data[currentPixel] = (map &lt; threshold) ? 0 : 255;</code></pre><h3 id="floydsteinberg-黑白-多色"><a href="#floydsteinberg-黑白-多色" class="headerlink" title="floydsteinberg 黑白 多色"></a>floydsteinberg 黑白 多色</h3><ul><li>将当前RGB颜色转换到LAB颜色空间</li><li>在LAB颜色空间计算 当前像素颜色值 和 可选颜色距离最近的颜色。</li><li>计算出选择的颜色和当前颜色RGB通道的误差。</li><li>将误差传递到 当前像素的 右，左下，下，右下方像素。即给这些像素的RGB通道分别加上误差*比率。</li><li>重复执行，直到所有像素被处理。</li></ul><p>计算最近距离的颜色：</p><pre><code class="js">    function getNearColorV2(color, palette) {      let minDistanceSquared = 255*255 + 255*255 + 255*255 + 1;      let bestIndex = 0;      for (let i = 0; i &lt; palette.length; i++) {          let rdiff = (color[0] &amp; 0xff) - (palette[i][0] &amp; 0xff);          let gdiff = (color[1] &amp; 0xff) - (palette[i][1] &amp; 0xff);          let bdiff = (color[2] &amp; 0xff) - (palette[i][2] &amp; 0xff);          let distanceSquared = rdiff*rdiff + gdiff*gdiff + bdiff*bdiff;          if (distanceSquared &lt; minDistanceSquared) {              minDistanceSquared = distanceSquared;              bestIndex = i;          }      }      return palette[bestIndex];    }</code></pre><p>算法实现：</p><pre><code class="js">    const newColor = getNearColorV2(imageData.data.slice(currentPixel, currentPixel+4), palette);    const err = getColorErr(imageData.data.slice(currentPixel, currentPixel+4), newColor, 16);    updatePixel(imageData.data, currentPixel, newColor);    updatePixelErr(imageData.data, currentPixel +4, err, 7);    updatePixelErr(imageData.data, currentPixel + 4*w - 4, err, 3);    updatePixelErr(imageData.data, currentPixel + 4*w, err, 5);    updatePixelErr(imageData.data, currentPixel + 4*w + 4, err, 1);    function updatePixelErr(imageData, index, err, rate) {      imageData[index] += err[0] * rate;      imageData[index+1] += err[1] * rate;      imageData[index+2] += err[2] * rate;    }</code></pre><h3 id="Atkinson-黑白-多色"><a href="#Atkinson-黑白-多色" class="headerlink" title="Atkinson 黑白 多色"></a>Atkinson 黑白 多色</h3><ul><li>和 floydsteinberg 类似操作，不同是 将误差再传递到下下一行像素。</li></ul><p>算法实现：</p><pre><code class="js">    const newColor = getNearColorV2(imageData.data.slice(currentPixel, currentPixel+4), palette);    const err = getColorErr(imageData.data.slice(currentPixel, currentPixel+4), newColor, 8);    updatePixel(imageData.data, currentPixel, newColor);    updatePixelErr(imageData.data, currentPixel +4, err, 1);    updatePixelErr(imageData.data, currentPixel +8, err, 1);    updatePixelErr(imageData.data, currentPixel +4 * w - 4, err, 1);    updatePixelErr(imageData.data, currentPixel +4 * w, err, 1);    updatePixelErr(imageData.data, currentPixel +4 * w + 4, err, 1);    updatePixelErr(imageData.data, currentPixel +8 * w, err, 1);</code></pre><p>具体可参考 git项目的 /web_tools/js/dithering.js</p><h2 id="资料、参考"><a href="#资料、参考" class="headerlink" title="资料、参考"></a>资料、参考</h2><h3 id="屏幕驱动资料"><a href="#屏幕驱动资料" class="headerlink" title="屏幕驱动资料"></a>屏幕驱动资料</h3><blockquote><p>github项目 /docs/SSD1680.pdf<br><a href="https://mc.dfrobot.com.cn/thread-311306-3-1.html" target="_blank" rel="noopener">屏幕相关讨论</a><br><a href="https://www.waveshare.net/wiki/Pico-ePaper-2.9" target="_blank" rel="noopener">微雪2.9寸墨水屏</a></p></blockquote><h3 id="主控芯片资料"><a href="#主控芯片资料" class="headerlink" title="主控芯片资料"></a>主控芯片资料</h3><blockquote><p><a href="https://github.com/atc1441/ATC_TLSR_Paper" target="_blank" rel="noopener">ATC_Paper github 项目</a><br><a href="http://wiki.telink-semi.cn/wiki/solution/ESL/" target="_blank" rel="noopener">泰凌微TLSR8359相关资源</a><br>github项目 /docs/DS_TLSR8359-E_Datasheet for Telink ULP 2.4GHz RF SoC TLSR8359.pdf<br>github项目 /docs/Telink Kite BLE SDK Developer Handbook中文.pdf</p></blockquote><h3 id="抖动算法资料"><a href="#抖动算法资料" class="headerlink" title="抖动算法资料"></a>抖动算法资料</h3><blockquote><p><a href="https://www.codenong.com/12422407/" target="_blank" rel="noopener">多种抖动算法</a><br><a href="https://blog.csdn.net/qq_61888524/article/details/123858959" target="_blank" rel="noopener">颜色距离计算方式</a><br><a href="https://www.cnblogs.com/yanye0xff/p/16073279.html" target="_blank" rel="noopener">多种抖动矩阵</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从闲鱼上淘了几块电子价签，发现大部分都是msp430的主控，要进行编程还比较麻烦，手头并没有设备。但是其中有一款主控为TLSR8359的2.9寸设备，github已经有相近型号固件的项目，只需要进行简单适配应该就可以运行。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;目录&lt;/li&gt;
&lt;li&gt;最终适配好的项目&lt;ul&gt;
&lt;li&gt;下载项目&lt;/li&gt;
&lt;li&gt;编译&lt;/li&gt;
&lt;li&gt;烧录&lt;/li&gt;
&lt;li&gt;蓝牙上传图片&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;主控芯片TLSR8359&lt;ul&gt;
&lt;li&gt;运行流程(软件)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;墨水屏屏幕驱动方式&lt;ul&gt;
&lt;li&gt;三色墨水屏原理&lt;/li&gt;
&lt;li&gt;屏幕驱动协议&lt;/li&gt;
&lt;li&gt;分辨率适配&lt;/li&gt;
&lt;li&gt;图像处理方式&lt;/li&gt;
&lt;li&gt;常用绘图操作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;远程控制&lt;ul&gt;
&lt;li&gt;蓝牙交互方式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;图像抖动算法&lt;ul&gt;
&lt;li&gt;为何需要抖动&lt;/li&gt;
&lt;li&gt;bayer 黑白&lt;/li&gt;
&lt;li&gt;floydsteinberg 黑白 多色&lt;/li&gt;
&lt;li&gt;Atkinson 黑白 多色&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;资料、参考&lt;ul&gt;
&lt;li&gt;屏幕驱动资料&lt;/li&gt;
&lt;li&gt;主控芯片资料&lt;/li&gt;
&lt;li&gt;抖动算法资料&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="固件" scheme="http://www.javabin.cn/tags/%E5%9B%BA%E4%BB%B6/"/>
    
      <category term="墨水屏" scheme="http://www.javabin.cn/tags/%E5%A2%A8%E6%B0%B4%E5%B1%8F/"/>
    
      <category term="C语言" scheme="http://www.javabin.cn/tags/C%E8%AF%AD%E8%A8%80/"/>
    
      <category term="单片机" scheme="http://www.javabin.cn/tags/%E5%8D%95%E7%89%87%E6%9C%BA/"/>
    
      <category term="抖动算法" scheme="http://www.javabin.cn/tags/%E6%8A%96%E5%8A%A8%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>【3D打印机降噪】MKs gen l v2.1主板更换4988电机驱动为TMC2209</title>
    <link href="http://www.javabin.cn/2022/3dprinter-2209.html"/>
    <id>http://www.javabin.cn/2022/3dprinter-2209.html</id>
    <published>2022-04-13T14:14:46.000Z</published>
    <updated>2024-03-11T14:49:03.590Z</updated>
    
    <content type="html"><![CDATA[<p>3d打印机晚上运行时，噪音非常影响睡眠，可通过将自带的4988电机驱动更新到静音驱动tmc2208或者tmc2209来解决。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>硬件情况</li><li>噪音产生的原因</li><li>电机噪音处理<ul><li>更换方法</li><li>跳帽接法</li><li>故障处理：电压调试</li></ul></li><li>其他噪音的处理方法<ul><li>主机震动</li><li>散热器风扇<ul><li>打印头</li><li>物料</li><li>开关电源</li></ul></li></ul></li><li>效果<a id="more"></a></li></ul><h2 id="硬件情况"><a href="#硬件情况" class="headerlink" title="硬件情况"></a>硬件情况</h2><ul><li>结构: corexy</li><li>主板：MKs gen l v2.1</li><li>电机驱动: 4988 * 5 (xy轴各一个，z轴两个，挤出机一个)</li><li>电源: 220v30A</li></ul><h2 id="噪音产生的原因"><a href="#噪音产生的原因" class="headerlink" title="噪音产生的原因"></a>噪音产生的原因</h2><ul><li>电机运动时的噪音：<ul><li>主要是4988电机驱动驱动步进电机不足够平滑，导致电机运动发出振动和噪音。参考<a href="https://www.jianshu.com/p/33f3f44b7840?ivk_sa=1024320u" target="_blank" rel="noopener">TMC2208与A4988步进电机驱动对比分析</a></li><li>运动时和轴的摩擦，刚组装的新机几乎无声。</li></ul></li><li>运动导致的框架振动。</li><li>散热器风扇<ul><li>3d打印机工作时，打印头需要持续散热维持稳定的温度；</li><li>而物料也需要风扇来使其在出打印头快速冷却下来，实现搭桥打印；</li><li>开关电源电流一般得10A以上，也需要风扇暴力散热。</li></ul></li></ul><h2 id="电机噪音处理"><a href="#电机噪音处理" class="headerlink" title="电机噪音处理"></a>电机噪音处理</h2><h3 id="升级方法"><a href="#升级方法" class="headerlink" title="升级方法"></a>升级方法</h3><h4 id="1-准备清单"><a href="#1-准备清单" class="headerlink" title="1. 准备清单"></a>1. 准备清单</h4><p>corexy结构的3d打印机，一般只需要更换运动比较频繁的 x轴，z轴和 挤出机电机即可。<br>物料清单：</p><ul><li>tmc2209 * 3 （也可以全换了 5个）</li><li>万用表 * 1</li></ul><h4 id="2-更换方法"><a href="#2-更换方法" class="headerlink" title="2. 更换方法"></a>2. 更换方法</h4><ul><li>注意！注意4988和tmc2209的引脚，tmc2209的GND引脚必须和主板上原来4988的gnd位置一致，<code>注意：如果插反，2209电机驱动板将会烧毁</code>。</li><li>1.拔下4988并记下GND位置。</li><li>2.将跳帽接到右上方两个。其他拔掉。如果是从4988升级，也就是拔掉最后一个。具体看下面的图。</li><li>3.对齐GND脚，插入tmc2209驱动</li><li>4.拔下电机的连接线（<code>注意：这一步需要操作，防止下一步损坏电机</code>）,每个驱动板旁边就是他的电机接口。具体参考<a href="https://blog.csdn.net/gjy_skyblue/article/details/119872104" target="_blank" rel="noopener">MKS GEN_L V2.1使用说明书</a></li><li>5.使用电压表测量电压调节螺丝和gnd引脚之间的电压，将电压调到0.96到1v左右即可，电压太高会导致电机过热而丢步。顺时钟旋转减小，逆时针增大。<code>注意：不要提前装装散热片，不好操作，会拧掉调节螺丝</code></li><li>6.将电机连接线翻转后插入。（如果不翻转，测试时将会看到电机的运动方向和实际相反）</li><li>7.装上驱动板散热器。</li><li>8.其他驱动按上面的操作即可。</li><li>9.测试：单独控制xyz测试是否正常</li><li>10.测试：自动回零和打印是否正常。如果丢步就再调小下电压；如果打印头运动方向相反，就翻转下对应的电机连接线。</li></ul><h3 id="跳帽接法"><a href="#跳帽接法" class="headerlink" title="跳帽接法"></a>跳帽接法</h3><p>将跳帽接到右上方两个。其他拔掉。如果是从4988升级，拔掉最后一个就行。<br><img src="/photo_2022/tm.png" alt="跳帽接法"></p><h3 id="故障处理：电压调试"><a href="#故障处理：电压调试" class="headerlink" title="故障处理：电压调试"></a>故障处理：电压调试</h3><p>红笔接螺丝，黑笔接GND。用起子旋转螺丝即可。<br><img src="/photo_2022/tmc2209.png" alt="电压调试位置"></p><h2 id="其他噪音的处理方法"><a href="#其他噪音的处理方法" class="headerlink" title="其他噪音的处理方法"></a>其他噪音的处理方法</h2><h3 id="主机震动"><a href="#主机震动" class="headerlink" title="主机震动"></a>主机震动</h3><p>使用海绵或者橡胶垫，垫在四角即可。</p><h3 id="散热器风扇"><a href="#散热器风扇" class="headerlink" title="散热器风扇"></a>散热器风扇</h3><ul><li>打印头<ul><li>将电源线接到散热器控制口，进行控制；</li><li>或者使用低转速风扇，打印头无需太强散热。我目前使用低转速风扇。</li></ul></li><li>物料<ul><li>使用静音风扇。风扇噪音情况: 液压轴承&gt;含油轴承&gt;滚珠轴承，买静音（转速不高）的液压轴承风扇换上即可。</li><li>并联两个风扇，降低风扇电压，转速。</li><li>串联电位器对风扇调速，注意对电位器贴散热片。（支持pwm调速的风扇可直接使用pwm调低转速）。</li><li>打印消音装置并安装。这个我打印了几个，试了下，效果并不明显，<a href="https://www.thingiverse.com/" target="_blank" rel="noopener">thingiverse</a>上大部分的消音装置, 网友都反馈效果不明显。</li><li>我目前使用低转速液压风扇，几乎无声。</li></ul></li><li>开关电源<ul><li>更换10A下无散热的电源。如果电流不足，可对热床单独供电。</li><li>同物料。</li><li>我目前还未更换。</li></ul></li></ul><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>更换完后，目前最近距离只能听到电源风扇声音。其他几乎无声。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;3d打印机晚上运行时，噪音非常影响睡眠，可通过将自带的4988电机驱动更新到静音驱动tmc2208或者tmc2209来解决。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;硬件情况&lt;/li&gt;
&lt;li&gt;噪音产生的原因&lt;/li&gt;
&lt;li&gt;电机噪音处理&lt;ul&gt;
&lt;li&gt;更换方法&lt;/li&gt;
&lt;li&gt;跳帽接法&lt;/li&gt;
&lt;li&gt;故障处理：电压调试&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他噪音的处理方法&lt;ul&gt;
&lt;li&gt;主机震动&lt;/li&gt;
&lt;li&gt;散热器风扇&lt;ul&gt;
&lt;li&gt;打印头&lt;/li&gt;
&lt;li&gt;物料&lt;/li&gt;
&lt;li&gt;开关电源&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;效果
    
    </summary>
    
    
      <category term="硬件" scheme="http://www.javabin.cn/tags/%E7%A1%AC%E4%BB%B6/"/>
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>【虚拟现实】Oculus Quest2, Pico neo3, 三星玄龙MR+虚拟现实头显真实使用体验</title>
    <link href="http://www.javabin.cn/2022/vr.html"/>
    <id>http://www.javabin.cn/2022/vr.html</id>
    <published>2022-04-04T06:47:24.000Z</published>
    <updated>2024-03-11T14:49:03.613Z</updated>
    
    <content type="html"><![CDATA[<p>目前购买并体验了三台vr设备，分别是facebook的quest2, 国内的pico neo3和三星的玄龙MR+, 其中前两款是一体机，最后一款是vr头显。下面谈一下各个设备的使用体验。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>目录</li><li>先说结论<ul><li>如何选择</li><li>购买渠道</li><li>多久会玩腻？vr设备缺点</li></ul></li><li>vr设备相关基本概念<ul><li>一体机</li><li>头显</li><li>可视角度</li><li>单眼分辨率</li><li>屏幕类型</li><li>屏幕刷新率</li><li>定位系统</li><li>有线串流</li><li>无线串流</li></ul></li><li>quest2, pico neo3, mr+对比<ul><li>参数、体验对比</li><li>quest2体验</li><li>pico neo3体验</li></ul></li><li>游戏和应用推荐</li><li>配件选购</li><li>quest2 常见问题</li><li>pico neo3 常见问题</li><li>mr+ 常见问题</li><li>如何开发vr游戏demo并在设备上运行<ul><li>pico neo3</li></ul></li></ul><a id="more"></a><h2 id="先说结论"><a href="#先说结论" class="headerlink" title="先说结论"></a>先说结论</h2><h3 id="如何选择"><a href="#如何选择" class="headerlink" title="如何选择"></a>如何选择</h3><ul><li>1.如果是想体验<code>海量vr游戏</code>，看vr电影，首选装满游戏的<code>oculus quest2</code>+舒适头戴。如果购买默认没有安装游戏的quest2, 还需要电脑科学上网后使用SSTAP分享网络对设备进行激活和升级。</li><li>2.如果喜欢看电影和进行<code>在线</code>互动且不在意<code>主流游戏支持程度</code>，可选 <code>pico neo3</code>。pico支持的游戏少的可怜。</li><li>3.如果<code>囊中羞涩</code>，且自身电脑显卡性能高于等于<code>GTX1660TI</code>， 可选 <code>三星玄龙MR+</code>。</li><li>4.如果你让我只推荐一个，我推荐Oculus quest2。</li></ul><h3 id="购买渠道"><a href="#购买渠道" class="headerlink" title="购买渠道"></a>购买渠道</h3><h4 id="1-海鲜市场-闲鱼"><a href="#1-海鲜市场-闲鱼" class="headerlink" title="1.海鲜市场(闲鱼)"></a>1.海鲜市场(闲鱼)</h4><p><code>quest2 128GB</code> 一手最低价1900+，一般2000左右，二手价格和一手价格一致。闲鱼上基本刚挂上就会有人买。</p><p><code>pico neo3 128GB带包+先锋卡</code>，我是1650收的，现在1400+就可收到。</p><p><code>三星玄龙mr+</code> 1100+可以收到，我当时是1700收的，1480卖的。</p><h4 id="2-官方渠道"><a href="#2-官方渠道" class="headerlink" title="2.官方渠道"></a>2.官方渠道</h4><p><code>quest2 128GB</code> 天猫海外直邮，有活动时 1950-2015元左右，一般10天送到（日本发货-国际航班-海关-清关-国内快递转运，国外一周，国内三天）。微信小程序 亚马逊一般含税2100左右（速度更慢）。</p><p><code>pico neo3</code> 旗舰店 2499，打卡180天返费一半，相当于半价（需要每天晃动手柄半小时以上且进行签到）。</p><p><code>三星玄龙mr+</code> 前几年京东4399+。现在官方已经下架。</p><h3 id="多久会玩腻？vr设备缺点"><a href="#多久会玩腻？vr设备缺点" class="headerlink" title="多久会玩腻？vr设备缺点"></a>多久会玩腻？vr设备缺点</h3><p>一般一到两个月左右就会吃灰。原因是： </p><ul><li>太累。普通PC，switch, ps主机玩游戏并不需要肢体大幅度运动，而vr游戏一般都需要身体和双手大幅度移动来控制游戏，长时间使用非常累。 </li><li>舒适度不够。目前家用的vr设备体积和重量还是很大，使用时间稍长就会感觉压鼻子，压眼睛，勒脑壳（特别是quest2，重量全在前方，非常不舒服）。而且使用vr设备看电影躺着并不舒服，要么设备太重压脸（quest2）；要么脑壳后方的系的带子不平，导致平躺硌脑壳（pico neo3, mr+）。 </li><li>起雾。虽然可以使用vr设备进行健身游戏，但是头戴内部的镜片会很快起雾，必须频繁擦拭才行。</li><li>可玩性有限。元宇宙概念吹的很大，但是现在还只是把屏幕戴脑壳上然后玩个游戏，看个电影，线上互动下。而且游戏和PC，主机比 还是差很多，大部分都是小游戏级别。</li></ul><h2 id="vr设备相关基本概念"><a href="#vr设备相关基本概念" class="headerlink" title="vr设备相关基本概念"></a>vr设备相关基本概念</h2><h4 id="一体机"><a href="#一体机" class="headerlink" title="一体机"></a>一体机</h4><p>设备既要显示图像，还需要责数据处理，图像渲染。一般有自己的操作系统，可以无需链接电脑，单独运行。</p><h3 id="头显"><a href="#头显" class="headerlink" title="头显"></a>头显</h3><p>设备只是一个特殊的显示器。图像的渲染需要PC来提供。一般没有操作系统，必须链接电脑才可以运行。</p><h3 id="可视角度"><a href="#可视角度" class="headerlink" title="可视角度"></a>可视角度</h3><p>FOV. 可视角度范围。FOV越大越好。人肉眼最大可到220度，中心60度左右最新清晰。vr设备FOV太小会影响沉浸感，即会有一种透过望远镜看东西的感觉，两侧有大范围黑色的盲区。</p><h3 id="单眼分辨率"><a href="#单眼分辨率" class="headerlink" title="单眼分辨率"></a>单眼分辨率</h3><p>vr设备单个屏幕的分辨率。分辨率越高越好。vr设备分辨率太小会让人感觉到模糊，并且纱窗效应很强烈。纱窗效应类似于当你看电视时，凑近电视屏幕，即可以看到图像是密密麻麻的像素点组成，在这个距离，你的眼睛已经可以明显感受到图像是由纵横的像素组成，非常影响沉浸感。</p><h3 id="屏幕类型"><a href="#屏幕类型" class="headerlink" title="屏幕类型"></a>屏幕类型</h3><p>参考手机屏幕类型。</p><h3 id="屏幕刷新率"><a href="#屏幕刷新率" class="headerlink" title="屏幕刷新率"></a>屏幕刷新率</h3><p>参考手机屏幕刷新率</p><h3 id="定位系统"><a href="#定位系统" class="headerlink" title="定位系统"></a>定位系统</h3><p>手柄移动时，需要判断手柄移动的速度和位置，来控制vr场景中的人物动作。定位系统会影响到游戏的流畅性。目前常见的定位系统有：陀螺仪，图像识别, 基站定位。<br>基站定位： 效果最好，精度最高，最贵，可全身追踪。缺点是：和其他的相比安装麻烦，比较贵。<br>图像识别：性价比最高，大部分就是这个。缺点是 1.不能关灯或者在光线太暗的环境使用 2.如果手伸到身后或其他特殊角度，导致头显上的摄像头看不到，就会突然丢失手柄位置。3.稳定性受图像识别算法的影响<br>陀螺仪： 手机和其他山寨盒子。</p><h3 id="有线串流"><a href="#有线串流" class="headerlink" title="有线串流"></a>有线串流</h3><p>如果想玩stream,OCULUS等PC平台的游戏，就需要进行PC串流，实际就是将一体机当做头显来使用。这时候可以使用串流线（如usb3.0, usb3.1 gen1, usb3.1 gen2链接线）来链接电脑进行有线串流。<br>即电脑运行游戏后，将画面串流到vr设备屏幕上。<br>优点是：画面流畅 延迟小，<br>缺点是：需要链接线，最好准备至少3米的usb3.0以上的链接线。</p><h3 id="无线串流"><a href="#无线串流" class="headerlink" title="无线串流"></a>无线串流</h3><p>和有线串流差不多，只不过是将串流线换成了wifi，一般需要wifi6+5G来提供流畅的链接。<br>优点是：没有线的束缚，自由；<br>缺点是：如果路由器带宽不行，那么延迟较大，容易受干扰而卡顿；</p><h2 id="quest2-pico-neo3-mr-对比"><a href="#quest2-pico-neo3-mr-对比" class="headerlink" title="quest2, pico neo3, mr+对比"></a>quest2, pico neo3, mr+对比</h2><h3 id="参数、体验对比"><a href="#参数、体验对比" class="headerlink" title="参数、体验对比"></a>参数、体验对比</h3><table><thead><tr><th>项目</th><th>quest2</th><th>pico neo3</th><th>mr+</th></tr></thead><tbody><tr><td>类型</td><td><font color="#008000">一体机</font></td><td><font color="#008000">一体机</font></td><td>头显</td></tr><tr><td>单眼分辨率</td><td><font color="#008000">1832x1920</font></td><td><font color="#008000">1832x1920</font></td><td>1440x1600</td></tr><tr><td>可视角度</td><td>90</td><td>90</td><td><font color="#008000">110</font></td></tr><tr><td>定位系统</td><td><font color="#008000">图像识别</font></td><td>图像识别</td><td>图像识别</td></tr><tr><td>屏幕类型</td><td>LCD</td><td>LCD</td><td><font color="#008000">OLED</font></td></tr><tr><td>屏幕刷新率</td><td><font color="#008000">60-120HZ</font></td><td>72-90HZ</td><td>单元格</td></tr><tr><td>有线串流</td><td><font color="#008000">支持</font></td><td><font color="#008000">支持</font></td><td><font color="#008000">支持</font></td></tr><tr><td>无线串流</td><td><font color="#008000">支持</font></td><td><font color="#008000">支持</font></td><td>不支持</td></tr><tr><td>购买渠道</td><td>海外代购,天猫直邮,亚马逊微信小程序</td><td><font color="#008000">国内旗舰店</font></td><td>闲鱼</td></tr><tr><td>支持pc平台</td><td>stream/ocolus</td><td>stream</td><td><font color="#008000">微软虚拟门户/stream/ocolus</font></td></tr><tr><td>支持游戏/应用数量</td><td><font color="#008000">很多</font></td><td>少</td><td>少</td></tr><tr><td>一手价格</td><td><font color="#008000">1950-2200</font></td><td>2399</td><td>无</td></tr><tr><td>二手价格</td><td>1900-2100</td><td>1600-1800</td><td><font color="#008000">1100</font></td></tr><tr><td>手势识别</td><td><font color="#008000">支持</font></td><td>不支持</td><td>不支持</td></tr><tr><td>清晰度(个人体验)</td><td><font color="#008000">很清晰</font></td><td>清晰</td><td>有纱窗效应，远处模糊</td></tr><tr><td>纱窗感(个人体验)</td><td><font color="#008000">几乎无</font></td><td><font color="#008000">几乎无</font></td><td>有</td></tr><tr><td>丢失手柄追踪(个人体验)</td><td><font color="#008000">少</font></td><td>偶尔</td><td>偶尔</td></tr><tr><td>卡顿(个人体验)</td><td><font color="#008000">几乎无</font></td><td>偶尔</td><td>偶尔</td></tr><tr><td>系统稳定性(个人体验)</td><td><font color="#008000">稳定</font></td><td>一般（偶尔重启）</td><td>差，需要反复插拔</td></tr><tr><td>安装难易(个人体验)</td><td>难，需科学上网，激活，facebook账户</td><td><font color="#008000">易</font></td><td><font color="#008000">易</font></td></tr><tr><td>看电影效果(个人体验)</td><td><font color="#008000">清晰，流畅</font></td><td><font color="#008000">清晰，流畅</font></td><td><font color="#008000">清晰，流畅</font></td></tr><tr><td>舒适度(个人体验)</td><td>不舒适</td><td><font color="#008000">舒适</font></td><td>一般</td></tr></tbody></table><h3 id="quest2体验"><a href="#quest2体验" class="headerlink" title="quest2体验"></a>quest2体验</h3><p>优点：清晰，流畅，游戏和应用非常多，支持手势识别，系统稳定。<br>缺点：初次激活需要facebook账户登录；联机没有体验过，必须开sstap全局代理和科学上网，比较麻烦；使用stream联机big screen经常卡顿，且频道内都是外语交流。</p><p><img src="/photo_2022/quest2.jpg" alt="quest2.jpg"></p><h3 id="pico-neo3体验"><a href="#pico-neo3体验" class="headerlink" title="pico neo3体验"></a>pico neo3体验</h3><p>优点：清晰，流畅，线上互动（多人影院，多人绘画，pico home影院，多人射击游戏）由于流量直接走的国内，体验非常流畅，国人很多。<br>缺点：游戏中心的游戏很少。大部分热门游戏只能通过stream串流玩。</p><p><img src="/photo_2022/picneo3.jpg" alt="picneo3.jpg"></p><h3 id="MR-体验"><a href="#MR-体验" class="headerlink" title="MR+体验"></a>MR+体验</h3><p>优点： 便宜; 可以串流玩stream平台,windows MR平台, Oculus平台;fov大，沉浸感强.<br>缺点： 有纱窗感（并不会影响游戏和观影，但是当你稍微细看，或者画面切换时，会立马有纱窗感）；远处模糊；</p><p><img src="/photo_2022/mr.jpg" alt="mr.jpg"></p><h2 id="配件选购"><a href="#配件选购" class="headerlink" title="配件选购"></a>配件选购</h2><h3 id="quest2"><a href="#quest2" class="headerlink" title="quest2"></a>quest2</h3><ul><li>舒适头戴。必选，不然体验非常差。淘宝，京东 100元左右。</li><li>科学上网。必选。<a href="https://ds2.club/22662d8" target="_blank" rel="noopener">科学上网</a></li><li>串流线。可选。闲鱼25元左右。</li><li>近视眼镜片。可选。 闲鱼80元+，淘宝180元+。</li><li>手柄保护套。可选。</li><li>gtx1660ti+显卡。可选。不进行stream串流的话，不需要。</li><li>充电电池。必选。京东 倍量充电电池 4节+充电器</li></ul><h3 id="pico-neo3"><a href="#pico-neo3" class="headerlink" title="pico neo3"></a>pico neo3</h3><ul><li>先锋卡激活码。必选。送四款游戏。（不选这个，几乎无任何游戏可玩，只能看电影和串流stream）</li><li>打卡返现激活码。可选。每天必须晃动手柄30分钟且签到，持续180天，可返一半现金。</li><li>收纳包。可选。会赠送。</li><li>串流线。可选。闲鱼25元左右。</li><li>近视眼镜片。可选。 闲鱼80元+，淘宝180元+。</li><li>手柄保护套。可选</li><li>gtx1660ti+显卡。可选。stream串流的话，需要。</li><li>充电电池。必选。京东 倍量充电电池 4节+充电器</li></ul><h3 id="MR"><a href="#MR" class="headerlink" title="MR+"></a>MR+</h3><ul><li>gtx1660ti+显卡。必选</li><li>充电电池。必选。京东 倍量充电电池 4节+充电器</li><li>近视眼镜片。可选。 闲鱼80元+，淘宝180元+。</li></ul><h2 id="游戏和应用推荐"><a href="#游戏和应用推荐" class="headerlink" title="游戏和应用推荐"></a>游戏和应用推荐</h2><h3 id="steam"><a href="#steam" class="headerlink" title="steam"></a>steam</h3><ul><li>半衰期。神作，真实度最高，射击，恐怖，变异生物。</li><li>节奏光剑。运动，音乐游戏，热门游戏。</li><li>燥热。射击，子弹时间。</li><li>美丽水世界。剧情</li><li>宇宙模拟器。真实数据还原宇宙目前所有已知天体，天文，科幻。</li><li>宇宙沙盒2。宇宙天体运动模拟。</li><li>热狗、马蹄和手榴弹。枪械模拟，射击。</li><li>Pistol Whip。 射击。</li><li>渔夫的故事。剧情。</li><li>乒乓球。运动，体育竞技。</li><li>假日模拟器。模拟。</li><li>机械重装。射击</li><li>夏日乐园。模拟，过山车，攀爬，射击，赛车，电影，游乐园其他项目。</li><li>深蓝。深海体验，vr体验。</li><li>onshape。墙来了，运动健身。</li><li>笔刷。vr作画。</li><li>虚拟桌面。无线串流</li><li>the lab。stream自带，模拟。</li><li>big screen. 模拟电影院，线上互动。</li><li>vrchat。虚拟社交，线上互动。</li></ul><h3 id="quest2-1"><a href="#quest2-1" class="headerlink" title="quest2"></a>quest2</h3><ul><li>节奏光剑。运动，音乐游戏，热门游戏。</li><li>乒乓球。运动，体育竞技。</li><li>燥热。射击，子弹时间。</li><li>onshape。墙来了，运动健身。</li><li>google画笔，重力草图。vr作画。</li><li>Pistol Whip。 射击。</li><li>假日模拟器。模拟。</li><li>夏日乐园。模拟，过山车，攀爬，射击，赛车，电影，游乐园其他项目。</li><li>风暴之地。射击</li><li>不惧跌倒。砍杀，魔法。</li><li>荣耀胸章。战争，射击。</li><li>多合一运动。运动，体育竞技。</li><li>巫师的华尔兹。魔法，手部追踪。</li><li>渔夫的故事。剧情。</li><li>莫斯。剧情。</li><li>七彩鸭传奇。剧情，电影。</li><li>红色物质。剧情。</li><li>水果忍者。切水果。</li><li>虚拟画廊。</li></ul><h3 id="pico-neo3-1"><a href="#pico-neo3-1" class="headerlink" title="pico neo3"></a>pico neo3</h3><p>游戏中心的游戏过少。无需推荐。可无线串流stream平台使用。</p><p>推荐先锋卡选择的游戏：</p><ul><li>亚利桑那阳光</li><li>乒乓球</li></ul><h3 id="mr"><a href="#mr" class="headerlink" title="mr+"></a>mr+</h3><ul><li>Halo Recruit. 射击</li><li>Halo tour. 旅游，风景，罗马。</li><li>veer。 vr视频。</li><li>free the light. vr体验。</li><li>invasion vr. 剧情。</li></ul><h3 id="quest2-常见问题"><a href="#quest2-常见问题" class="headerlink" title="quest2 常见问题"></a>quest2 常见问题</h3><h3 id="如何激活设备？"><a href="#如何激活设备？" class="headerlink" title="如何激活设备？"></a>如何激活设备？</h3><ul><li>方法1： 安装87vr软件，进行一键激活。</li><li>方法2： 找人（闲鱼就有）帮忙激活</li><li>方法3： 准备科学上网工具，sstap软件，注册facebook账户。根据网络上的教程自行尝试。</li></ul><h3 id="如何下载学习版游戏"><a href="#如何下载学习版游戏" class="headerlink" title="如何下载学习版游戏"></a>如何下载学习版游戏</h3><ul><li>先根据网络上的教程开启开发者模式。 </li><li>设备内安装 87VR应用中心</li></ul><h3 id="近视怎么办？"><a href="#近视怎么办？" class="headerlink" title="近视怎么办？"></a>近视怎么办？</h3><ul><li>可直接带眼镜使用，但是容易刮花屏幕</li><li>去淘宝或者闲鱼，配一副 适合的 眼镜片。</li></ul><h3 id="不清晰"><a href="#不清晰" class="headerlink" title="不清晰"></a>不清晰</h3><ul><li>尝试调节瞳距 </li><li>尝试调整头戴的松紧</li></ul><h2 id="pico-neo3-常见问题"><a href="#pico-neo3-常见问题" class="headerlink" title="pico neo3 常见问题"></a>pico neo3 常见问题</h2><h3 id="手柄失灵或者设备卡顿"><a href="#手柄失灵或者设备卡顿" class="headerlink" title="手柄失灵或者设备卡顿"></a>手柄失灵或者设备卡顿</h3><p>升级系统到最新版</p><h3 id="无线串流卡顿，画面让人感觉眩晕"><a href="#无线串流卡顿，画面让人感觉眩晕" class="headerlink" title="无线串流卡顿，画面让人感觉眩晕"></a>无线串流卡顿，画面让人感觉眩晕</h3><ul><li>降低屏幕刷新率</li><li>降低清晰度</li><li>降低stream vr采样率</li><li>使用wifi6 路由器</li><li>使用有线串流</li><li>升级pc端串流软件</li></ul><h2 id="mr-常见问题"><a href="#mr-常见问题" class="headerlink" title="mr+ 常见问题"></a>mr+ 常见问题</h2><h3 id="设备链接异常"><a href="#设备链接异常" class="headerlink" title="设备链接异常"></a>设备链接异常</h3><ul><li>重新插拔usb</li><li>卸载windows mr 后重新安装</li><li>更新NVIDIA显卡驱动</li></ul><h2 id="如何开发vr游戏demo并在设备上运行"><a href="#如何开发vr游戏demo并在设备上运行" class="headerlink" title="如何开发vr游戏demo并在设备上运行"></a>如何开发vr游戏demo并在设备上运行</h2><h3 id="pico-neo3-2"><a href="#pico-neo3-2" class="headerlink" title="pico neo3"></a>pico neo3</h3><p>如果已经会js等其他开发语言，按以下内容学习即可（预计需要6-10小时，可以达到自己的demo游戏在设备里运行，可操作手柄，移动，进行场景交互），之后就是unity的学习了。</p><ul><li>pico neo3 环境创建, 参考这个视频: <a href="https://www.bilibili.com/video/BV1Sy4y1K7Nd?spm_id_from=333.337.search-card.all.click" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Sy4y1K7Nd?spm_id_from=333.337.search-card.all.click</a></li><li>unity开发vr程序，参考这个视频：<a href="https://www.bilibili.com/video/BV1Yt4y1y7sx?spm_id_from=333.999.0.0" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Yt4y1y7sx?spm_id_from=333.999.0.0</a></li><li>3d模型下载，网站：<a href="https://3dexport.com/free-3d-models/format(fbx" target="_blank" rel="noopener">https://3dexport.com/free-3d-models/format(fbx</a>)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前购买并体验了三台vr设备，分别是facebook的quest2, 国内的pico neo3和三星的玄龙MR+, 其中前两款是一体机，最后一款是vr头显。下面谈一下各个设备的使用体验。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;目录&lt;/li&gt;
&lt;li&gt;先说结论&lt;ul&gt;
&lt;li&gt;如何选择&lt;/li&gt;
&lt;li&gt;购买渠道&lt;/li&gt;
&lt;li&gt;多久会玩腻？vr设备缺点&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;vr设备相关基本概念&lt;ul&gt;
&lt;li&gt;一体机&lt;/li&gt;
&lt;li&gt;头显&lt;/li&gt;
&lt;li&gt;可视角度&lt;/li&gt;
&lt;li&gt;单眼分辨率&lt;/li&gt;
&lt;li&gt;屏幕类型&lt;/li&gt;
&lt;li&gt;屏幕刷新率&lt;/li&gt;
&lt;li&gt;定位系统&lt;/li&gt;
&lt;li&gt;有线串流&lt;/li&gt;
&lt;li&gt;无线串流&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;quest2, pico neo3, mr+对比&lt;ul&gt;
&lt;li&gt;参数、体验对比&lt;/li&gt;
&lt;li&gt;quest2体验&lt;/li&gt;
&lt;li&gt;pico neo3体验&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;游戏和应用推荐&lt;/li&gt;
&lt;li&gt;配件选购&lt;/li&gt;
&lt;li&gt;quest2 常见问题&lt;/li&gt;
&lt;li&gt;pico neo3 常见问题&lt;/li&gt;
&lt;li&gt;mr+ 常见问题&lt;/li&gt;
&lt;li&gt;如何开发vr游戏demo并在设备上运行&lt;ul&gt;
&lt;li&gt;pico neo3&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="测评" scheme="http://www.javabin.cn/tags/%E6%B5%8B%E8%AF%84/"/>
    
      <category term="科技" scheme="http://www.javabin.cn/tags/%E7%A7%91%E6%8A%80/"/>
    
      <category term="虚拟现实" scheme="http://www.javabin.cn/tags/%E8%99%9A%E6%8B%9F%E7%8E%B0%E5%AE%9E/"/>
    
  </entry>
  
  <entry>
    <title>【爬虫】cloudflare反爬虫机制绕过方法</title>
    <link href="http://www.javabin.cn/2022/bot.html"/>
    <id>http://www.javabin.cn/2022/bot.html</id>
    <published>2022-03-07T14:14:46.000Z</published>
    <updated>2024-03-11T14:49:03.592Z</updated>
    
    <content type="html"><![CDATA[<p>当我们爬取cloudflare保护的网站时，网页会停留在跳转页面，然后运行一堆js来检测你的浏览器环境是不是真实用户访问，<br>如果检测不通过，就会一直卡在跳转页面，爬虫无法正常访问真实网页。如何解决？</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>爬虫检测网页[工具]</li><li>常见前端反爬虫方式</li><li>常见解决方法</li><li>如何解决</li><li>cloudflare调试方法</li><li>有用的一些链接和参考资料<a id="more"></a></li></ul><h2 id="爬虫检测网页"><a href="#爬虫检测网页" class="headerlink" title="爬虫检测网页"></a>爬虫检测网页</h2><p>可使用你的爬虫打开这个<a href="/bot/bot.html">爬虫检测</a>网页，查看爬虫是否可以被检测到。</p><h2 id="常见前端反爬虫方式"><a href="#常见前端反爬虫方式" class="headerlink" title="常见前端反爬虫方式"></a>常见前端反爬虫方式</h2><h3 id="检查浏览器user-agent"><a href="#检查浏览器user-agent" class="headerlink" title="检查浏览器user-agent"></a>检查浏览器user-agent</h3><p>通过检测当前user-agent是否为真实浏览器来区分当前请求是否来自真实用户。爬虫使用的常见user-agent类型：</p><ul><li>user-agent为空。没有设置user-agent。</li><li>user-agent中包含特殊字符。如：python,java,bot,spider, headless等。其中，使用chromedriver驱动无头浏览器<br>访问网站时，user-agent中会自动添加Headless字段。</li></ul><h3 id="检查浏览器是否有真实JS运行环境"><a href="#检查浏览器是否有真实JS运行环境" class="headerlink" title="检查浏览器是否有真实JS运行环境"></a>检查浏览器是否有真实JS运行环境</h3><p>常见方式： </p><ul><li>检查浏览器运行时对象。如<code>window, document, window.navigator, document.onmouseover,document.title, navigator.platform, window.location.href, history</code>等</li><li>检查浏览器功能。如: 操作cookie, 操作localStorage, 操作历史记录，操作iframe, 操作canvas, 操作window.performance</li></ul><h3 id="检查浏览器是否以无头模式运行"><a href="#检查浏览器是否以无头模式运行" class="headerlink" title="检查浏览器是否以无头模式运行"></a>检查浏览器是否以无头模式运行</h3><ul><li>检查调试器是否加载。如：将覆盖了toString方法的对象使用console.log输出到控制台，当控制台打开时，覆盖后的toString方法会被调用，可获取到控制台已被打开。</li></ul><p>以无头浏览器模式运行chrome时，会与真实浏览器存在差异。无头浏览器运行时差异主要有：</p><ul><li><code>window.chrome</code>不存在</li><li><code>navigator.plugins</code>为空。无任何浏览器插件是不正常的，正常情况会有一些默认的插件。</li><li><code>navigator.languages</code>为空。未设置浏览器当前语言环境。</li><li><code>navigator.webdriver</code>为true。chrome浏览器被chromdriver驱动时，这个值会设为true,正常应该为undefined</li><li>存在<code>document.$cdc_asdjflasutopfhvcZLmcfl_</code>。chromedriver 驱动的chrome浏览器，会设置一个这个属性。</li></ul><h3 id="图形验证码"><a href="#图形验证码" class="headerlink" title="图形验证码"></a>图形验证码</h3><p>不讨论这种情况。</p><h2 id="常见解决方法"><a href="#常见解决方法" class="headerlink" title="常见解决方法"></a>常见解决方法</h2><h3 id="后端检测了user-agent-解决：设置user-agent信息"><a href="#后端检测了user-agent-解决：设置user-agent信息" class="headerlink" title="后端检测了user-agent. 解决：设置user-agent信息"></a>后端检测了user-agent. 解决：设置user-agent信息</h3><p>以requests, httpx， scrapy访问网页时，设置user-agent信息.</p><pre><code class="python">    import requests    url = &#39;xx&#39;    session = requests.Session()    data = session.get(url, headers={&#39;User-Agent&#39;: &quot;xxxx&quot;}).json()</code></pre><h3 id="浏览器js检测了user-agent-解决方法：修改user-agent"><a href="#浏览器js检测了user-agent-解决方法：修改user-agent" class="headerlink" title="浏览器js检测了user-agent. 解决方法：修改user-agent"></a>浏览器js检测了user-agent. 解决方法：修改user-agent</h3><p>以chrome-driver浏览器运行时 删除user-agent中的headless字段。</p><pre><code class="python">    driver.execute_cdp_cmd(        &quot;Network.setUserAgentOverride&quot;,        {            &quot;userAgent&quot;: driver.execute_script(                &quot;return navigator.userAgent&quot;            ).replace(&quot;Headless&quot;, &quot;&quot;)        },    )</code></pre><h3 id="网页运行了一段js。-解决方法：JSV8运行页面js"><a href="#网页运行了一段js。-解决方法：JSV8运行页面js" class="headerlink" title="网页运行了一段js。 解决方法：JSV8运行页面js"></a>网页运行了一段js。 解决方法：JSV8运行页面js</h3><p>如果网页运行了一段混淆后js，计算出了一个token，访问时必须带着这个token, 可用JSV8运行网页js, 生成token.</p><h3 id="网页检测了window-chrome是否存在-解决方法：-设置window-chrome"><a href="#网页检测了window-chrome是否存在-解决方法：-设置window-chrome" class="headerlink" title="网页检测了window.chrome是否存在. 解决方法： 设置window.chrome"></a>网页检测了window.chrome是否存在. 解决方法： 设置window.chrome</h3><pre><code class="python">    driver.execute_cdp_cmd(        &quot;Page.addScriptToEvaluateOnNewDocument&quot;,        {            &quot;source&quot;: &quot;&quot;&quot;                Object.defineProperty(window, &#39;chrome&#39;, {                        get: () =&gt; {}                })&quot;&quot;&quot;        },    )</code></pre><h3 id="网页检测了navigator-webdriver是否为true-解决方法：-设置navigator-webdriver为undefined"><a href="#网页检测了navigator-webdriver是否为true-解决方法：-设置navigator-webdriver为undefined" class="headerlink" title="网页检测了navigator.webdriver是否为true. 解决方法： 设置navigator.webdriver为undefined"></a>网页检测了<code>navigator.webdriver</code>是否为<code>true</code>. 解决方法： 设置navigator.webdriver为undefined</h3><pre><code class="python">    driver.execute_cdp_cmd(        &quot;Page.addScriptToEvaluateOnNewDocument&quot;,        {            &quot;source&quot;: &quot;&quot;&quot;                Object.defineProperty(navigator, &#39;webdriver&#39;, {                        get: () =&gt; undefined                })&quot;&quot;&quot;        },    )</code></pre><h3 id="网页检测了navigator-plugins是否为空-解决方法：-设置navigator-plugins为自定义数据。"><a href="#网页检测了navigator-plugins是否为空-解决方法：-设置navigator-plugins为自定义数据。" class="headerlink" title="网页检测了navigator.plugins是否为空. 解决方法： 设置navigator.plugins为自定义数据。"></a>网页检测了<code>navigator.plugins</code>是否为空. 解决方法： 设置navigator.plugins为自定义数据。</h3><pre><code class="python">    driver.execute_cdp_cmd(        &quot;Page.addScriptToEvaluateOnNewDocument&quot;,        {            &quot;source&quot;: &quot;&quot;&quot;                Object.defineProperty(navigator, &#39;webdriver&#39;, {                        get: () =&gt; [1, 2, 3]                })&quot;&quot;&quot;        },    )</code></pre><h3 id="网页检测了navigator-languages是否为空-解决方法：-设置navigator-languages为自定义数据。"><a href="#网页检测了navigator-languages是否为空-解决方法：-设置navigator-languages为自定义数据。" class="headerlink" title="网页检测了navigator.languages是否为空. 解决方法： 设置navigator.languages为自定义数据。"></a>网页检测了<code>navigator.languages</code>是否为空. 解决方法： 设置navigator.languages为自定义数据。</h3><pre><code class="python">    driver.execute_cdp_cmd(        &quot;Page.addScriptToEvaluateOnNewDocument&quot;,        {            &quot;source&quot;: &quot;&quot;&quot;                Object.defineProperty(navigator, &#39;languages&#39;, {                        get: () =&gt; [&#39;en-US&#39;, &#39;en&#39;]                })&quot;&quot;&quot;        },    )</code></pre><h3 id="网页检测了Notification-permission是否为空-解决方法：-设置navigator-permission为自定义数据。"><a href="#网页检测了Notification-permission是否为空-解决方法：-设置navigator-permission为自定义数据。" class="headerlink" title="网页检测了Notification.permission是否为空. 解决方法： 设置navigator.permission为自定义数据。"></a>网页检测了<code>Notification.permission</code>是否为空. 解决方法： 设置navigator.permission为自定义数据。</h3><pre><code class="python">    driver.execute_cdp_cmd(        &quot;Page.addScriptToEvaluateOnNewDocument&quot;,        {            &quot;source&quot;: &quot;&quot;&quot;            Object.defineProperty(Notification, &#39;permission&#39;, { get: () =&gt; &quot;default&quot;});            &quot;&quot;&quot;        },    )</code></pre><h3 id="网页检测了-cdc-asdjflasutopfhvcZLmcfl-是否存在"><a href="#网页检测了-cdc-asdjflasutopfhvcZLmcfl-是否存在" class="headerlink" title="网页检测了$cdc_asdjflasutopfhvcZLmcfl_是否存在."></a>网页检测了<code>$cdc_asdjflasutopfhvcZLmcfl_</code>是否存在.</h3><p>需要修改<code>chromedriver</code>程序, 将<code>$cdc_asdjflasutopfhvcZLmcfl_</code> 替换为其他字符串。</p><pre><code class="shell">    sed -b -i &#39;s/$cdc_asdjflasutopfhvcZLmcfl_/$cda_asdjflasutopfhvcZLmcfl_/g&#39; /tmp/chromedriver</code></pre><h2 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h2><p>如果遇到疯狂进行环境检测的网站，要绕过会非常恶心。这种情况最好的方式就是直接运行浏览器以无头模型进行爬取。</p><p>上面这些脚本和方法有人已经写了个库，<a href="https://github.com/ultrafunkamsterdam/undetected-chromedriver" target="_blank" rel="noopener">undetected-chromedriver</a>,使用这个库启动chrome就可以自动将无头浏览器的差异屏蔽掉。</p><p>但是这个只能开着浏览器界面运行，当以无头模式运行时，浏览器内部还是可以检测到chrome并没有在真实屏幕上运行。</p><p>解决方法：可以使用<code>pyvirtualdisplay</code>，将程序界面运行到虚拟屏幕上。此时也不需要弹出浏览器界面，在linux服务器上可以正常运行。</p><ul><li>安装依赖</li></ul><pre><code class="shell">    sudo apt-get install xvfb xserver-xephyr tigervnc-standalone-server x11-utils gnumeric    sudo apt install python3-virtualenv    virtualenv -p `which python3` venv    source venv/bin/active    pip3 install undetected-chromedriver    pip3 install pyvirtualdisplay pillow EasyProcess    python3 undetected_bot.py</code></pre><ul><li>示例代码</li></ul><pre><code class="python">    import time    import tempfile    import undetected_chromedriver.v2 as uc    from pyvirtualdisplay import Display    def main():        with Display(visible=False, size=(1366, 768), backend=&#39;xvfb&#39;) as _display:            options = uc.ChromeOptions()            options.add_argument(&#39;--user-data-dir={}&#39;.format(tempfile.mktemp()))            options.add_argument(&#39;--no-first-run --no-service-autorun --password-store=basic&#39;)            driver = uc.Chrome(version_main=98, options=options)            driver.get(&#39;http://javabin.cn/bot/bot.html?headless&#39;)            # time.sleep(8)            print(driver.find_element_by_tag_name(&#39;body&#39;).text)    if __name__ == &#39;__main__&#39;:        main()</code></pre><h2 id="cloudflare类似网页-检测点调试方法"><a href="#cloudflare类似网页-检测点调试方法" class="headerlink" title="cloudflare类似网页 检测点调试方法"></a>cloudflare类似网页 检测点调试方法</h2><ul><li>chrome F11, 在<code>Source</code>–&gt; <code>Event Listener BreakPoints--&gt;XHR</code>中，对<code>readystatechange</code>事件下断点。</li><li>刷新网页，待从后端拿到第二次 检测浏览器环境的js时，附近单步执行几步就会对加密js进行解密，拿到解密后的js.</li><li>复制出依赖的几个全局变量，window._cf_chl_opt， window._cf_chl_ctx 复制出第一次的js。</li><li>将上面这些变量和js存到 sources–&gt;Snippets 中。</li><li>格式化后对语法进行分析，可看出一个大的Switch case结构，而且所有的数据均是从<code>_</code>数组里取出。对switch下断点，单步调试就可。大部分都是对常规操作加了一些花指令，从<code>_</code>里解密出一堆字符串，然后用这些字符串计算一下，拼出要调用的真实函数名称和变量，再进行调用。</li></ul><h2 id="有用的一些链接和参考资料"><a href="#有用的一些链接和参考资料" class="headerlink" title="有用的一些链接和参考资料"></a>有用的一些链接和参考资料</h2><ul><li><a href="https://github.com/ycq0125/cloudflare_detection/tree/870e61abf5a43239afede6a7ceb41f22acdbfab7/detection" target="_blank" rel="noopener">cloudflare 监测点相关js 可阅读代码</a></li><li><a href="https://mp.weixin.qq.com/s/efloBirboVfH2hK3cNoU5A" target="_blank" rel="noopener">Cloudflare的分析流程</a></li><li><a href="https://mp.weixin.qq.com/s/Bv8v7kbjX5NWWzdwCnwFHg" target="_blank" rel="noopener">Cloudflare（5秒盾）分析！！</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当我们爬取cloudflare保护的网站时，网页会停留在跳转页面，然后运行一堆js来检测你的浏览器环境是不是真实用户访问，&lt;br&gt;如果检测不通过，就会一直卡在跳转页面，爬虫无法正常访问真实网页。如何解决？&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;爬虫检测网页[工具]&lt;/li&gt;
&lt;li&gt;常见前端反爬虫方式&lt;/li&gt;
&lt;li&gt;常见解决方法&lt;/li&gt;
&lt;li&gt;如何解决&lt;/li&gt;
&lt;li&gt;cloudflare调试方法&lt;/li&gt;
&lt;li&gt;有用的一些链接和参考资料
    
    </summary>
    
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="python" scheme="http://www.javabin.cn/tags/python/"/>
    
      <category term="爬虫" scheme="http://www.javabin.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>【智能音箱】小爱音箱pro拆机,刷机,开启ssh,备份系统的方法</title>
    <link href="http://www.javabin.cn/2021/xiaoai_fm.html"/>
    <id>http://www.javabin.cn/2021/xiaoai_fm.html</id>
    <published>2021-06-18T18:22:24.000Z</published>
    <updated>2024-03-11T14:49:03.614Z</updated>
    
    <content type="html"><![CDATA[<p>618买了一个小爱音箱（型号:lx06），发现预留了ttl接口也可以进入uboot，打算刷机折腾一下。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>拆机</li><li>获取管理员权限</li><li>uboot 刷机</li><li>系统内 刷机</li><li>系统备份</li><li>后续玩法探索<a id="more"></a></li></ul><h2 id="拆机"><a href="#拆机" class="headerlink" title="拆机"></a>拆机</h2><p>  小爱音箱pro底部有个软垫子，用细的起子别进去，然然后撕下来就可以看到螺丝孔。依次拆下螺丝，打开底盖。之后会看到电源线和AUX线在壳子上用四个螺丝固定着，拆下这四个螺丝即可分离壳子和内部主体部分。（注意螺丝别乱了，有三处不一样的螺丝：底座，电源线，电路板固定螺丝）<br>  底座拆开后：<br>  <img src="/photo_2021/xiaoai_base.jpg" alt="后盖图"></p><p>  内部电路板,图中标箭头的位置rx,tx,gnd即为ttl链接的触点，在电路板后面也有对应的rx, tx, gnd,在板子后面的这三个位置用电烙铁分别焊接一条杜邦线，准备链接usb转ttl模块(CH340模块)，进行调试。</p><p>  <img src="/photo_2021/xiaoai_board.jpg" alt="电路板"></p><h2 id="获取管理员权限"><a href="#获取管理员权限" class="headerlink" title="获取管理员权限"></a>获取管理员权限</h2><p>  使用nmap扫描音箱端口可以发现音箱只开启了53和9999端口，无法telnet或者ssh链接，所以只能使用ttl进行链接。<br>1、ubuntu安装putty<code>sudo apt install putty</code><br>2、链接usb转ttl模块(接线rx–&gt;rx, tx–&gt;tx, gnd–&gt;gnd)，电脑查询新增的COM端口设备 <code>ls /dev</code>，新增的一般在最后一个, 类似<code>/dev/ttyUSB0</code> 或者 <code>/dev/COM0</code><br>3、  启动putty 并设置链接类型为<code>serial</code>，设置tty端口速率为<code>115200</code>，设置端口为你本机的端口如<code>/dev/ttyUSB0</code>. 点击 open<br> <img src="/photo_2021/xiaoai_putty.png" alt="putty设置"><br>4、按下回车之后可看到linux 登录界面，输入用户名:root 后回车，由于旧版的系统没有密码，如果你的系统版本比较旧，会直接以root进入系统。<br>5、如果你的系统升级到了最新版，已经无法直接进入系统， 需要切换到另一个系统尝试（小爱音箱有两套系统，升级时会往另一套系统刷入固件，这样好处就是升级时断电的话，可以自动切换到另一个系统，下一次升级时，刷坏的系统会被重新刷机激活）。<br>6、切换系统的方法。链接上ttl，拔掉小爱电源并重新插上来重启小爱，此时ttl会输出大量启动日志，在重启时迅速按ctrl+c或者任意按键，会进入uboot. </p><ul><li>uboot 执行<code>printenv</code>可查看系统当前的环境变量，其中<code>boot_part</code>指定了当前启动的是哪个系统。</li><li>uboot 执行<code>setenv boot_part boot0</code>切换到另一个系统，如果你以前是boot1那就设置为boot0，反之以前是boot0就设置为boot1.</li><li>uboot 执行 <code>run bootcmd</code> 进入系统引导，此时会进入你设置的另一个系统<br>7、在新的系统尝试root无密码登录。如果不行，且系统没有提示<code>[magic][release]</code>字样，可以尝试使用计算的密码登录。密码计算方法:先将SN码和固定字符串拼接之后取md5, 然后前14位字符为密码。伪代码<code>md5(SN+SALT).hex().lower()[:14]</code>，其中SN为你机器的SN码，在机器底座上写着，一般为<code>XXXXX/A0XUXXXX</code>；SALT为一个固定的字符串，可以用<code>9C78089F-83C7-3CDC-BCC9-93B378868E7F</code> 或者 <code>B0168A6D-119A-F21A-DA58-980732F80A19</code> 试试。执行下面的语句也行，将SN处替换为你的SN编码.</li></ul><pre><code class="shell">python -c &#39;import hashlib;print(hashlib.md5(b&quot;SN&quot;+b&quot;B0168A6D-119A-F21A-DA58-980732F80A19&quot;).hexdigest()[:14])&#39;python -c &#39;import hashlib;print(hashlib.md5(b&quot;SN&quot;+b&quot;9C78089F-83C7-3CDC-BCC9-93B378868E7F&quot;).hexdigest()[:14])&#39;</code></pre><p>8、如果还是无法进入系统，尝试进入<code>failsafe</code>模式（小爱固件改自openwrt）。进入方法: 重启音箱，等一点时间(log会有提示 push f … failsafe)后连续按<code>f + enter</code>，此时会进入failsafe模式。</p><ul><li>查看当前固件布局<code>cat /proc/mtd</code>。可看到当前的固件中存在<code>system1 和sytem0</code></li><li>查看当前系统磁盘 <code>df -h</code>（system0 对应/dev/mtdblock4, sytem1对应/dev/mtdblock5）</li><li>挂载syetem0 <code>mount  /dev/mtdblock4 /mnt/sys1</code> ,直接执行<code>etc/mi_console</code>，会输出当前root用户的密码。或者查看 <code>etc/mi_console</code>文件中的SALT（密码生成逻辑在这个文件里，<code>/mnt/sys1/etc/init.d/boot_check</code>会调用<code>mi_console</code>生成密码）。执行<code>strings /mnt/sys1/bin/mi_console | grep -E &#39;^[A-Z0-9-]+$&#39;&#39;</code> 获得SALT，然后用上一步的计算方法计算密码，然后切换到对应系统进行登录，如果还是无法登录，可用试试这样操作另一个系统。</li></ul><p>9、如果还是无法登录系统或者无法进入failsafe模式，直接使用uboot刷写固件。</p><h2 id="刷机"><a href="#刷机" class="headerlink" title="刷机"></a>刷机</h2><h3 id="uboot刷机"><a href="#uboot刷机" class="headerlink" title="uboot刷机"></a>uboot刷机</h3><p>1、前提：需要准备一个系统镜像。方法：将当系统镜像通过uboot提供的<code>loady,loadx,loadb</code>命令从计算机传输到音箱系统指定的内存地址中，然后使用<code>nand earse</code>擦除 磁盘 分区，最后使用<code>nand write 将内存中的数据写入固定的磁盘地址中</code>。<br>2、准备系统镜像。</p><ul><li>可通过升级时抓包或者使用网上别人已经提供的系统升级文件分离出system.img。固件文件提取命令<code>binwalk -e xxx.bin</code></li><li>也可直接使用网上已经登录系统的用户，备份的自己的系统镜像。这里我直接使用网络上分享的备份系统，系统版本<code>1.66.8</code>。来源: <a href="https://bbs.hassbian.com/forum.php?mod=redirect&amp;goto=findpost&amp;ptid=8754&amp;pid=349732" target="_blank" rel="noopener">https://bbs.hassbian.com/forum.php?mod=redirect&amp;goto=findpost&amp;ptid=8754&amp;pid=349732</a>  感谢原作者提供的系统镜像。<br>3、修改系统固件。</li><li>解包系统: <code>sudo unsquashfs -dest m5 m5.img</code></li><li>设置ttl无密码登录: 修改<code>m5/etc/inittab</code> 文件，将<code>ttyS0::askfirst:/bin/login</code> 这一行改为 <code>ttyS0::askfirst:/bin/ash --login</code></li><li>重置系统密码: 准备修改<code>m5/etc/shadow</code>文件，将root密码替换为自己的密码。shadow文件中<code>:</code>分割的第二位为密码，格式为<code>$加密类型$SALT$加密的密码</code>，加密类型 1代表md5. 可使用openssl 生成, 执行<code>openssl passwd -1 -salt &#39;N0Iz0LLs&#39; &#39;reece++&#39;</code>生成 salt为<code>N0lz0LLs</code>,密码为<code>reece++</code>的加密后密码，将输出的字符串替换掉 <code>m5/etc/shadow</code>里<code>root:xxxx:</code>处的xxxx，改为<code>root:$1$N0Iz0LLs$EPqv7WOl5S7ub/.EppH73.:18128:0:99999:7:::</code>。如果不需要密码，可以直接留空，改为<code>root::0:99999:7:::</code>（建议设置密码，不然ssh链接不安全）。</li><li>设置开机启动脚本: 编辑<code>etc/rc.local</code>，在其中加入你准备开机执行的命令，此处可加入以下命令开启ssh。</li></ul><pre><code class="shell"># open ssh[ -d /data/dropbear ] || mkdir /data/dropbear[ -s /data/dropbear/rsa.key ] || dropbearkey -t rsa -s 1024 -f /data/dropbear/rsa.key &amp;&gt; /dev/null/usr/sbin/dropbear -E -P /var/run/dropbear.pid -r /data/dropbear/rsa.key &gt; /tmp/ssh.log# your script/data/user.sh</code></pre><p>含义: <code>/data/dropbear</code>文件夹不存在则创建, <code>/data/dropbear/rsa.key</code> 文件不存在则自动生成，然后启动dropbear 在默认22端口开启ssh 服务，最后执行<code>/data/user.sh</code>， 以后可编辑这个文件来加入自己的开机自启命令，不用修改系统（运行时系统是只读模式，只有data和tmp可写）</p><ul><li>禁止自动升级系统: 编辑<code>m5/etc/crontabs/root</code>,注释掉<code>* 3 * * * /bin/ota slient  # check ota</code>，也可以将执行<code>mv /bin/ota /usr/share/ota.bk</code>将升级程序移动到其他地方，防止远程执行升级。</li><li>重新打包系统: <code>sudo mksquashfs m5 m5_1.img -b 131072 -comp xz -no-xattrs</code><br>4、进入uboot,刷写固件。</li><li>查看固件分区布局，启动时输出的日志里会输出固件分区布局，如下所示:</li></ul><pre><code class="shell">Creating 6 MTD partitions on &quot;A revision NAND 1Gib TC5XXXXXXXXX&quot;:0x000000800000-0x000001000000 : &quot;tpl&quot;0x000001000000-0x000001600000 : &quot;boot0&quot;0x000001600000-0x000001c00000 : &quot;boot1&quot;0x000001c00000-0x000004400000 : &quot;system0&quot;0x000004400000-0x000006c20000 : &quot;system1&quot; NAND bbt detect factory Bad block at 60000000x000006c20000-0x000008000000 : &quot;data&quot;</code></pre><p>本次将刷入system0分区，当然你也可以刷入sytem1分区。</p><ul><li>将m5_1.img传输的windows下，电脑切换到windows系统，下载secureCRT, 或者其他支持 kermit, xmodem, ymodem,zmodem协议的终端链接软件。使用secureCRT链接对应com端口（可在设备管理中查看对应com端口），设置好传输速率115200和流控制XON/XOFF。重启小爱进入uboot.</li><li>uboot执行<code>loady 1c00000</code>准备将ymodem协议传输的文件写入内存地址1c00000</li><li>使用电脑secureCRT–菜单传输–菜单发送ymodem文件，添加刚才打包的镜像文件m5_1.img，开始传输。这个过程相当漫长，传输速率大概1-5kb/s，需要2个小时才能传输完。</li><li>传输完后, 开始擦除系统分区，根据刚才的固件分区布局可知，system0在<code>0x000001c00000-0x000004400000</code>地址范围内，先将该部分地址擦除为1（注意！注意！注意！！！请确认你的固件分区布局，不同设备可能不一致，如果你擦除了其他分区的数据，可能导致系统无法启动，请确认输入执行的地址正确。）执行 <code>nand earse 1c00000 2800000</code>，从1c00000地址处开始擦除2800000大小的空间，用结束地址减去开始地址就是分区大小。也可以执行<code>python -c &#39;print(&quot;%x&quot; % (0x4400000-0x1c00000,))&#39;</code>计算出分区大小。</li><li>写入固件到分区。执行<code>nand write 1c00000 1c00000 2800000</code>将内存1c00000处的数据写入磁盘1c00000处，写入大小2800000。</li><li>必须先擦除才能写入，<code>nand write</code>只能将1 变为0 不能将0变为1，所以需要先执行nand earse 将磁盘擦除为 0xff。如果你忘记擦除就写入，系统0启动时会由于squashfs 文件系统错误而无法启动，此时按以上流程刷重新写系统就行。</li><li>刷写成功之后，执行<code>setenv boot_part boot0</code>设置启动刚才刷写的系统0，然后执行<code>run bootcmd</code>加载系统。</li><li>如果刷写成功，ttl中直接按回车就可以进入系统，ssh服务也已经启动。</li></ul><p><img src="/photo_2021/xiaoai_nand.jpeg" alt="uboot"></p><h2 id="系统内刷机"><a href="#系统内刷机" class="headerlink" title="系统内刷机"></a>系统内刷机</h2><p>  如果你已经以root进入了系统，且链接上网络，可通过 dd 或者 mtd命令直接刷写系统。此处假设你已经进入system1,需要刷写system0.<br>  1.执行 <code>scp xxx@192.168.1.x:m5.img /tmp/m5.img</code>使用scp将系统镜像从ubuntu传输到音箱系统。其中xxx@192.168.1.x 替换为你ubuntu系统的用户名和ip地址，如果不能链接，可能是你ubuntu系统sshd服务配置有问题。<br>  2.执行 <code>mtd write /tmp/m5.img system0</code>将系统镜像写入system0分区内。<br>  3.执行<code>/usr/bin/fw_env -s boot_part boot0</code> 设置环境变量boot_part, 使重启时进入system0<br>  4.重启系统<code>reboot -f</code></p><p>第2步也可用<code>dd if=/tmp/m5.img of=/dev/mtdblock4</code>代替，不过<code>dd</code>无法跳过系统坏块，如果你的系统有坏块，将报错提示 io error。</p><h2 id="系统备份"><a href="#系统备份" class="headerlink" title="系统备份"></a>系统备份</h2><h3 id="备份系统分区的方法"><a href="#备份系统分区的方法" class="headerlink" title="备份系统分区的方法"></a>备份系统分区的方法</h3><p>将自己的各个分区备份，出问题后可通过uboot恢复。例如要备份系统0, mtdblock4:</p><ol><li>备份系统。<code>dd if=/dev/mtdblock4 of=/tmp/m4.img</code></li><li>传输到电脑 。<code>scp /tmp/m4.img xxx@192.168.1.x:m4.img</code></li><li>删除临时文件 <code>rm /tmp/m4.img</code></li></ol><p>如果当前分区有坏块，可拆分为多个文件，备份。</p><h3 id="备份data分区的方法"><a href="#备份data分区的方法" class="headerlink" title="备份data分区的方法"></a>备份data分区的方法</h3><p>在<code>电脑</code>linux系统里执行下面的命令即可将data分区(mtdblock6)备份到当前linux系统目录。其中将<code>192.168.1.192</code>换为你的小爱同学的ip地址。</p><pre><code class="shell">  ssh root@192.168.1.192  &quot;dd if=/dev/mtdblock6 | gzip -1 -&quot; | dd of=backup.gz</code></pre><p>文件已压缩，恢复时注意先用gzip解压缩</p><pre><code class="shell">  gzip -dk backup.gz</code></pre><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>  后续玩法探索<br> 1、替换唤醒词，自定义唤醒词<br> 2、拦截im消息，实现自定义命令</p><p> 声明：本文只作为技术研究，您如果使用以上内容用作其他用途，请自行承担后果，与本人无关。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;618买了一个小爱音箱（型号:lx06），发现预留了ttl接口也可以进入uboot，打算刷机折腾一下。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;拆机&lt;/li&gt;
&lt;li&gt;获取管理员权限&lt;/li&gt;
&lt;li&gt;uboot 刷机&lt;/li&gt;
&lt;li&gt;系统内 刷机&lt;/li&gt;
&lt;li&gt;系统备份&lt;/li&gt;
&lt;li&gt;后续玩法探索
    
    </summary>
    
    
      <category term="系统" scheme="http://www.javabin.cn/tags/%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="刷机" scheme="http://www.javabin.cn/tags/%E5%88%B7%E6%9C%BA/"/>
    
      <category term="小爱音箱" scheme="http://www.javabin.cn/tags/%E5%B0%8F%E7%88%B1%E9%9F%B3%E7%AE%B1/"/>
    
      <category term="智能家居" scheme="http://www.javabin.cn/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"/>
    
  </entry>
  
  <entry>
    <title>python单元测试中使用mock patch来解决不易测试的依赖</title>
    <link href="http://www.javabin.cn/2019/mock_patch.html"/>
    <id>http://www.javabin.cn/2019/mock_patch.html</id>
    <published>2019-12-04T13:15:11.000Z</published>
    <updated>2024-03-11T14:49:03.607Z</updated>
    
    <content type="html"><![CDATA[<p>在进行单元测试时，有时会遇到代码里有大量 的外部接口请求，异步逻辑，复杂的校验，无法进行单元测试的函数调用等情况。这时，可使用mock工具来对这些逻辑进行patch。</p><a id="more"></a><h2 id="待测逻辑中有异步调用"><a href="#待测逻辑中有异步调用" class="headerlink" title="待测逻辑中有异步调用"></a>待测逻辑中有异步调用</h2><p>对异步调用的逻辑进行patch，使非异步调用逻辑可以正常执行。在当前单元测试中只校验异步调用时传入的参数正确。然后对异步调用的函数单独编写单元测试即可。</p><p>示例:</p><pre><code class="python"># coding: utf-8&quot;&quot;&quot;tasks/task.py简单模拟一个异步任务&quot;&quot;&quot;import threadingimport timeclass Task(object):    def __init__(self, val):        self.val = val    def run(self):        raise NotImplementedError    def apply(self):        self.run()    def apply_async(self):        thread = threading.Thread(target=self.run)        thread.start()class AddTask(Task):    def run(self):        print(&quot;running&quot;)        time.sleep(10)  # 耗时任务        self.val *= 30if __name__ == &quot;__main__&quot;:    print(AddTask(12).apply_async())    print(123)</code></pre><p>编写单元测试,patch异步调用</p><pre><code class="python"># coding: utf-8&quot;&quot;&quot;tests/tests.py&quot;&quot;&quot;from unittest import TestCasefrom mock import patchfrom tasks.task import func, AddTaskclass TestTask(TestCase):    @patch(&quot;tasks.task.AddTask&quot;)    # patch addtask    def test_task(self, mocked_task):        a = 1        b = 2        func(a, b)        mocked_task.assert_called_once_with(a+b)  # 只判断调用时参数正确    def test_add_task(self):        x = 10        task = AddTask(x)        task.apply()        assert x * 30 == task.val</code></pre><h2 id="待测逻辑中有无法单元测试的逻辑"><a href="#待测逻辑中有无法单元测试的逻辑" class="headerlink" title="待测逻辑中有无法单元测试的逻辑"></a>待测逻辑中有无法单元测试的逻辑</h2><p>对于待测逻辑中存在依赖外部接口，进行参数校验等逻辑，可进行patch,忽略这些校验使测试继续进行。</p><pre><code class="python"># tasks/task.pydef trans_name(name, val):  # 模拟一个简单场景:调用api校验数据后，然后才进行一系列逻辑    data = requests.get(u&quot;http://123.com/api/v2/check_name?name={}&quot;.format(name)).json()    if data[&#39;status&#39;] == &#39;ok&#39;:        name = &quot;{}_{}&quot;.format(name, val)        return name    return None</code></pre><p>编写单元测试，<code>patch requests.get</code>返回值和<code>json</code>方法</p><pre><code class="python"># coding: utf-8&quot;&quot;&quot;tests/test_process.py对于待测逻辑中存在依赖外部接口，进行参数校验等逻辑，可进行patch,忽略这些校验使测试继续进行。&quot;&quot;&quot;from unittest import TestCasefrom mock import patch, Mockfrom tasks.task import trans_nameclass TestTask(TestCase):    @patch(&quot;tasks.task.requests&quot;)    # patch addtask    def test_trans_name(self, mocked_requests):        name = &quot;ccc&quot;        val = &quot;222&quot;        mocked_response = Mock()        mocked_response.status_code = 200        mocked_response.json.return_value = {            &quot;status&quot;: &quot;ok&quot;        }        mocked_requests.get.side_effect = [mocked_response]        self.assertEqual(&quot;{}_{}&quot;.format(name, val), trans_name(name, val))        assert mocked_requests.get.call_count == 1</code></pre><h2 id="有多个patch对象"><a href="#有多个patch对象" class="headerlink" title="有多个patch对象"></a>有多个patch对象</h2><p>多个patch对象的参数顺序如下示例所示:</p><pre><code class="python">    @patch(&quot;tasks.task.AddTask1&quot;)    @patch(&quot;tasks.task.AddTask2&quot;)    def test_add_task(self, AddTask2, AddTask1):        pass</code></pre><h2 id="修改函数功能"><a href="#修改函数功能" class="headerlink" title="修改函数功能"></a>修改函数功能</h2><p>可将函数替换为自己定义的函数，实现修改函数的执行效果。这里为了简单说明情况，以替换time.sleep函数来说明情况。</p><pre><code class="python">@patch(&quot;tasks.task.time&quot;)    def test_func(self, mocked_time):        def sleep(t):   # 修改函数功能，以time.sleep举例            if t &gt; 3:                time.sleep(3)            else:                time.sleep(t)        mocked_time.sleep = Mock(side_effect=sleep)        func(3, 2)</code></pre><h2 id="替换函数返回值"><a href="#替换函数返回值" class="headerlink" title="替换函数返回值"></a>替换函数返回值</h2><p>直接改变函数返回值，用于patch token校验类的逻辑</p><pre><code class="python">@patch(&quot;tasks.task.token_check&quot;, return_value=True)    def test_func_ret(self, mocked_token_check):        pass</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在进行单元测试时，有时会遇到代码里有大量 的外部接口请求，异步逻辑，复杂的校验，无法进行单元测试的函数调用等情况。这时，可使用mock工具来对这些逻辑进行patch。&lt;/p&gt;
    
    </summary>
    
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="python" scheme="http://www.javabin.cn/tags/python/"/>
    
      <category term="单元测试" scheme="http://www.javabin.cn/tags/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>由数据不一致引起的对事物处理，事物可见性和session缓存的思考</title>
    <link href="http://www.javabin.cn/2019/sqlalchemy_session.html"/>
    <id>http://www.javabin.cn/2019/sqlalchemy_session.html</id>
    <published>2019-12-03T14:39:11.000Z</published>
    <updated>2024-03-11T14:49:03.612Z</updated>
    
    <content type="html"><![CDATA[<p>最近遇到一个问题：使用<code>sqlalchemy</code>创建了两个session,然后在第一个session里创建了一条数据，并commit到数据库后，在第二个session中修改了这条数据并commit到数据库，接着回到第一个session中查询,查询到的竟然是没有修改之前的数据。<br><a id="more"></a></p><h2 id="猜测原因"><a href="#猜测原因" class="headerlink" title="猜测原因"></a>猜测原因</h2><p>刚开始猜测到了两种原因：session缓存，事物可见性。这里一一说一下每种的分析。</p><h3 id="猜测是事物可见性导致的"><a href="#猜测是事物可见性导致的" class="headerlink" title="猜测是事物可见性导致的"></a>猜测是事物可见性导致的</h3><p>由于关闭了<code>auto commit</code>，采用手动显示提交事物的方式。猜测第一个session中的查询处于事物中，导致无法看到另一个session中事物对数据库做出的修改。</p><p>调试方法:<br>1.单步debug后发现的确是第一个session在最后一次查询时，查询到的结果和数据库不一致。第二个session的修改内容没有在这里体现。<br>2.对第一个session中创建对象并commit之后的<code>session</code>对象调试，发现它的<code>transaction</code>属性的状态为<code>ACTIVE</code><br>3.继续进入源码调试发现<code>transaction</code>状态在提交后会变为CLOSED然后又会重新开始一个事物。也就是说<code>第二个session修改数据时，第一个session也处于事物中</code>。</p><pre><code class="python">    # transaction的close方法中将自己的状态置为CLOSED,然后又开始一个事物。     def close(self, invalidate=False):        self.session.transaction = self._parent        if self._parent is None:            for connection, transaction, autoclose in set(                self._connections.values()            ):                if invalidate:                    connection.invalidate()                if autoclose:                    connection.close()                else:                    transaction.close()        self._state = CLOSED        self.session.dispatch.after_transaction_end(self.session, self)        if self._parent is None:            if not self.session.autocommit:                self.session.begin()        self.session = None        self._connections = None</code></pre><p>4.但是第二个session的事物明明已经提交了，在处于默认的事物级别（读已提交READ_COMMITED）下按理第一个session是可以读到修改的数据的。但是还是尝试下是否可以解决问题。即查询前提交或者放弃一次事物。</p><p>解决方法：<br>在查询之前调用一次<code>session.commit()</code>或者<code>session.rollback()</code>即可，也<code>的确管用</code>。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p><code>sqlalchemy</code>如果关闭<code>auto commit</code>的话，事物的管理是这样的：如果前一个事物被提交后，会将当前事物关闭，然后 马上重新开启一个事物。也就是说，你每时每刻都处于一个事物里，这会由于事物隔离级别导致不同事物间可能无法查询到写入数据库的数据。</p><p>但是目前这个场景却不是事物可见性导致的情况。因为第二个session 的事物已经提交，READ_COMMITED事物隔离级别下，这时数据库里的数据谁都可以查到的。（pgsql数据库查询默认事物隔离级别:<code>show default_transaction_isolation;</code>查看当前事物隔离级别:<code>show transaction_isolation;</code>）</p><h2 id="session-缓存"><a href="#session-缓存" class="headerlink" title="session 缓存"></a>session 缓存</h2><p>ORM框架建立了对象和数据库表之间的映射关系，当然也需要维护这个关系。一般都会实现持久化对象状态的维护(分别标记：未持久化的对象，已持久化到数据库的对象，已从数据库中删除的对象,已脱离session的对象等) <code>sqlalchemy</code>的状态可以查看文档:<a href="https://docs.sqlalchemy.org/en/13/orm/session_state_management.html" target="_blank" rel="noopener">持久化状态</a>)，而且也实现了session级的对象缓存，比如查询一个对象两次，第二次发现这个对象是已经持久化到数据库的对象，就不走数据库，直接从session缓存里取。这也就导致了，如果有两个session，那么每个session都不会知道自己从数据库里获取的对象已经被修改了，这时再从缓存中取的话，就会取出已经过期的数据。（一般以事物级别维护，同一个事物内才会如此）</p><h3 id="调试方法"><a href="#调试方法" class="headerlink" title="调试方法"></a>调试方法</h3><p>1.直接调用<code>session.expire(obj)</code>使当前对象过期后，查询数据库发现<code>查询结果和数据库一致</code>。<br>2.尝试打开<code>sqlalchemy</code>的<code>query log</code>. 在<code>create_engine</code>时设置<code>echo=True</code></p><pre><code class="python">    engine = create_engine(&quot;db_url&quot;, echo=True)</code></pre><p>发现有查询日志。</p><p>3.在我的场景里，第一个session创建的是B对象且已经提交。第二个session实际修改的是对象A的关联对象 对象B的字段。debug一下，在第一个session查询对象A之后，尝试读出对象B和B的属性时。下断点。可以F7跟进时看到进入了下面的代码。</p><pre><code class="python">    class InstrumentedAttribute(QueryableAttribute):        &quot;&quot;&quot;Class bound instrumented attribute which adds basic        :term:`descriptor` methods.        See :class:`.QueryableAttribute` for a description of most features.        &quot;&quot;&quot;        def __set__(self, instance, value):            self.impl.set(                instance_state(instance), instance_dict(instance), value, None            )        def __delete__(self, instance):            self.impl.delete(instance_state(instance), instance_dict(instance))        def __get__(self, instance, owner):            if instance is None:                return self            dict_ = instance_dict(instance)            if self._supports_population and self.key in dict_:                return dict_[self.key]            else:                return self.impl.get(instance_state(instance), dict_)</code></pre><p>其中 <code>__get__</code>方法即为获取属性时调用的方法，获取B对象的属性时，发现传入的B对象的地址和第一个session初次创建的对象B的地址一致，直接是同一个对象。的确是缓存导致了数据不一致的情况。</p><p>4.继续测试发现在新的事物中，读取上一个事物中已经commit的对象B的属性，会导致如果从当前事物中查询和这个对象B一样的query，就会直接使用对象B的缓存。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>查询前<code>session.expire(obj)</code>，<code>session.refresh(obj)</code>或者<code>session.commit()</code>，<code>sessoion.rollback()</code></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><code>sqlalchemy</code>关闭<code>auto commit</code>。就算commit后也其实一直在一个事物中</li><li>多个事物修改查询同一个数据，事物隔离级别和查询时机会影响查询的结果。</li><li>session缓存会出现拿到过期数据。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近遇到一个问题：使用&lt;code&gt;sqlalchemy&lt;/code&gt;创建了两个session,然后在第一个session里创建了一条数据，并commit到数据库后，在第二个session中修改了这条数据并commit到数据库，接着回到第一个session中查询,查询到的竟然是没有修改之前的数据。&lt;br&gt;
    
    </summary>
    
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="数据库" scheme="http://www.javabin.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="事物" scheme="http://www.javabin.cn/tags/%E4%BA%8B%E7%89%A9/"/>
    
  </entry>
  
  <entry>
    <title>Python性能优化浅谈</title>
    <link href="http://www.javabin.cn/2019/python_profile.html"/>
    <id>http://www.javabin.cn/2019/python_profile.html</id>
    <published>2019-11-28T13:39:11.000Z</published>
    <updated>2024-03-11T14:49:03.610Z</updated>
    
    <content type="html"><![CDATA[<p>最近在做一些性能优化方面的事情，总结一下遇到的一些问题和解决方法。<br><a id="more"></a></p><h2 id="优化哪里？"><a href="#优化哪里？" class="headerlink" title="优化哪里？"></a>优化哪里？</h2><p>既然需要性能优化，肯定是程序运行缓慢，那首先需要知道的是<code>慢在哪里</code>？<code>从哪里开始优化</code>?下面分情况说一下这个问题。</p><h3 id="系统那一部分运行缓慢"><a href="#系统那一部分运行缓慢" class="headerlink" title="系统那一部分运行缓慢?"></a>系统那一部分运行缓慢?</h3><p>对于分布式或者大型系统，由于子模块较多，组件较多，某一部分的运行缓慢可能会导致整个系统的运行缓慢。定位子模块，组件的方法很简单，只需要分别对每个部分进行benchmark,统计出每个部分的运行时间即可。常用的方法有：</p><ul><li><p>1.最简单的，直接使用，调用，运行单独的部分，统计运行时间。</p><ul><li>如:检查接口缓慢时。可先检查网络是否正常<code>ping  myserver.com</code> ，然后检查nginx是否正常<code>time curl -IL http://www.myserver.com/1.png</code> 接着检查web 服务是否缓慢  <code>time curl -IL http://www.myserver.com/api/v1/2</code>  </li></ul></li><li><p>2.在应用中添加统计运行时间的日志。</p><ul><li>如配置<code>logging</code>,或者开启服务的<code>慢查询日志</code>。</li></ul></li><li>3.借助相关工具查看性能差异。<ul><li>如:<code>top,htop</code>查看系统负载。<code>redis-benchmark，memaslap</code>等测试服务性能。</li></ul></li></ul><h3 id="Python程序缓慢如何定位"><a href="#Python程序缓慢如何定位" class="headerlink" title="Python程序缓慢如何定位"></a>Python程序缓慢如何定位</h3><ul><li>1.程序不是我写的,源码丢失。<ul><li>将pyc反编译为py。借助<code>uncompile2</code>工具反编译。记得<code>IDA 反编译工具</code>也可以，具体我没有试过，可以研究下。</li><li>使用<code>strace</code>  加载运行的进程，然后尝试触发缓慢的操作，接着将日志dump出来。可通过对日志中的系统调用时间进行分析，来确认那一部分缓慢，一般需要这样操作的情况，很大可能需要关注<code>socket</code>的 <code>recv send</code> 调用或者对<code>mutex_lock</code>锁的请求。</li></ul></li><li>2.有源码，但是只能线上运行或者只能线上重现。<ul><li><code>strace</code> 加载运行的进程</li><li>加日志分析统计运行缓慢的地方</li><li>使用pickle保存要调试函数的运行时的依赖参数对象，然后载入这些参数后进行正常情况的分析。</li></ul></li><li>3.正常情况<ul><li>使用<code>cprofile,line_profiler,vprof,yappi_profiler</code>之类的工具分析程序中各个函数的调用次数和时间</li><li>使用<code>Pycham</code>自带的<code>profile</code>工具，直接run菜单点击<code>Profile &#39;xxx.py&#39;</code>即可。自带的工具支持<code>call graph</code>模式查看耗时情况，非常直观。</li></ul></li></ul><h2 id="如何优化"><a href="#如何优化" class="headerlink" title="如何优化"></a>如何优化</h2><h3 id="Python中一般优化方法"><a href="#Python中一般优化方法" class="headerlink" title="Python中一般优化方法"></a>Python中一般优化方法</h3><ul><li>1.低风险高收益的情况。优化调用次数高的 util 方法，公共方法，多重迭代中调用的方法。 </li><li>2.高风险的情况。优化（重构）指定复杂函数的实现，优化拥有大量逻辑的方法，优化不熟悉的逻辑，改变函数功能。</li></ul><h3 id="开始优化"><a href="#开始优化" class="headerlink" title="开始优化"></a>开始优化</h3><ul><li>1.尝试去掉无用的<code>deepcopy</code>,<code>deepcopy</code>中为了检测相互引用，性能损失非常大。干掉deepcopy的方法如下:<ul><li>使用浅拷贝或者赋值。分析逻辑，如果deepcopy的结果只是修改了1,2层，直接使用copy,或者列表推导式重新生成一个修改后的结果。</li><li>使用其他序列化方法。如果只是字典，可<code>json.loads(json.dumps(x))</code>，注意:这种操作会导致<code>int类型的key被转换为字符串</code>，如果无法接受，请不要采取这种方法。</li></ul></li><li>3.将正则表达式提前编译<ul><li>当被大量调用时，<code>re.search(r&#39;xc+&#39;, &#39;xxxccxx&#39;)</code>要比先一次<code>p=re.compile(r&#39;xc+&#39;)</code>之后再<code>p.search(&#39;xxxccxx&#39;)</code>慢很多。原因是正则表达使用时如果没有编译的话，会先进行编译，然后缓存下来，但是这个缓存很小，先编译并自行保存起来将可以有效提升速度。</li></ul></li><li>4.使用C库替换py库，如使用<code>cPickle替换pickle</code>。</li><li>5.去掉不需要的迭代。如使用<code>(x for x in c if x.id==1)替代[x for x in c if x.id==1]</code>，使用<code>xrange替代range</code></li><li>6.去掉无用的逻辑。如 <code>while len(x) &gt; idx</code>先计算出<code>cnt = len(x) 然后在while cnt &gt; idx</code></li><li>7.将无参数依赖的函数或者功能，转为类变量或者cached_property。</li><li>8.去重时在set中判断 而不是 list.</li><li>9.对于重复遍历的数据。先以id或者其他label建立mapping，减少遍历次数。</li><li>10.对于可缓存参数的函数。添加<code>function_tools.lru_cache</code>或者 <code>pyhon2.7</code>里的<code>backports.functools_lru_cache.lru_cache</code>或者对于逻辑不复杂的，直接建立<code>mapping</code>全局变量保存</li><li>11.预生成结果。对于参数规模可以预测的，预先生成全局mapping即可。</li><li>12.减少数据库操作。迭代执行sql的操作，转换为一条语句执行，或批量执行。</li><li>13.尝试将串行转并行<ul><li>对于执行结果无依赖的逻辑,可串行执行。如使用子进程生成结果时，如果生成的结果在当前函数里不在读取，可直接执行子进程，然后在函数末尾等待进程结束。</li></ul></li><li>14.对于IO密集型的程序（大量时间在等待IO），使用多线程，简单实现可直接使用线程池<code>multiprocessing.dummy.Pool</code>.</li><li>15.对于计算密集型程序（大量时间是执行算法逻辑），使用低级语言，多任务，分布式，异步任务和消息队列，高性能计算。如<code>Cpython实现,numpy,cpickle</code>,<code>multiprocessing</code>，<code>celery+rabbitmq+GFS</code>, <code>numba</code>。</li><li>16.对于web程序,使用异步IO。如添加<code>gevent</code>支持，使用<code>tornado web框架</code>, 使用<code>python3 await/async</code>新特性。</li><li>17.对于网络接口添加缓存。如使用内存数据库<code>memcache</code>, 可持久化内存数据库<code>redis</code>,mmap类数据库<code>mongo</code>等缓存服务。</li><li>18.建立本地缓存。如在前端使用<code>localstorage</code>缓存接口数据，安卓使用<code>sqlite</code>缓存离线数据。</li><li>19.页面静态化。如将页面分块后转为静态文件，以前只在java web项目中见到过。</li><li>20.升级硬件，网络，增加CDN服务。有钱可以优先考虑这个。</li></ul><h2 id="优化后如何保持"><a href="#优化后如何保持" class="headerlink" title="优化后如何保持"></a>优化后如何保持</h2><ul><li>使用<code>pylint</code>等工具做静态代码检查，并加入持续集成。</li><li>进行<code>code review</code></li><li>使用<code>ngios，zabbix</code>等做监控，加入性能监控脚本，业务中使用<code>statsd, logging</code>加入<code>benchmark</code>检测埋点，持续监控<code>slow query</code>,<code>slow log</code>。</li><li>进行 单元/API测试（保证代码 可单元测试）</li><li>对单一服务进行benchmark测试。</li><li>升级硬件</li></ul><h2 id="性能与实现简单之间的平衡"><a href="#性能与实现简单之间的平衡" class="headerlink" title="性能与实现简单之间的平衡"></a>性能与实现简单之间的平衡</h2><ul><li>先做低风险高收益优化</li><li>可以升级硬件解决的也不是啥大问题</li><li>太复杂必然会带入bug</li><li>做好充足测试</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在做一些性能优化方面的事情，总结一下遇到的一些问题和解决方法。&lt;br&gt;
    
    </summary>
    
    
      <category term="总结" scheme="http://www.javabin.cn/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="性能优化" scheme="http://www.javabin.cn/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
</feed>
